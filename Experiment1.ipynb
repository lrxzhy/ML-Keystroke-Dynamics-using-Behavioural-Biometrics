{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UUID\n",
       "0    48\n",
       "1    48\n",
       "2    48\n",
       "3    48\n",
       "4    48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pLN1</th>\n",
       "      <th>p.2</th>\n",
       "      <th>pLN3</th>\n",
       "      <th>pt4</th>\n",
       "      <th>pi5</th>\n",
       "      <th>pe6</th>\n",
       "      <th>pLN7</th>\n",
       "      <th>p58</th>\n",
       "      <th>pLN9</th>\n",
       "      <th>pSH10</th>\n",
       "      <th>...</th>\n",
       "      <th>du2a13</th>\n",
       "      <th>du2n14</th>\n",
       "      <th>du2n15</th>\n",
       "      <th>avgdu</th>\n",
       "      <th>avgud</th>\n",
       "      <th>avgdd</th>\n",
       "      <th>avguu</th>\n",
       "      <th>avdu2</th>\n",
       "      <th>avgp</th>\n",
       "      <th>avga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>37.875</td>\n",
       "      <td>24.466667</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>88.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>670740857</td>\n",
       "      <td>3015</td>\n",
       "      <td>1081</td>\n",
       "      <td>37.625</td>\n",
       "      <td>31.933333</td>\n",
       "      <td>64.066667</td>\n",
       "      <td>63.266667</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2361</td>\n",
       "      <td>1918</td>\n",
       "      <td>884</td>\n",
       "      <td>64.125</td>\n",
       "      <td>453.733333</td>\n",
       "      <td>515.933333</td>\n",
       "      <td>513.133333</td>\n",
       "      <td>575.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1918</td>\n",
       "      <td>1438</td>\n",
       "      <td>827</td>\n",
       "      <td>63.250</td>\n",
       "      <td>347.733333</td>\n",
       "      <td>407.733333</td>\n",
       "      <td>406.400000</td>\n",
       "      <td>466.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>69.375</td>\n",
       "      <td>-9.133333</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pLN1  p.2  pLN3  pt4  pi5  pe6  pLN7  p58  pLN9  pSH10    ...     \\\n",
       "0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "1   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "2   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "3   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "4   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "\n",
       "      du2a13     du2n14  du2n15   avgdu       avgud       avgdd       avguu  \\\n",
       "0       2302  670740857     973  37.875   24.466667   56.800000   55.866667   \n",
       "1  670740857       3015    1081  37.625   31.933333   64.066667   63.266667   \n",
       "2       2361       1918     884  64.125  453.733333  515.933333  513.133333   \n",
       "3       1918       1438     827  63.250  347.733333  407.733333  406.400000   \n",
       "4       2302  670740857     973  69.375   -9.133333   56.800000   55.866667   \n",
       "\n",
       "        avdu2  avgp      avga  \n",
       "0   88.200000   1.0  0.004412  \n",
       "1   95.400000   1.0  0.004167  \n",
       "2  575.333333   1.0  0.008333  \n",
       "3  466.400000   1.0  0.008211  \n",
       "4  121.800000   1.0  0.009804  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2310 entries, 0 to 2309\n",
      "Columns: 147 entries, pLN1 to avga\n",
      "dtypes: float64(71), int64(76)\n",
      "memory usage: 2.6 MB\n",
      "initial data info None\n",
      "data is (2310, 147)\n",
      "(2310, 147)\n",
      "(2310,)\n",
      "147\n",
      "metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  3\n",
      "f1  0.21366203718807766\n",
      "Accuracy: 0.2722943722943723\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  4\n",
      "f1  0.4668496126033941\n",
      "Accuracy: 0.5\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  5\n",
      "f1  0.48122783486630133\n",
      "Accuracy: 0.5125541125541125\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  6\n",
      "f1  0.474068722922489\n",
      "Accuracy: 0.5043290043290043\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  7\n",
      "f1  0.5318955035448361\n",
      "Accuracy: 0.554978354978355\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  8\n",
      "f1  0.5395534558282092\n",
      "Accuracy: 0.5632034632034632\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  9\n",
      "f1  0.5306624396200694\n",
      "Accuracy: 0.5536796536796537\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  10\n",
      "f1  0.5477364646732952\n",
      "Accuracy: 0.5696969696969696\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  11\n",
      "f1  0.5478326109002258\n",
      "Accuracy: 0.5696969696969697\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  12\n",
      "f1  0.5580668164476151\n",
      "Accuracy: 0.5774891774891775\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  13\n",
      "f1  0.5586109225561912\n",
      "Accuracy: 0.5783549783549783\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  14\n",
      "f1  0.5951291749365007\n",
      "Accuracy: 0.6121212121212121\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  15\n",
      "f1  0.5919784657665689\n",
      "Accuracy: 0.6099567099567099\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  16\n",
      "f1  0.5945687113919123\n",
      "Accuracy: 0.6103896103896104\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  17\n",
      "f1  0.5900253066241096\n",
      "Accuracy: 0.606926406926407\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  18\n",
      "f1  0.5900542760419254\n",
      "Accuracy: 0.6077922077922078\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  19\n",
      "f1  0.6029720567867317\n",
      "Accuracy: 0.6186147186147186\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  20\n",
      "f1  0.620852652522425\n",
      "Accuracy: 0.6337662337662338\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  21\n",
      "f1  0.6472108136809561\n",
      "Accuracy: 0.6597402597402597\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  22\n",
      "f1  0.6557453854296729\n",
      "Accuracy: 0.6653679653679655\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  23\n",
      "f1  0.6678680983105816\n",
      "Accuracy: 0.6779220779220779\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  24\n",
      "f1  0.6686655841444551\n",
      "Accuracy: 0.6792207792207792\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  25\n",
      "f1  0.6810868622285097\n",
      "Accuracy: 0.69004329004329\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  26\n",
      "f1  0.6759539014099678\n",
      "Accuracy: 0.6848484848484849\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  27\n",
      "f1  0.688453130966983\n",
      "Accuracy: 0.6961038961038961\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  28\n",
      "f1  0.6973483366422935\n",
      "Accuracy: 0.7043290043290042\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  29\n",
      "f1  0.6993858013420958\n",
      "Accuracy: 0.706060606060606\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  30\n",
      "f1  0.6947749886943255\n",
      "Accuracy: 0.7008658008658009\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  31\n",
      "f1  0.7068682213952941\n",
      "Accuracy: 0.7116883116883117\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  32\n",
      "f1  0.7081260222113372\n",
      "Accuracy: 0.7125541125541126\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  33\n",
      "f1  0.7098300538423923\n",
      "Accuracy: 0.7151515151515151\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  34\n",
      "f1  0.7085221240754995\n",
      "Accuracy: 0.713852813852814\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  35\n",
      "f1  0.7136566606195089\n",
      "Accuracy: 0.7177489177489178\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  36\n",
      "f1  0.7127814045862376\n",
      "Accuracy: 0.7173160173160172\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  37\n",
      "f1  0.7136639524727258\n",
      "Accuracy: 0.7194805194805195\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  38\n",
      "f1  0.714536343500437\n",
      "Accuracy: 0.7203463203463204\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  39\n",
      "f1  0.7038599891531753\n",
      "Accuracy: 0.7086580086580087\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  40\n",
      "f1  0.7085441620605802\n",
      "Accuracy: 0.7134199134199134\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  41\n",
      "f1  0.7147072968140411\n",
      "Accuracy: 0.7194805194805195\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  42\n",
      "f1  0.7186435348976797\n",
      "Accuracy: 0.7225108225108225\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  43\n",
      "f1  0.7164491302735376\n",
      "Accuracy: 0.7199134199134198\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  44\n",
      "f1  0.7192182569970342\n",
      "Accuracy: 0.7225108225108225\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  45\n",
      "f1  0.7230599827176161\n",
      "Accuracy: 0.7264069264069264\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  46\n",
      "f1  0.7263955902120833\n",
      "Accuracy: 0.729004329004329\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  47\n",
      "f1  0.7287979788151421\n",
      "Accuracy: 0.7307359307359308\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  48\n",
      "f1  0.7328266665610054\n",
      "Accuracy: 0.7350649350649351\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  49\n",
      "f1  0.7375255558320521\n",
      "Accuracy: 0.7406926406926407\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  50\n",
      "f1  0.7431820343702001\n",
      "Accuracy: 0.7454545454545454\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  51\n",
      "f1  0.744249028037175\n",
      "Accuracy: 0.7476190476190477\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  52\n",
      "f1  0.7458337587075003\n",
      "Accuracy: 0.7493506493506494\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  53\n",
      "f1  0.7477127131199274\n",
      "Accuracy: 0.7502164502164502\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  54\n",
      "f1  0.7501581832323854\n",
      "Accuracy: 0.7528138528138528\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  55\n",
      "f1  0.7460292055571227\n",
      "Accuracy: 0.7502164502164501\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  56\n",
      "f1  0.7472112969569376\n",
      "Accuracy: 0.7506493506493507\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  57\n",
      "f1  0.7427256905139126\n",
      "Accuracy: 0.7458874458874459\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  58\n",
      "f1  0.7494088502847264\n",
      "Accuracy: 0.7523809523809525\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  59\n",
      "f1  0.7471569646497405\n",
      "Accuracy: 0.7506493506493507\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  60\n",
      "f1  0.7450043611203194\n",
      "Accuracy: 0.7471861471861472\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  61\n",
      "f1  0.7440602979684641\n",
      "Accuracy: 0.7463203463203463\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  62\n",
      "f1  0.7432470996227982\n",
      "Accuracy: 0.745887445887446\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  63\n",
      "f1  0.7446332214487461\n",
      "Accuracy: 0.7476190476190476\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  64\n",
      "f1  0.7493478416784678\n",
      "Accuracy: 0.7528138528138528\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  65\n",
      "f1  0.751626538450671\n",
      "Accuracy: 0.754978354978355\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  66\n",
      "f1  0.750537342464004\n",
      "Accuracy: 0.7541125541125542\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  67\n",
      "f1  0.7511442565047567\n",
      "Accuracy: 0.7536796536796537\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  68\n",
      "f1  0.7502309934134148\n",
      "Accuracy: 0.7523809523809524\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  69\n",
      "f1  0.7497600372947163\n",
      "Accuracy: 0.7519480519480519\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  70\n",
      "f1  0.7509962387597329\n",
      "Accuracy: 0.7528138528138528\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  71\n",
      "f1  0.7510932035893181\n",
      "Accuracy: 0.7528138528138528\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  72\n",
      "f1  0.757692185873803\n",
      "Accuracy: 0.7588744588744589\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  73\n",
      "f1  0.7542172281633904\n",
      "Accuracy: 0.7562770562770563\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  74\n",
      "f1  0.7544590559704547\n",
      "Accuracy: 0.7562770562770564\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  75\n",
      "f1  0.7520397533793788\n",
      "Accuracy: 0.7545454545454545\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  76\n",
      "f1  0.7600227229781732\n",
      "Accuracy: 0.7627705627705629\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  77\n",
      "f1  0.7574019076844001\n",
      "Accuracy: 0.7610389610389612\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  78\n",
      "f1  0.7612968285428178\n",
      "Accuracy: 0.7640692640692641\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  79\n",
      "f1  0.7614513799999689\n",
      "Accuracy: 0.7645021645021646\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  80\n",
      "f1  0.757554943258574\n",
      "Accuracy: 0.7606060606060606\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  81\n",
      "f1  0.7586780067678285\n",
      "Accuracy: 0.7614718614718614\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  82\n",
      "f1  0.7594988628167832\n",
      "Accuracy: 0.7632034632034632\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  83\n",
      "f1  0.7635752587023142\n",
      "Accuracy: 0.7675324675324674\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  84\n",
      "f1  0.764994902876331\n",
      "Accuracy: 0.7688311688311688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  85\n",
      "f1  0.7652863420828118\n",
      "Accuracy: 0.7701298701298702\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  86\n",
      "f1  0.7668466637388272\n",
      "Accuracy: 0.7718614718614718\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  87\n",
      "f1  0.7650987944279327\n",
      "Accuracy: 0.7696969696969698\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  88\n",
      "f1  0.7677699894894217\n",
      "Accuracy: 0.7727272727272727\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  89\n",
      "f1  0.7628424677061242\n",
      "Accuracy: 0.7679653679653681\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  90\n",
      "f1  0.7633125418624191\n",
      "Accuracy: 0.7688311688311689\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  91\n",
      "f1  0.765998807689876\n",
      "Accuracy: 0.770995670995671\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  92\n",
      "f1  0.7648278800587023\n",
      "Accuracy: 0.7692640692640692\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  93\n",
      "f1  0.7668932503899245\n",
      "Accuracy: 0.7705627705627706\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  94\n",
      "f1  0.767465619956303\n",
      "Accuracy: 0.7718614718614718\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  95\n",
      "f1  0.7669140793846676\n",
      "Accuracy: 0.7709956709956709\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  96\n",
      "f1  0.7627865367045537\n",
      "Accuracy: 0.7675324675324675\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  97\n",
      "f1  0.7596564947746645\n",
      "Accuracy: 0.764069264069264\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  98\n",
      "f1  0.7571246797600383\n",
      "Accuracy: 0.7614718614718614\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  99\n",
      "f1  0.759915560469177\n",
      "Accuracy: 0.7645021645021646\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  100\n",
      "f1  0.7644593868106794\n",
      "Accuracy: 0.7688311688311689\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  101\n",
      "f1  0.7601905320810471\n",
      "Accuracy: 0.7658008658008657\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  102\n",
      "f1  0.7575562531098697\n",
      "Accuracy: 0.7636363636363637\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  103\n",
      "f1  0.759330328622677\n",
      "Accuracy: 0.7649350649350649\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  104\n",
      "f1  0.7598041494288126\n",
      "Accuracy: 0.7658008658008657\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  105\n",
      "f1  0.7594800146890935\n",
      "Accuracy: 0.7653679653679654\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  106\n",
      "f1  0.7559537860492787\n",
      "Accuracy: 0.7623376623376623\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  107\n",
      "f1  0.7560630707040179\n",
      "Accuracy: 0.7623376623376623\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  108\n",
      "f1  0.7555681543649458\n",
      "Accuracy: 0.7619047619047619\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  109\n",
      "f1  0.7527359542873595\n",
      "Accuracy: 0.758008658008658\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  110\n",
      "f1  0.7563564470622017\n",
      "Accuracy: 0.761038961038961\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  111\n",
      "f1  0.7553565725284928\n",
      "Accuracy: 0.7606060606060605\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  112\n",
      "f1  0.7558638132091149\n",
      "Accuracy: 0.7601731601731603\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  113\n",
      "f1  0.7550272700731067\n",
      "Accuracy: 0.7601731601731603\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  114\n",
      "f1  0.7549518476600218\n",
      "Accuracy: 0.7597402597402597\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  115\n",
      "f1  0.7532936357420696\n",
      "Accuracy: 0.7584415584415585\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  116\n",
      "f1  0.7523587337032714\n",
      "Accuracy: 0.758008658008658\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  117\n",
      "f1  0.7556342074912298\n",
      "Accuracy: 0.7606060606060605\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  118\n",
      "f1  0.7581586272143951\n",
      "Accuracy: 0.7632034632034632\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  119\n",
      "f1  0.7580807716178227\n",
      "Accuracy: 0.7632034632034632\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  120\n",
      "f1  0.7579703942606921\n",
      "Accuracy: 0.7632034632034632\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  121\n",
      "f1  0.756812671582725\n",
      "Accuracy: 0.7623376623376623\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  122\n",
      "f1  0.7564053437841313\n",
      "Accuracy: 0.7623376623376623\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  123\n",
      "f1  0.754224415044761\n",
      "Accuracy: 0.7601731601731603\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  124\n",
      "f1  0.7557029361216095\n",
      "Accuracy: 0.7619047619047619\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  125\n",
      "f1  0.7548392064381699\n",
      "Accuracy: 0.7610389610389611\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  126\n",
      "f1  0.7530889190377682\n",
      "Accuracy: 0.7601731601731602\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  127\n",
      "f1  0.7511390207058988\n",
      "Accuracy: 0.7584415584415585\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  128\n",
      "f1  0.7486977975318945\n",
      "Accuracy: 0.7549783549783549\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  129\n",
      "f1  0.7481646569605568\n",
      "Accuracy: 0.7545454545454545\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  130\n",
      "f1  0.7476094431394912\n",
      "Accuracy: 0.754112554112554\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  131\n",
      "f1  0.7414107555695277\n",
      "Accuracy: 0.7480519480519481\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  132\n",
      "f1  0.7396187452083476\n",
      "Accuracy: 0.7467532467532468\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  133\n",
      "f1  0.735863449213207\n",
      "Accuracy: 0.7432900432900433\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  134\n",
      "f1  0.7352927356554805\n",
      "Accuracy: 0.7428571428571429\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  135\n",
      "f1  0.733077944589658\n",
      "Accuracy: 0.7411255411255412\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  136\n",
      "f1  0.7339902026906099\n",
      "Accuracy: 0.741991341991342\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  137\n",
      "f1  0.7326678076592765\n",
      "Accuracy: 0.7406926406926407\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  138\n",
      "f1  0.7343464464807725\n",
      "Accuracy: 0.741991341991342\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  139\n",
      "f1  0.734333084916609\n",
      "Accuracy: 0.7415584415584415\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  140\n",
      "f1  0.7324342351033206\n",
      "Accuracy: 0.7398268398268398\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  141\n",
      "f1  0.7333526240736576\n",
      "Accuracy: 0.7402597402597403\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  142\n",
      "f1  0.7331314599563896\n",
      "Accuracy: 0.7402597402597403\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  143\n",
      "f1  0.7324874490144414\n",
      "Accuracy: 0.7398268398268398\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  144\n",
      "f1  0.7320367393811494\n",
      "Accuracy: 0.7393939393939394\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  145\n",
      "f1  0.7379700610684488\n",
      "Accuracy: 0.7450216450216449\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  146\n",
      "f1  0.73950137246989\n",
      "Accuracy: 0.7458874458874459\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  147\n",
      "f1  0.7394566091523993\n",
      "Accuracy: 0.7458874458874459\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7kAAAEDCAYAAADwacP6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkqTBMMmVJEmSJA3GD/ezZBfPW9poAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc384cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import lsanomaly\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn import utils  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "\n",
    "# import the CSV from http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "# this will return a pandas dataframe.\n",
    "data = pd.read_csv('C:/Users/S/Documents/PY/increased30featureswodevice.csv', low_memory=False)\n",
    "'''data.loc[data['UUID'] == \"RVTNB1502866560357\", \"attack\"] = 1  \n",
    "data.loc[data['UUID'] != \"RVTNB1502866560357\", \"attack\"] = -1\n",
    "df_majority = data[data['attack']==-1]\n",
    "df_minority = data[data['attack']==1]\n",
    "from sklearn.utils import resample\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=830,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#print(data['attack'].value_counts())'''\n",
    "\n",
    "#target=np.array(target)\n",
    "#target = pd.DataFrame(target,columns=['attack'])\n",
    "\n",
    "#data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "categorical_columns=[\"UUID\"]\n",
    "cate_data = data[categorical_columns]\n",
    "\n",
    "#for col in data.columns.values:\n",
    "#    print(col, data[col].unique())\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def label_encode(cate_data, columns):\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        col_values_unique = list(cate_data[col].unique())\n",
    "        le_fitted = le.fit(col_values_unique)\n",
    " \n",
    "        col_values = list(cate_data[col].values)\n",
    "        le.classes_\n",
    "        col_values_transformed = le.transform(col_values)\n",
    "        cate_data[col] = col_values_transformed\n",
    " \n",
    "to_be_encoded_cols = cate_data.columns.values\n",
    "label_encode(cate_data, to_be_encoded_cols)\n",
    "display(cate_data.head())\n",
    "target=cate_data['UUID']\n",
    "target=np.array(target)\n",
    "#target = pd.DataFrame(target)\n",
    "#target=target1.values\n",
    "\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "data=pd.concat([data,cate_data], axis=1)\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "\n",
    "# check the shape for sanity checking.\n",
    "data.shape\n",
    "display(data.head())\n",
    "print(\"initial data info\",data.info())\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"data is\",data.shape)\n",
    "from skfeature.function.information_theoretical_based import LCSI\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "\n",
    "from skfeature.utility.entropy_estimators import *\n",
    "import scipy.io\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#scaled_data=data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaleddata= pd.DataFrame(scaled_data)\n",
    "scaled_data=np.array(scaled_data)\n",
    "\n",
    "print(scaled_data.shape)\n",
    "print(target.shape)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "#display(target.head())\n",
    "#idx=MRMR.mrmr(scaled_data,target,n_selected_features=50)\n",
    "'''from sklearn import cross_validation\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "correct = 0\n",
    "print(\"scaled data details - \",scaled_data.info())\n",
    "print(\"target data details - \",target.info())\n",
    "for train, test in ss:\n",
    "    #print(scaled_data[train])\n",
    "    #print(target[train])\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=50)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:50]]\n",
    "print(features)    '''\n",
    "'''skb= SVC(kernel=\"linear\")\n",
    "rfe = RFE(estimator=skb, n_features_to_select=70)\n",
    "rfe=rfe.fit(scaleddata,target)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "skft = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "for train, test in skft:\n",
    "    X_train,X_test=scaled_data.iloc[train],scaled_data.iloc[test]\n",
    "    Y_train,y_test=target.iloc[train],target.iloc[test]\n",
    "    model1 = svm.OneClassSVM(nu=nu, kernel='rbf', gamma=0.10000000000000001)  \n",
    "    model1.fit(X_train, Y_train)\n",
    "    scores = cross_val_score(model1,X_test,y_test, cv=5, scoring='accuracy')\n",
    "    print(scores)\n",
    "print(scores.mean())'''\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5,random_state=36851234)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "clf=SVC(kernel='linear')\n",
    "#clf = svm.SVC(decision_function_shape='ovo')    # linear SVM\n",
    "correct = 0\n",
    "fscoreTotal =0\n",
    "print(scaled_data.shape[1])\n",
    "plt.figure(figsize=(16, 8))\n",
    "accuracy = plt.subplot(221)\n",
    "\n",
    "x=np.array([])\n",
    "y=np.array([])\n",
    "f1val=np.array([])\n",
    "numoffeatures= lambda start, end: range(start, end+1)\n",
    "for i in numoffeatures(3,scaled_data.shape[1]):\n",
    "    for train, test in skf.split(scaled_data,target):\n",
    "        # obtain the index of each feature on the training set\n",
    "        idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=i)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "        features = scaled_data[:, idx[0:i]]\n",
    "        #print(target[train])\n",
    "        # train a classification model with the selected features on the training dataset\n",
    "        clf.fit(features[train], target[train])\n",
    "\n",
    "        # predict the class labels of test data\n",
    "        y_predict = clf.predict(features[test])\n",
    "        print(\"metrics\")\n",
    "        # obtain the classification accuracy on the test data\n",
    "        acc = accuracy_score(target[test], y_predict)\n",
    "        correct = correct + acc\n",
    "        fscore=f1_score(target[test], y_predict,average='weighted')\n",
    "        fscoreTotal=fscoreTotal+fscore\n",
    "        #print(\"fsc \",f1_score(target[test], y_predict,average='weighted'))\n",
    "        #print(\"conf mat \",confusion_matrix(target[test],y_predict))\n",
    "        #print(\"ACCURACY: \", (accuracy_score(target[test], y_predict)))\n",
    "        #report = classification_report(target[test], y_predict)\n",
    "        #print(report)\n",
    "    x=np.append(x,i)\n",
    "    accscores=float(correct)/5\n",
    "    f1scores=float(fscoreTotal)/5\n",
    "    y=np.append(y,accscores)\n",
    "    f1val=np.append(f1val,f1scores)\n",
    "    np.savetxt('exp2.txt', (y,f1val), fmt='%.5g', delimiter=',', newline='\\n')\n",
    "    print(\"loop \",i)\n",
    "    print(\"f1 \",f1scores)\n",
    "    # output the average classification accuracy over all 10 folds\n",
    "    print(\"Accuracy:\", accscores)\n",
    "    fscore=0\n",
    "    acc=0\n",
    "    correct=0\n",
    "    fscoreTotal=0\n",
    "##svc=SelectKBest(mutual_info_classif, k=50).fit_transform(data,target)\n",
    "#svc = SVC(kernel=\"linear\")\n",
    "#rfe = RFE(estimator=svc, n_features_to_select=10)\n",
    "#rfe.fit(data, target)\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, m*x + b, '-')\n",
    "accuracy.plot(x,y)\n",
    "accuracy.set_title(\"mRMR feature selection\")\n",
    "accuracy.set_xlim(3, scaled_data.shape[1])\n",
    "accuracy.set_xlabel(\"Number of Features\")\n",
    "accuracy.set_ylim(0.5, 0.8)\n",
    "accuracy.set_ylabel(\"Classification Accuracy\")\n",
    "f1=plt.subplot(222)\n",
    "n, c = np.polyfit(x, f1val, 1)\n",
    "plt.plot(x, f1val, '.')\n",
    "plt.plot(x, n*x + c, '-')\n",
    "f1.plot(x,f1val)\n",
    "f1.set_title(\"mRMR feature selection\")\n",
    "f1.set_xlim(3, scaled_data.shape[1])\n",
    "f1.set_xlabel(\"Number of Features\")\n",
    "f1.set_ylim(0.5, 0.8)\n",
    "f1.set_ylabel(\"Classification F1 Score\")\n",
    "plt.show()\n",
    "print(\"here\")\n",
    "#score = svc.score(data, target)\n",
    "##print(svc)\n",
    "#ranking = rfe.feature_importances_\n",
    "#print(\"no of feat\",ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UUID\n",
       "0    48\n",
       "1    48\n",
       "2    48\n",
       "3    48\n",
       "4    48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pLN1</th>\n",
       "      <th>p.2</th>\n",
       "      <th>pLN3</th>\n",
       "      <th>pt4</th>\n",
       "      <th>pi5</th>\n",
       "      <th>pe6</th>\n",
       "      <th>pLN7</th>\n",
       "      <th>p58</th>\n",
       "      <th>pLN9</th>\n",
       "      <th>pSH10</th>\n",
       "      <th>...</th>\n",
       "      <th>du2a13</th>\n",
       "      <th>du2n14</th>\n",
       "      <th>du2n15</th>\n",
       "      <th>avgdu</th>\n",
       "      <th>avgud</th>\n",
       "      <th>avgdd</th>\n",
       "      <th>avguu</th>\n",
       "      <th>avdu2</th>\n",
       "      <th>avgp</th>\n",
       "      <th>avga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>37.875</td>\n",
       "      <td>24.466667</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>88.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>670740857</td>\n",
       "      <td>3015</td>\n",
       "      <td>1081</td>\n",
       "      <td>37.625</td>\n",
       "      <td>31.933333</td>\n",
       "      <td>64.066667</td>\n",
       "      <td>63.266667</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2361</td>\n",
       "      <td>1918</td>\n",
       "      <td>884</td>\n",
       "      <td>64.125</td>\n",
       "      <td>453.733333</td>\n",
       "      <td>515.933333</td>\n",
       "      <td>513.133333</td>\n",
       "      <td>575.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1918</td>\n",
       "      <td>1438</td>\n",
       "      <td>827</td>\n",
       "      <td>63.250</td>\n",
       "      <td>347.733333</td>\n",
       "      <td>407.733333</td>\n",
       "      <td>406.400000</td>\n",
       "      <td>466.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>69.375</td>\n",
       "      <td>-9.133333</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pLN1  p.2  pLN3  pt4  pi5  pe6  pLN7  p58  pLN9  pSH10    ...     \\\n",
       "0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "1   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "2   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "3   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "4   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "\n",
       "      du2a13     du2n14  du2n15   avgdu       avgud       avgdd       avguu  \\\n",
       "0       2302  670740857     973  37.875   24.466667   56.800000   55.866667   \n",
       "1  670740857       3015    1081  37.625   31.933333   64.066667   63.266667   \n",
       "2       2361       1918     884  64.125  453.733333  515.933333  513.133333   \n",
       "3       1918       1438     827  63.250  347.733333  407.733333  406.400000   \n",
       "4       2302  670740857     973  69.375   -9.133333   56.800000   55.866667   \n",
       "\n",
       "        avdu2  avgp      avga  \n",
       "0   88.200000   1.0  0.004412  \n",
       "1   95.400000   1.0  0.004167  \n",
       "2  575.333333   1.0  0.008333  \n",
       "3  466.400000   1.0  0.008211  \n",
       "4  121.800000   1.0  0.009804  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2310 entries, 0 to 2309\n",
      "Columns: 147 entries, pLN1 to avga\n",
      "dtypes: float64(71), int64(76)\n",
      "memory usage: 2.6 MB\n",
      "initial data info None\n",
      "data is (2310, 147)\n",
      "(2310, 147)\n",
      "(2310,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.67      0.80         6\n",
      "          1       0.83      0.83      0.83         6\n",
      "          2       1.00      1.00      1.00         6\n",
      "          3       0.83      0.83      0.83         6\n",
      "          4       0.67      0.67      0.67         6\n",
      "          5       1.00      0.83      0.91         6\n",
      "          6       1.00      0.83      0.91         6\n",
      "          7       1.00      1.00      1.00         6\n",
      "          8       0.11      0.17      0.13         6\n",
      "          9       0.67      1.00      0.80         6\n",
      "         10       0.60      0.50      0.55         6\n",
      "         11       0.50      0.67      0.57         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       1.00      1.00      1.00         6\n",
      "         15       0.57      0.67      0.62         6\n",
      "         16       1.00      1.00      1.00         6\n",
      "         17       0.83      0.83      0.83         6\n",
      "         18       0.86      1.00      0.92         6\n",
      "         19       0.71      0.83      0.77         6\n",
      "         20       1.00      0.83      0.91         6\n",
      "         21       0.86      1.00      0.92         6\n",
      "         22       1.00      1.00      1.00         6\n",
      "         23       1.00      0.83      0.91         6\n",
      "         24       0.67      0.33      0.44         6\n",
      "         25       0.40      0.67      0.50         6\n",
      "         26       0.83      0.83      0.83         6\n",
      "         27       0.25      0.17      0.20         6\n",
      "         28       1.00      0.83      0.91         6\n",
      "         29       0.80      0.67      0.73         6\n",
      "         30       1.00      1.00      1.00         6\n",
      "         31       0.71      0.83      0.77         6\n",
      "         32       0.86      1.00      0.92         6\n",
      "         33       0.75      1.00      0.86         6\n",
      "         34       0.71      0.83      0.77         6\n",
      "         35       0.86      1.00      0.92         6\n",
      "         36       1.00      0.83      0.91         6\n",
      "         37       1.00      0.67      0.80         6\n",
      "         38       0.75      0.50      0.60         6\n",
      "         39       0.75      0.50      0.60         6\n",
      "         40       0.67      0.67      0.67         6\n",
      "         41       0.00      0.00      0.00         6\n",
      "         42       0.57      0.67      0.62         6\n",
      "         43       0.75      1.00      0.86         6\n",
      "         44       1.00      0.67      0.80         6\n",
      "         45       0.67      0.67      0.67         6\n",
      "         46       0.36      0.67      0.47         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       0.67      0.33      0.44         6\n",
      "         49       1.00      0.83      0.91         6\n",
      "         50       0.75      0.50      0.60         6\n",
      "         51       0.17      0.17      0.17         6\n",
      "         52       1.00      1.00      1.00         6\n",
      "         53       0.67      0.67      0.67         6\n",
      "         54       0.71      0.83      0.77         6\n",
      "         55       0.86      1.00      0.92         6\n",
      "         56       1.00      1.00      1.00         6\n",
      "         57       0.60      0.50      0.55         6\n",
      "         58       0.83      0.83      0.83         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      0.83      0.91         6\n",
      "         62       1.00      1.00      1.00         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       0.86      1.00      0.92         6\n",
      "         65       0.00      0.00      0.00         6\n",
      "         66       0.67      0.67      0.67         6\n",
      "         67       1.00      0.83      0.91         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       1.00      0.83      0.91         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       0.75      0.50      0.60         6\n",
      "         73       0.44      0.67      0.53         6\n",
      "         74       0.62      0.83      0.71         6\n",
      "         75       0.14      0.17      0.15         6\n",
      "         76       0.83      0.83      0.83         6\n",
      "\n",
      "avg / total       0.78      0.76      0.76       462\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.67      0.80         6\n",
      "          1       0.62      0.83      0.71         6\n",
      "          2       1.00      1.00      1.00         6\n",
      "          3       0.83      0.83      0.83         6\n",
      "          4       0.62      0.83      0.71         6\n",
      "          5       1.00      0.83      0.91         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       1.00      1.00      1.00         6\n",
      "          8       0.30      0.50      0.37         6\n",
      "          9       0.75      1.00      0.86         6\n",
      "         10       0.50      0.50      0.50         6\n",
      "         11       0.50      0.67      0.57         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       1.00      0.67      0.80         6\n",
      "         15       0.50      0.83      0.62         6\n",
      "         16       1.00      1.00      1.00         6\n",
      "         17       0.75      1.00      0.86         6\n",
      "         18       0.86      1.00      0.92         6\n",
      "         19       0.83      0.83      0.83         6\n",
      "         20       1.00      0.83      0.91         6\n",
      "         21       1.00      1.00      1.00         6\n",
      "         22       1.00      1.00      1.00         6\n",
      "         23       1.00      1.00      1.00         6\n",
      "         24       0.50      0.33      0.40         6\n",
      "         25       0.57      0.67      0.62         6\n",
      "         26       0.71      0.83      0.77         6\n",
      "         27       0.33      0.33      0.33         6\n",
      "         28       1.00      0.83      0.91         6\n",
      "         29       0.43      0.50      0.46         6\n",
      "         30       1.00      1.00      1.00         6\n",
      "         31       0.33      0.33      0.33         6\n",
      "         32       0.86      1.00      0.92         6\n",
      "         33       0.60      1.00      0.75         6\n",
      "         34       0.83      0.83      0.83         6\n",
      "         35       0.86      1.00      0.92         6\n",
      "         36       1.00      0.83      0.91         6\n",
      "         37       0.71      0.83      0.77         6\n",
      "         38       0.80      0.67      0.73         6\n",
      "         39       0.67      0.67      0.67         6\n",
      "         40       0.75      0.50      0.60         6\n",
      "         41       0.75      0.50      0.60         6\n",
      "         42       0.67      0.67      0.67         6\n",
      "         43       0.75      1.00      0.86         6\n",
      "         44       1.00      0.67      0.80         6\n",
      "         45       0.57      0.67      0.62         6\n",
      "         46       0.43      0.50      0.46         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       0.60      0.50      0.55         6\n",
      "         49       1.00      0.83      0.91         6\n",
      "         50       0.75      0.50      0.60         6\n",
      "         51       0.25      0.17      0.20         6\n",
      "         52       1.00      1.00      1.00         6\n",
      "         53       0.71      0.83      0.77         6\n",
      "         54       0.71      0.83      0.77         6\n",
      "         55       1.00      1.00      1.00         6\n",
      "         56       1.00      1.00      1.00         6\n",
      "         57       0.67      0.67      0.67         6\n",
      "         58       0.75      0.50      0.60         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       1.00      1.00      1.00         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       0.86      1.00      0.92         6\n",
      "         65       0.40      0.33      0.36         6\n",
      "         66       0.80      0.67      0.73         6\n",
      "         67       1.00      0.83      0.91         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       1.00      0.67      0.80         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       0.67      0.33      0.44         6\n",
      "         73       0.80      0.67      0.73         6\n",
      "         74       0.83      0.83      0.83         6\n",
      "         75       0.25      0.17      0.20         6\n",
      "         76       0.67      0.67      0.67         6\n",
      "\n",
      "avg / total       0.79      0.78      0.78       462\n",
      "\n",
      "each loop acc 0.764069264069\n",
      "each loop rbf acc 0.779220779221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91         6\n",
      "          1       0.83      0.83      0.83         6\n",
      "          2       0.86      1.00      0.92         6\n",
      "          3       1.00      0.83      0.91         6\n",
      "          4       0.67      0.33      0.44         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       0.86      1.00      0.92         6\n",
      "          8       0.33      0.67      0.44         6\n",
      "          9       0.71      0.83      0.77         6\n",
      "         10       1.00      0.50      0.67         6\n",
      "         11       0.67      0.67      0.67         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       1.00      0.67      0.80         6\n",
      "         15       0.50      0.33      0.40         6\n",
      "         16       0.86      1.00      0.92         6\n",
      "         17       0.86      1.00      0.92         6\n",
      "         18       0.86      1.00      0.92         6\n",
      "         19       0.86      1.00      0.92         6\n",
      "         20       0.86      1.00      0.92         6\n",
      "         21       1.00      1.00      1.00         6\n",
      "         22       1.00      0.83      0.91         6\n",
      "         23       0.67      0.33      0.44         6\n",
      "         24       0.40      0.33      0.36         6\n",
      "         25       0.50      0.33      0.40         6\n",
      "         26       0.83      0.83      0.83         6\n",
      "         27       0.43      0.50      0.46         6\n",
      "         28       1.00      0.67      0.80         6\n",
      "         29       0.86      1.00      0.92         6\n",
      "         30       0.86      1.00      0.92         6\n",
      "         31       1.00      0.67      0.80         6\n",
      "         32       1.00      1.00      1.00         6\n",
      "         33       0.57      0.67      0.62         6\n",
      "         34       0.86      1.00      0.92         6\n",
      "         35       0.83      0.83      0.83         6\n",
      "         36       1.00      1.00      1.00         6\n",
      "         37       0.50      0.50      0.50         6\n",
      "         38       0.83      0.83      0.83         6\n",
      "         39       0.75      0.50      0.60         6\n",
      "         40       0.50      0.67      0.57         6\n",
      "         41       0.43      0.50      0.46         6\n",
      "         42       1.00      0.83      0.91         6\n",
      "         43       0.86      1.00      0.92         6\n",
      "         44       0.83      0.83      0.83         6\n",
      "         45       0.50      0.33      0.40         6\n",
      "         46       0.27      0.50      0.35         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       0.67      0.33      0.44         6\n",
      "         49       0.60      0.50      0.55         6\n",
      "         50       0.50      0.67      0.57         6\n",
      "         51       0.00      0.00      0.00         6\n",
      "         52       0.86      1.00      0.92         6\n",
      "         53       0.86      1.00      0.92         6\n",
      "         54       0.83      0.83      0.83         6\n",
      "         55       0.86      1.00      0.92         6\n",
      "         56       1.00      1.00      1.00         6\n",
      "         57       0.62      0.83      0.71         6\n",
      "         58       0.71      0.83      0.77         6\n",
      "         59       0.86      1.00      0.92         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       1.00      1.00      1.00         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       1.00      1.00      1.00         6\n",
      "         65       0.00      0.00      0.00         6\n",
      "         66       0.67      0.67      0.67         6\n",
      "         67       0.80      0.67      0.73         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       0.86      1.00      0.92         6\n",
      "         70       0.83      0.83      0.83         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       0.50      0.33      0.40         6\n",
      "         73       0.25      0.17      0.20         6\n",
      "         74       1.00      0.83      0.91         6\n",
      "         75       0.38      0.83      0.53         6\n",
      "         76       0.57      0.67      0.62         6\n",
      "\n",
      "avg / total       0.77      0.77      0.76       462\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91         6\n",
      "          1       1.00      1.00      1.00         6\n",
      "          2       0.86      1.00      0.92         6\n",
      "          3       1.00      1.00      1.00         6\n",
      "          4       0.80      0.67      0.73         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       0.75      1.00      0.86         6\n",
      "          8       0.33      0.50      0.40         6\n",
      "          9       0.75      1.00      0.86         6\n",
      "         10       0.57      0.67      0.62         6\n",
      "         11       0.67      0.67      0.67         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       1.00      0.67      0.80         6\n",
      "         15       0.00      0.00      0.00         6\n",
      "         16       0.86      1.00      0.92         6\n",
      "         17       0.86      1.00      0.92         6\n",
      "         18       0.86      1.00      0.92         6\n",
      "         19       1.00      1.00      1.00         6\n",
      "         20       0.86      1.00      0.92         6\n",
      "         21       1.00      1.00      1.00         6\n",
      "         22       1.00      0.83      0.91         6\n",
      "         23       1.00      0.17      0.29         6\n",
      "         24       0.38      0.50      0.43         6\n",
      "         25       0.20      0.17      0.18         6\n",
      "         26       0.80      0.67      0.73         6\n",
      "         27       0.50      0.50      0.50         6\n",
      "         28       1.00      0.67      0.80         6\n",
      "         29       0.67      0.67      0.67         6\n",
      "         30       0.86      1.00      0.92         6\n",
      "         31       0.60      0.50      0.55         6\n",
      "         32       1.00      1.00      1.00         6\n",
      "         33       0.50      0.67      0.57         6\n",
      "         34       0.86      1.00      0.92         6\n",
      "         35       0.62      0.83      0.71         6\n",
      "         36       1.00      1.00      1.00         6\n",
      "         37       0.40      0.33      0.36         6\n",
      "         38       1.00      0.83      0.91         6\n",
      "         39       0.80      0.67      0.73         6\n",
      "         40       0.57      0.67      0.62         6\n",
      "         41       0.25      0.33      0.29         6\n",
      "         42       1.00      0.83      0.91         6\n",
      "         43       0.86      1.00      0.92         6\n",
      "         44       0.83      0.83      0.83         6\n",
      "         45       0.67      0.67      0.67         6\n",
      "         46       0.43      0.50      0.46         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       0.50      0.50      0.50         6\n",
      "         49       0.80      0.67      0.73         6\n",
      "         50       0.67      0.67      0.67         6\n",
      "         51       0.33      0.33      0.33         6\n",
      "         52       0.86      1.00      0.92         6\n",
      "         53       0.86      1.00      0.92         6\n",
      "         54       0.83      0.83      0.83         6\n",
      "         55       0.86      1.00      0.92         6\n",
      "         56       1.00      1.00      1.00         6\n",
      "         57       0.67      0.67      0.67         6\n",
      "         58       0.40      0.33      0.36         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       1.00      1.00      1.00         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       1.00      1.00      1.00         6\n",
      "         65       0.29      0.33      0.31         6\n",
      "         66       0.71      0.83      0.77         6\n",
      "         67       0.80      0.67      0.73         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.83      0.83      0.83         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       0.75      0.50      0.60         6\n",
      "         73       0.75      0.50      0.60         6\n",
      "         74       1.00      1.00      1.00         6\n",
      "         75       0.33      0.17      0.22         6\n",
      "         76       0.57      0.67      0.62         6\n",
      "\n",
      "avg / total       0.78      0.77      0.76       462\n",
      "\n",
      "each loop acc 0.766233766234\n",
      "each loop rbf acc 0.770562770563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         6\n",
      "          1       0.83      0.83      0.83         6\n",
      "          2       1.00      1.00      1.00         6\n",
      "          3       0.83      0.83      0.83         6\n",
      "          4       0.83      0.83      0.83         6\n",
      "          5       0.86      1.00      0.92         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       1.00      1.00      1.00         6\n",
      "          8       0.11      0.33      0.16         6\n",
      "          9       0.67      0.67      0.67         6\n",
      "         10       0.00      0.00      0.00         6\n",
      "         11       0.40      1.00      0.57         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       1.00      0.33      0.50         6\n",
      "         15       0.33      0.50      0.40         6\n",
      "         16       1.00      1.00      1.00         6\n",
      "         17       1.00      1.00      1.00         6\n",
      "         18       0.86      1.00      0.92         6\n",
      "         19       0.86      1.00      0.92         6\n",
      "         20       1.00      0.83      0.91         6\n",
      "         21       1.00      1.00      1.00         6\n",
      "         22       1.00      0.83      0.91         6\n",
      "         23       1.00      0.83      0.91         6\n",
      "         24       0.25      0.17      0.20         6\n",
      "         25       0.45      0.83      0.59         6\n",
      "         26       0.86      1.00      0.92         6\n",
      "         27       1.00      0.17      0.29         6\n",
      "         28       1.00      0.67      0.80         6\n",
      "         29       0.60      0.50      0.55         6\n",
      "         30       0.86      1.00      0.92         6\n",
      "         31       0.50      0.50      0.50         6\n",
      "         32       1.00      0.83      0.91         6\n",
      "         33       0.67      0.33      0.44         6\n",
      "         34       1.00      0.83      0.91         6\n",
      "         35       0.60      0.50      0.55         6\n",
      "         36       0.86      1.00      0.92         6\n",
      "         37       0.75      0.50      0.60         6\n",
      "         38       0.83      0.83      0.83         6\n",
      "         39       0.60      0.50      0.55         6\n",
      "         40       0.71      0.83      0.77         6\n",
      "         41       0.67      0.33      0.44         6\n",
      "         42       0.43      0.50      0.46         6\n",
      "         43       1.00      1.00      1.00         6\n",
      "         44       0.86      1.00      0.92         6\n",
      "         45       0.33      0.17      0.22         6\n",
      "         46       0.30      0.50      0.37         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       1.00      0.33      0.50         6\n",
      "         49       1.00      0.67      0.80         6\n",
      "         50       0.43      0.50      0.46         6\n",
      "         51       0.00      0.00      0.00         6\n",
      "         52       0.67      1.00      0.80         6\n",
      "         53       0.40      0.33      0.36         6\n",
      "         54       1.00      0.83      0.91         6\n",
      "         55       0.86      1.00      0.92         6\n",
      "         56       1.00      0.67      0.80         6\n",
      "         57       1.00      0.33      0.50         6\n",
      "         58       0.62      0.83      0.71         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       1.00      1.00      1.00         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       0.86      1.00      0.92         6\n",
      "         65       1.00      0.17      0.29         6\n",
      "         66       0.50      0.33      0.40         6\n",
      "         67       0.71      0.83      0.77         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.67      0.67      0.67         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       0.38      0.50      0.43         6\n",
      "         73       0.36      0.67      0.47         6\n",
      "         74       0.83      0.83      0.83         6\n",
      "         75       0.14      0.33      0.20         6\n",
      "         76       0.33      0.17      0.22         6\n",
      "\n",
      "avg / total       0.76      0.72      0.71       462\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91         6\n",
      "          1       0.80      0.67      0.73         6\n",
      "          2       1.00      1.00      1.00         6\n",
      "          3       0.83      0.83      0.83         6\n",
      "          4       0.75      1.00      0.86         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       0.75      1.00      0.86         6\n",
      "          8       0.25      0.50      0.33         6\n",
      "          9       0.80      0.67      0.73         6\n",
      "         10       0.33      0.33      0.33         6\n",
      "         11       0.62      0.83      0.71         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       1.00      0.67      0.80         6\n",
      "         15       0.44      0.67      0.53         6\n",
      "         16       0.71      0.83      0.77         6\n",
      "         17       1.00      1.00      1.00         6\n",
      "         18       1.00      1.00      1.00         6\n",
      "         19       0.86      1.00      0.92         6\n",
      "         20       0.83      0.83      0.83         6\n",
      "         21       1.00      1.00      1.00         6\n",
      "         22       1.00      1.00      1.00         6\n",
      "         23       1.00      0.83      0.91         6\n",
      "         24       0.33      0.17      0.22         6\n",
      "         25       0.40      1.00      0.57         6\n",
      "         26       0.86      1.00      0.92         6\n",
      "         27       0.83      0.83      0.83         6\n",
      "         28       0.83      0.83      0.83         6\n",
      "         29       0.50      0.50      0.50         6\n",
      "         30       1.00      1.00      1.00         6\n",
      "         31       0.50      0.50      0.50         6\n",
      "         32       1.00      0.83      0.91         6\n",
      "         33       0.43      0.50      0.46         6\n",
      "         34       1.00      0.83      0.91         6\n",
      "         35       0.80      0.67      0.73         6\n",
      "         36       0.86      1.00      0.92         6\n",
      "         37       1.00      0.33      0.50         6\n",
      "         38       1.00      1.00      1.00         6\n",
      "         39       0.60      0.50      0.55         6\n",
      "         40       1.00      0.67      0.80         6\n",
      "         41       0.50      0.17      0.25         6\n",
      "         42       0.43      0.50      0.46         6\n",
      "         43       0.86      1.00      0.92         6\n",
      "         44       0.86      1.00      0.92         6\n",
      "         45       0.50      0.33      0.40         6\n",
      "         46       0.25      0.33      0.29         6\n",
      "         47       1.00      0.83      0.91         6\n",
      "         48       0.44      0.67      0.53         6\n",
      "         49       1.00      0.83      0.91         6\n",
      "         50       0.33      0.17      0.22         6\n",
      "         51       0.00      0.00      0.00         6\n",
      "         52       0.75      1.00      0.86         6\n",
      "         53       0.40      0.33      0.36         6\n",
      "         54       1.00      0.83      0.91         6\n",
      "         55       0.83      0.83      0.83         6\n",
      "         56       1.00      0.67      0.80         6\n",
      "         57       0.83      0.83      0.83         6\n",
      "         58       0.50      0.33      0.40         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       1.00      1.00      1.00         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       0.86      1.00      0.92         6\n",
      "         65       0.50      0.17      0.25         6\n",
      "         66       0.50      0.67      0.57         6\n",
      "         67       0.75      1.00      0.86         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.83      0.83      0.83         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       0.29      0.33      0.31         6\n",
      "         73       0.33      0.17      0.22         6\n",
      "         74       1.00      1.00      1.00         6\n",
      "         75       0.25      0.17      0.20         6\n",
      "         76       0.50      0.50      0.50         6\n",
      "\n",
      "avg / total       0.76      0.74      0.74       462\n",
      "\n",
      "each loop acc 0.718614718615\n",
      "each loop rbf acc 0.742424242424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         6\n",
      "          1       0.86      1.00      0.92         6\n",
      "          2       0.86      1.00      0.92         6\n",
      "          3       1.00      0.83      0.91         6\n",
      "          4       0.80      0.67      0.73         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       0.86      1.00      0.92         6\n",
      "          8       0.30      0.50      0.37         6\n",
      "          9       0.80      0.67      0.73         6\n",
      "         10       1.00      0.67      0.80         6\n",
      "         11       0.80      0.67      0.73         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       0.86      1.00      0.92         6\n",
      "         14       0.71      0.83      0.77         6\n",
      "         15       0.40      0.33      0.36         6\n",
      "         16       0.83      0.83      0.83         6\n",
      "         17       0.86      1.00      0.92         6\n",
      "         18       1.00      1.00      1.00         6\n",
      "         19       1.00      1.00      1.00         6\n",
      "         20       1.00      1.00      1.00         6\n",
      "         21       0.86      1.00      0.92         6\n",
      "         22       0.86      1.00      0.92         6\n",
      "         23       0.83      0.83      0.83         6\n",
      "         24       1.00      0.33      0.50         6\n",
      "         25       0.33      0.33      0.33         6\n",
      "         26       0.86      1.00      0.92         6\n",
      "         27       0.75      0.50      0.60         6\n",
      "         28       1.00      1.00      1.00         6\n",
      "         29       0.71      0.83      0.77         6\n",
      "         30       1.00      1.00      1.00         6\n",
      "         31       0.80      0.67      0.73         6\n",
      "         32       1.00      0.67      0.80         6\n",
      "         33       0.86      1.00      0.92         6\n",
      "         34       1.00      0.67      0.80         6\n",
      "         35       1.00      0.83      0.91         6\n",
      "         36       0.75      1.00      0.86         6\n",
      "         37       0.62      0.83      0.71         6\n",
      "         38       0.80      0.67      0.73         6\n",
      "         39       0.75      1.00      0.86         6\n",
      "         40       0.75      1.00      0.86         6\n",
      "         41       0.40      0.33      0.36         6\n",
      "         42       0.44      0.67      0.53         6\n",
      "         43       1.00      1.00      1.00         6\n",
      "         44       1.00      1.00      1.00         6\n",
      "         45       0.62      0.83      0.71         6\n",
      "         46       0.40      0.33      0.36         6\n",
      "         47       1.00      0.50      0.67         6\n",
      "         48       0.80      0.67      0.73         6\n",
      "         49       0.75      1.00      0.86         6\n",
      "         50       0.62      0.83      0.71         6\n",
      "         51       0.25      0.17      0.20         6\n",
      "         52       1.00      1.00      1.00         6\n",
      "         53       0.60      0.50      0.55         6\n",
      "         54       1.00      1.00      1.00         6\n",
      "         55       1.00      1.00      1.00         6\n",
      "         56       1.00      1.00      1.00         6\n",
      "         57       0.75      1.00      0.86         6\n",
      "         58       1.00      1.00      1.00         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       0.86      1.00      0.92         6\n",
      "         63       1.00      0.83      0.91         6\n",
      "         64       1.00      1.00      1.00         6\n",
      "         65       0.60      0.50      0.55         6\n",
      "         66       0.80      0.67      0.73         6\n",
      "         67       0.83      0.83      0.83         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       0.67      1.00      0.80         6\n",
      "         70       0.75      0.50      0.60         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       1.00      0.50      0.67         6\n",
      "         73       0.60      0.50      0.55         6\n",
      "         74       0.83      0.83      0.83         6\n",
      "         75       0.60      0.50      0.55         6\n",
      "         76       0.86      1.00      0.92         6\n",
      "\n",
      "avg / total       0.82      0.81      0.81       462\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         6\n",
      "          1       0.60      1.00      0.75         6\n",
      "          2       0.86      1.00      0.92         6\n",
      "          3       0.83      0.83      0.83         6\n",
      "          4       1.00      0.17      0.29         6\n",
      "          5       0.86      1.00      0.92         6\n",
      "          6       0.86      1.00      0.92         6\n",
      "          7       0.75      1.00      0.86         6\n",
      "          8       0.57      0.67      0.62         6\n",
      "          9       0.75      0.50      0.60         6\n",
      "         10       0.67      0.67      0.67         6\n",
      "         11       0.71      0.83      0.77         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       0.67      1.00      0.80         6\n",
      "         15       0.50      0.33      0.40         6\n",
      "         16       0.83      0.83      0.83         6\n",
      "         17       0.86      1.00      0.92         6\n",
      "         18       1.00      1.00      1.00         6\n",
      "         19       1.00      0.50      0.67         6\n",
      "         20       1.00      1.00      1.00         6\n",
      "         21       1.00      1.00      1.00         6\n",
      "         22       0.75      1.00      0.86         6\n",
      "         23       0.83      0.83      0.83         6\n",
      "         24       0.67      0.33      0.44         6\n",
      "         25       0.38      0.50      0.43         6\n",
      "         26       0.86      1.00      0.92         6\n",
      "         27       0.62      0.83      0.71         6\n",
      "         28       1.00      1.00      1.00         6\n",
      "         29       0.71      0.83      0.77         6\n",
      "         30       1.00      0.83      0.91         6\n",
      "         31       0.80      0.67      0.73         6\n",
      "         32       1.00      0.67      0.80         6\n",
      "         33       0.75      1.00      0.86         6\n",
      "         34       1.00      0.83      0.91         6\n",
      "         35       0.83      0.83      0.83         6\n",
      "         36       0.75      1.00      0.86         6\n",
      "         37       0.43      0.50      0.46         6\n",
      "         38       0.67      0.67      0.67         6\n",
      "         39       0.67      1.00      0.80         6\n",
      "         40       0.55      1.00      0.71         6\n",
      "         41       0.17      0.17      0.17         6\n",
      "         42       0.62      0.83      0.71         6\n",
      "         43       1.00      1.00      1.00         6\n",
      "         44       1.00      1.00      1.00         6\n",
      "         45       1.00      1.00      1.00         6\n",
      "         46       0.33      0.17      0.22         6\n",
      "         47       1.00      0.50      0.67         6\n",
      "         48       1.00      0.67      0.80         6\n",
      "         49       0.60      1.00      0.75         6\n",
      "         50       0.67      0.67      0.67         6\n",
      "         51       0.50      0.17      0.25         6\n",
      "         52       1.00      1.00      1.00         6\n",
      "         53       0.75      0.50      0.60         6\n",
      "         54       0.86      1.00      0.92         6\n",
      "         55       1.00      1.00      1.00         6\n",
      "         56       1.00      0.83      0.91         6\n",
      "         57       0.86      1.00      0.92         6\n",
      "         58       1.00      0.83      0.91         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       0.86      1.00      0.92         6\n",
      "         63       1.00      0.83      0.91         6\n",
      "         64       1.00      1.00      1.00         6\n",
      "         65       0.67      0.67      0.67         6\n",
      "         66       1.00      0.67      0.80         6\n",
      "         67       0.71      0.83      0.77         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       0.86      1.00      0.92         6\n",
      "         70       1.00      0.50      0.67         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       1.00      0.67      0.80         6\n",
      "         73       1.00      1.00      1.00         6\n",
      "         74       0.80      0.67      0.73         6\n",
      "         75       1.00      0.67      0.80         6\n",
      "         76       1.00      1.00      1.00         6\n",
      "\n",
      "avg / total       0.83      0.81      0.80       462\n",
      "\n",
      "each loop acc 0.813852813853\n",
      "each loop rbf acc 0.811688311688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.67      0.80         6\n",
      "          1       0.83      0.83      0.83         6\n",
      "          2       1.00      1.00      1.00         6\n",
      "          3       0.80      0.67      0.73         6\n",
      "          4       0.71      0.83      0.77         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       0.83      0.83      0.83         6\n",
      "          8       0.29      0.67      0.40         6\n",
      "          9       0.86      1.00      0.92         6\n",
      "         10       1.00      0.50      0.67         6\n",
      "         11       0.67      1.00      0.80         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       0.83      0.83      0.83         6\n",
      "         15       0.71      0.83      0.77         6\n",
      "         16       1.00      0.83      0.91         6\n",
      "         17       0.86      1.00      0.92         6\n",
      "         18       0.86      1.00      0.92         6\n",
      "         19       0.86      1.00      0.92         6\n",
      "         20       1.00      1.00      1.00         6\n",
      "         21       1.00      1.00      1.00         6\n",
      "         22       1.00      1.00      1.00         6\n",
      "         23       1.00      0.67      0.80         6\n",
      "         24       0.75      0.50      0.60         6\n",
      "         25       0.67      0.67      0.67         6\n",
      "         26       1.00      1.00      1.00         6\n",
      "         27       0.43      0.50      0.46         6\n",
      "         28       0.80      0.67      0.73         6\n",
      "         29       0.67      0.67      0.67         6\n",
      "         30       0.67      1.00      0.80         6\n",
      "         31       0.33      0.17      0.22         6\n",
      "         32       0.75      1.00      0.86         6\n",
      "         33       1.00      0.67      0.80         6\n",
      "         34       0.83      0.83      0.83         6\n",
      "         35       0.80      0.67      0.73         6\n",
      "         36       0.80      0.67      0.73         6\n",
      "         37       1.00      0.50      0.67         6\n",
      "         38       1.00      1.00      1.00         6\n",
      "         39       1.00      0.67      0.80         6\n",
      "         40       0.80      0.67      0.73         6\n",
      "         41       0.44      0.67      0.53         6\n",
      "         42       0.75      1.00      0.86         6\n",
      "         43       0.75      1.00      0.86         6\n",
      "         44       0.86      1.00      0.92         6\n",
      "         45       0.50      0.67      0.57         6\n",
      "         46       0.44      0.67      0.53         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       0.80      0.67      0.73         6\n",
      "         49       1.00      0.83      0.91         6\n",
      "         50       0.57      0.67      0.62         6\n",
      "         51       0.50      0.33      0.40         6\n",
      "         52       1.00      1.00      1.00         6\n",
      "         53       1.00      0.83      0.91         6\n",
      "         54       1.00      0.67      0.80         6\n",
      "         55       0.86      1.00      0.92         6\n",
      "         56       1.00      1.00      1.00         6\n",
      "         57       0.80      0.67      0.73         6\n",
      "         58       0.75      1.00      0.86         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      0.83      0.91         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       1.00      1.00      1.00         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       1.00      1.00      1.00         6\n",
      "         65       0.67      0.33      0.44         6\n",
      "         66       0.62      0.83      0.71         6\n",
      "         67       0.83      0.83      0.83         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.71      0.83      0.77         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       0.50      0.50      0.50         6\n",
      "         73       0.20      0.17      0.18         6\n",
      "         74       1.00      1.00      1.00         6\n",
      "         75       0.50      0.17      0.25         6\n",
      "         76       0.60      0.50      0.55         6\n",
      "\n",
      "avg / total       0.82      0.80      0.80       462\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91         6\n",
      "          1       0.83      0.83      0.83         6\n",
      "          2       1.00      1.00      1.00         6\n",
      "          3       1.00      0.50      0.67         6\n",
      "          4       0.56      0.83      0.67         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       0.86      1.00      0.92         6\n",
      "          8       0.50      0.67      0.57         6\n",
      "          9       0.71      0.83      0.77         6\n",
      "         10       0.57      0.67      0.62         6\n",
      "         11       0.86      1.00      0.92         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       1.00      0.83      0.91         6\n",
      "         15       0.50      0.83      0.62         6\n",
      "         16       0.83      0.83      0.83         6\n",
      "         17       0.75      1.00      0.86         6\n",
      "         18       1.00      1.00      1.00         6\n",
      "         19       0.86      1.00      0.92         6\n",
      "         20       1.00      1.00      1.00         6\n",
      "         21       1.00      1.00      1.00         6\n",
      "         22       1.00      1.00      1.00         6\n",
      "         23       1.00      0.50      0.67         6\n",
      "         24       0.67      0.33      0.44         6\n",
      "         25       0.50      0.33      0.40         6\n",
      "         26       1.00      1.00      1.00         6\n",
      "         27       0.50      0.67      0.57         6\n",
      "         28       1.00      0.83      0.91         6\n",
      "         29       0.67      0.67      0.67         6\n",
      "         30       0.75      1.00      0.86         6\n",
      "         31       0.50      0.33      0.40         6\n",
      "         32       0.75      1.00      0.86         6\n",
      "         33       0.67      1.00      0.80         6\n",
      "         34       0.83      0.83      0.83         6\n",
      "         35       0.80      0.67      0.73         6\n",
      "         36       1.00      0.67      0.80         6\n",
      "         37       0.57      0.67      0.62         6\n",
      "         38       0.83      0.83      0.83         6\n",
      "         39       0.83      0.83      0.83         6\n",
      "         40       0.75      0.50      0.60         6\n",
      "         41       0.43      0.50      0.46         6\n",
      "         42       0.75      1.00      0.86         6\n",
      "         43       0.86      1.00      0.92         6\n",
      "         44       0.86      1.00      0.92         6\n",
      "         45       0.62      0.83      0.71         6\n",
      "         46       0.33      0.33      0.33         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       0.67      1.00      0.80         6\n",
      "         49       0.71      0.83      0.77         6\n",
      "         50       0.67      0.33      0.44         6\n",
      "         51       0.17      0.17      0.17         6\n",
      "         52       1.00      1.00      1.00         6\n",
      "         53       1.00      0.67      0.80         6\n",
      "         54       1.00      0.67      0.80         6\n",
      "         55       0.86      1.00      0.92         6\n",
      "         56       1.00      1.00      1.00         6\n",
      "         57       0.86      1.00      0.92         6\n",
      "         58       1.00      0.67      0.80         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      0.83      0.91         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       1.00      1.00      1.00         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       1.00      1.00      1.00         6\n",
      "         65       0.50      0.33      0.40         6\n",
      "         66       0.57      0.67      0.62         6\n",
      "         67       0.83      0.83      0.83         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.75      1.00      0.86         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       0.71      0.83      0.77         6\n",
      "         73       0.00      0.00      0.00         6\n",
      "         74       1.00      1.00      1.00         6\n",
      "         75       0.50      0.17      0.25         6\n",
      "         76       1.00      0.83      0.91         6\n",
      "\n",
      "avg / total       0.81      0.80      0.79       462\n",
      "\n",
      "each loop acc 0.800865800866\n",
      "each loop rbf acc 0.80303030303\n",
      "f1  0.7677699894894217\n",
      "Accuracy: 0.7727272727272727\n",
      "f1  0.7743622810429533\n",
      "Accuracy: 0.7813852813852813\n",
      "here\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAADqCAYAAAClduxYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcc305f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import lsanomaly\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn import utils  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "\n",
    "# import the CSV from http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "# this will return a pandas dataframe.\n",
    "data = pd.read_csv('C:/Users/S/Documents/PY/increased30featureswodevice.csv', low_memory=False)\n",
    "'''data.loc[data['UUID'] == \"RVTNB1502866560357\", \"attack\"] = 1  \n",
    "data.loc[data['UUID'] != \"RVTNB1502866560357\", \"attack\"] = -1\n",
    "df_majority = data[data['attack']==-1]\n",
    "df_minority = data[data['attack']==1]\n",
    "from sklearn.utils import resample\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=830,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#print(data['attack'].value_counts())'''\n",
    "\n",
    "#target=np.array(target)\n",
    "#target = pd.DataFrame(target,columns=['attack'])\n",
    "\n",
    "#data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "categorical_columns=[\"UUID\"]\n",
    "cate_data = data[categorical_columns]\n",
    "\n",
    "#for col in data.columns.values:\n",
    "#    print(col, data[col].unique())\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def label_encode(cate_data, columns):\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        col_values_unique = list(cate_data[col].unique())\n",
    "        le_fitted = le.fit(col_values_unique)\n",
    " \n",
    "        col_values = list(cate_data[col].values)\n",
    "        le.classes_\n",
    "        col_values_transformed = le.transform(col_values)\n",
    "        cate_data[col] = col_values_transformed\n",
    " \n",
    "to_be_encoded_cols = cate_data.columns.values\n",
    "label_encode(cate_data, to_be_encoded_cols)\n",
    "display(cate_data.head())\n",
    "target=cate_data['UUID']\n",
    "target=np.array(target)\n",
    "#target = pd.DataFrame(target)\n",
    "#target=target1.values\n",
    "\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "data=pd.concat([data,cate_data], axis=1)\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "\n",
    "# check the shape for sanity checking.\n",
    "data.shape\n",
    "display(data.head())\n",
    "print(\"initial data info\",data.info())\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"data is\",data.shape)\n",
    "from skfeature.function.information_theoretical_based import LCSI\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "\n",
    "from skfeature.utility.entropy_estimators import *\n",
    "import scipy.io\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#scaled_data=data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaleddata= pd.DataFrame(scaled_data)\n",
    "scaled_data=np.array(scaled_data)\n",
    "\n",
    "print(scaled_data.shape)\n",
    "print(target.shape)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "#display(target.head())\n",
    "#idx=MRMR.mrmr(scaled_data,target,n_selected_features=50)\n",
    "'''from sklearn import cross_validation\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "correct = 0\n",
    "print(\"scaled data details - \",scaled_data.info())\n",
    "print(\"target data details - \",target.info())\n",
    "for train, test in ss:\n",
    "    #print(scaled_data[train])\n",
    "    #print(target[train])\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=50)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:50]]\n",
    "print(features)    '''\n",
    "'''skb= SVC(kernel=\"linear\")\n",
    "rfe = RFE(estimator=skb, n_features_to_select=70)\n",
    "rfe=rfe.fit(scaleddata,target)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "skft = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "for train, test in skft:\n",
    "    X_train,X_test=scaled_data.iloc[train],scaled_data.iloc[test]\n",
    "    Y_train,y_test=target.iloc[train],target.iloc[test]\n",
    "    model1 = svm.OneClassSVM(nu=nu, kernel='rbf', gamma=0.10000000000000001)  \n",
    "    model1.fit(X_train, Y_train)\n",
    "    scores = cross_val_score(model1,X_test,y_test, cv=5, scoring='accuracy')\n",
    "    print(scores)\n",
    "print(scores.mean())'''\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5,random_state=36851234)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "'''clf = svm.SVC(decision_function_shape='ovo',kernel='rbf')    # linear SVM\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=skf)\n",
    "grid.fit(scaled_data, target)\n",
    "print(\"The best parameters are %s with a score of %0.2f\"% (grid.best_params_, grid.best_score_))'''\n",
    "plt.figure(figsize=(8, 8))\n",
    "accuracy = plt.subplot(211)\n",
    "box=plt.subplot(211)\n",
    "from sklearn.svm import LinearSVC\n",
    "clf=SVC(kernel='linear')\n",
    "#clf=LinearSVC()\n",
    "rbf=SVC(decision_function_shape='ovo',gamma=0.001,C=1000000.0)\n",
    "correct = 0\n",
    "fscoreTotal =0\n",
    "rbf_correct = 0\n",
    "rbf_fscoreTotal =0\n",
    "results=[]\n",
    "for train, test in skf.split(scaled_data,target):\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=88)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:88]]\n",
    "        #print(target[train])\n",
    "        # train a classification model with the selected features on the training dataset\n",
    "    clf.fit(features[train], target[train])\n",
    "    #clf1.fit(scaled_data[train],target[train])\n",
    "    rbf.fit(features[train], target[train])\n",
    "        # predict the class labels of test data\n",
    "    y_predict = clf.predict(features[test])\n",
    "    #y_predict = clf1.predict(scaled_data[test])\n",
    "    rbf_y_predict = rbf.predict(features[test])\n",
    "    print(\"metrics\")\n",
    "        # obtain the classification accuracy on the test data\n",
    "    acc = accuracy_score(target[test], y_predict)\n",
    "    rbf_acc = accuracy_score(target[test], rbf_y_predict)\n",
    "    correct = correct + acc\n",
    "    rbf_correct = rbf_correct + rbf_acc\n",
    "    fscore=f1_score(target[test], y_predict,average='weighted')\n",
    "    rbf_fscore=f1_score(target[test], rbf_y_predict,average='weighted')\n",
    "    fscoreTotal=fscoreTotal+fscore\n",
    "    rbf_fscoreTotal=rbf_fscoreTotal+rbf_fscore\n",
    "        #print(\"fsc \",f1_score(target[test], y_predict,average='weighted'))\n",
    "        #print(\"conf mat \",confusion_matrix(target[test],y_predict))\n",
    "        #print(\"ACCURACY: \", (accuracy_score(target[test], y_predict)))\n",
    "    report = classification_report(target[test], y_predict)\n",
    "    print(report)\n",
    "    rbreport = classification_report(target[test], rbf_y_predict)\n",
    "    print(rbreport)\n",
    "    print(\"each loop acc\",acc)\n",
    "    print(\"each loop rbf acc\",rbf_acc)\n",
    "score=float(correct)/5\n",
    "rbfscore=float(rbf_correct)/5\n",
    "results.append(score)\n",
    "results.append(rbfscore)\n",
    "print(\"f1 \",float(fscoreTotal)/5)\n",
    "    # output the average classification accuracy over all 10 folds\n",
    "print(\"Accuracy:\", float(correct)/5)\n",
    "print(\"f1 \",float(rbf_fscoreTotal)/5)\n",
    "    # output the average classification accuracy over all 10 folds\n",
    "print(\"Accuracy:\", float(rbf_correct)/5)\n",
    "\n",
    "accuracy.plot([scaled_data.shape[1], scaled_data.shape[0]],\n",
    "              [score, score], label=\"linear svm\")\n",
    "accuracy.plot([scaled_data.shape[1], scaled_data.shape[0]],\n",
    "              [rbfscore, rbfscore], label=\"rbf svm with grid search C=1000000 and gamma=0.001\")\n",
    "accuracy.set_title(\"Classification accuracy\")\n",
    "accuracy.set_xlim(scaled_data.shape[1], scaled_data.shape[0])\n",
    "accuracy.set_xticks(())\n",
    "accuracy.set_ylim(0.70, 0.80)\n",
    "accuracy.set_ylabel(\"Classification accuracy\")\n",
    "accuracy.legend(loc='best')\n",
    "##svc=SelectKBest(mutual_info_classif, k=50).fit_transform(data,target)\n",
    "#svc = SVC(kernel=\"linear\")\n",
    "#rfe = RFE(estimator=svc, n_features_to_select=10)\n",
    "#rfe.fit(data, target)\n",
    "print(\"here\")\n",
    "box.boxplot(results)\n",
    "box.set_title(\"Classification accuracy\")\n",
    "box.set_xlim(scaled_data.shape[1], scaled_data.shape[0])\n",
    "box.set_xticks(())\n",
    "box.set_ylim(0.70, 0.80)\n",
    "box.set_ylabel(\"Classification accuracy\")\n",
    "box.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UUID\n",
       "0    48\n",
       "1    48\n",
       "2    48\n",
       "3    48\n",
       "4    48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pLN1</th>\n",
       "      <th>p.2</th>\n",
       "      <th>pLN3</th>\n",
       "      <th>pt4</th>\n",
       "      <th>pi5</th>\n",
       "      <th>pe6</th>\n",
       "      <th>pLN7</th>\n",
       "      <th>p58</th>\n",
       "      <th>pLN9</th>\n",
       "      <th>pSH10</th>\n",
       "      <th>...</th>\n",
       "      <th>du2a13</th>\n",
       "      <th>du2n14</th>\n",
       "      <th>du2n15</th>\n",
       "      <th>avgdu</th>\n",
       "      <th>avgud</th>\n",
       "      <th>avgdd</th>\n",
       "      <th>avguu</th>\n",
       "      <th>avdu2</th>\n",
       "      <th>avgp</th>\n",
       "      <th>avga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>37.875</td>\n",
       "      <td>24.466667</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>88.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>670740857</td>\n",
       "      <td>3015</td>\n",
       "      <td>1081</td>\n",
       "      <td>37.625</td>\n",
       "      <td>31.933333</td>\n",
       "      <td>64.066667</td>\n",
       "      <td>63.266667</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2361</td>\n",
       "      <td>1918</td>\n",
       "      <td>884</td>\n",
       "      <td>64.125</td>\n",
       "      <td>453.733333</td>\n",
       "      <td>515.933333</td>\n",
       "      <td>513.133333</td>\n",
       "      <td>575.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1918</td>\n",
       "      <td>1438</td>\n",
       "      <td>827</td>\n",
       "      <td>63.250</td>\n",
       "      <td>347.733333</td>\n",
       "      <td>407.733333</td>\n",
       "      <td>406.400000</td>\n",
       "      <td>466.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>69.375</td>\n",
       "      <td>-9.133333</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pLN1  p.2  pLN3  pt4  pi5  pe6  pLN7  p58  pLN9  pSH10    ...     \\\n",
       "0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "1   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "2   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "3   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "4   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "\n",
       "      du2a13     du2n14  du2n15   avgdu       avgud       avgdd       avguu  \\\n",
       "0       2302  670740857     973  37.875   24.466667   56.800000   55.866667   \n",
       "1  670740857       3015    1081  37.625   31.933333   64.066667   63.266667   \n",
       "2       2361       1918     884  64.125  453.733333  515.933333  513.133333   \n",
       "3       1918       1438     827  63.250  347.733333  407.733333  406.400000   \n",
       "4       2302  670740857     973  69.375   -9.133333   56.800000   55.866667   \n",
       "\n",
       "        avdu2  avgp      avga  \n",
       "0   88.200000   1.0  0.004412  \n",
       "1   95.400000   1.0  0.004167  \n",
       "2  575.333333   1.0  0.008333  \n",
       "3  466.400000   1.0  0.008211  \n",
       "4  121.800000   1.0  0.009804  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2310 entries, 0 to 2309\n",
      "Columns: 147 entries, pLN1 to avga\n",
      "dtypes: float64(71), int64(76)\n",
      "memory usage: 2.6 MB\n",
      "initial data info None\n",
      "data is (2310, 147)\n",
      "(2310, 147)\n",
      "(2310,)\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         6\n",
      "          1       0.55      1.00      0.71         6\n",
      "          2       0.86      1.00      0.92         6\n",
      "          3       1.00      1.00      1.00         6\n",
      "          4       0.75      1.00      0.86         6\n",
      "          5       0.75      1.00      0.86         6\n",
      "          6       1.00      0.67      0.80         6\n",
      "          7       1.00      1.00      1.00         6\n",
      "          8       0.50      0.67      0.57         6\n",
      "          9       1.00      1.00      1.00         6\n",
      "         10       1.00      1.00      1.00         6\n",
      "         11       0.71      0.83      0.77         6\n",
      "         12       0.86      1.00      0.92         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       0.86      1.00      0.92         6\n",
      "         15       0.75      1.00      0.86         6\n",
      "         16       1.00      1.00      1.00         6\n",
      "         17       0.86      1.00      0.92         6\n",
      "         18       0.75      1.00      0.86         6\n",
      "         19       0.86      1.00      0.92         6\n",
      "         20       0.83      0.83      0.83         6\n",
      "         21       0.75      0.50      0.60         6\n",
      "         22       0.86      1.00      0.92         6\n",
      "         23       1.00      1.00      1.00         6\n",
      "         24       1.00      0.50      0.67         6\n",
      "         25       0.83      0.83      0.83         6\n",
      "         26       1.00      1.00      1.00         6\n",
      "         27       0.67      0.67      0.67         6\n",
      "         28       0.86      1.00      0.92         6\n",
      "         29       0.75      1.00      0.86         6\n",
      "         30       1.00      1.00      1.00         6\n",
      "         31       1.00      0.83      0.91         6\n",
      "         32       1.00      1.00      1.00         6\n",
      "         33       0.83      0.83      0.83         6\n",
      "         34       1.00      0.83      0.91         6\n",
      "         35       0.83      0.83      0.83         6\n",
      "         36       0.83      0.83      0.83         6\n",
      "         37       0.71      0.83      0.77         6\n",
      "         38       1.00      1.00      1.00         6\n",
      "         39       0.57      0.67      0.62         6\n",
      "         40       1.00      0.67      0.80         6\n",
      "         41       0.83      0.83      0.83         6\n",
      "         42       0.75      1.00      0.86         6\n",
      "         43       0.86      1.00      0.92         6\n",
      "         44       0.67      0.67      0.67         6\n",
      "         45       0.86      1.00      0.92         6\n",
      "         46       0.86      1.00      0.92         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       1.00      0.83      0.91         6\n",
      "         49       1.00      1.00      1.00         6\n",
      "         50       1.00      0.83      0.91         6\n",
      "         51       0.33      0.17      0.22         6\n",
      "         52       1.00      0.83      0.91         6\n",
      "         53       1.00      0.50      0.67         6\n",
      "         54       1.00      1.00      1.00         6\n",
      "         55       1.00      1.00      1.00         6\n",
      "         56       1.00      1.00      1.00         6\n",
      "         57       0.86      1.00      0.92         6\n",
      "         58       0.83      0.83      0.83         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       1.00      1.00      1.00         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       0.83      0.83      0.83         6\n",
      "         65       0.50      0.33      0.40         6\n",
      "         66       0.67      0.33      0.44         6\n",
      "         67       1.00      0.67      0.80         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       1.00      0.83      0.91         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       0.67      0.67      0.67         6\n",
      "         73       0.83      0.83      0.83         6\n",
      "         74       1.00      0.83      0.91         6\n",
      "         75       0.80      0.67      0.73         6\n",
      "         76       1.00      0.50      0.67         6\n",
      "\n",
      "avg / total       0.88      0.87      0.86       462\n",
      "\n",
      "each loop acc 0.867965367965\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91         6\n",
      "          1       0.75      1.00      0.86         6\n",
      "          2       1.00      0.83      0.91         6\n",
      "          3       1.00      1.00      1.00         6\n",
      "          4       1.00      1.00      1.00         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       0.86      1.00      0.92         6\n",
      "          7       0.75      1.00      0.86         6\n",
      "          8       0.60      1.00      0.75         6\n",
      "          9       0.46      1.00      0.63         6\n",
      "         10       0.83      0.83      0.83         6\n",
      "         11       0.86      1.00      0.92         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       0.67      1.00      0.80         6\n",
      "         15       1.00      1.00      1.00         6\n",
      "         16       0.56      0.83      0.67         6\n",
      "         17       0.75      1.00      0.86         6\n",
      "         18       0.60      1.00      0.75         6\n",
      "         19       0.50      0.50      0.50         6\n",
      "         20       0.83      0.83      0.83         6\n",
      "         21       0.83      0.83      0.83         6\n",
      "         22       0.75      1.00      0.86         6\n",
      "         23       1.00      0.83      0.91         6\n",
      "         24       0.71      0.83      0.77         6\n",
      "         25       1.00      1.00      1.00         6\n",
      "         26       1.00      0.83      0.91         6\n",
      "         27       0.45      0.83      0.59         6\n",
      "         28       0.67      1.00      0.80         6\n",
      "         29       0.83      0.83      0.83         6\n",
      "         30       1.00      1.00      1.00         6\n",
      "         31       0.71      0.83      0.77         6\n",
      "         32       1.00      1.00      1.00         6\n",
      "         33       0.75      0.50      0.60         6\n",
      "         34       0.83      0.83      0.83         6\n",
      "         35       0.80      0.67      0.73         6\n",
      "         36       1.00      1.00      1.00         6\n",
      "         37       0.86      1.00      0.92         6\n",
      "         38       1.00      1.00      1.00         6\n",
      "         39       0.71      0.83      0.77         6\n",
      "         40       0.83      0.83      0.83         6\n",
      "         41       1.00      0.83      0.91         6\n",
      "         42       0.86      1.00      0.92         6\n",
      "         43       0.86      1.00      0.92         6\n",
      "         44       1.00      1.00      1.00         6\n",
      "         45       1.00      0.33      0.50         6\n",
      "         46       0.86      1.00      0.92         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       1.00      0.83      0.91         6\n",
      "         49       1.00      0.83      0.91         6\n",
      "         50       1.00      0.50      0.67         6\n",
      "         51       0.50      0.33      0.40         6\n",
      "         52       1.00      0.67      0.80         6\n",
      "         53       1.00      0.83      0.91         6\n",
      "         54       1.00      0.67      0.80         6\n",
      "         55       1.00      1.00      1.00         6\n",
      "         56       1.00      1.00      1.00         6\n",
      "         57       1.00      0.17      0.29         6\n",
      "         58       0.71      0.83      0.77         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      0.83      0.91         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       1.00      1.00      1.00         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       1.00      0.83      0.91         6\n",
      "         65       1.00      0.50      0.67         6\n",
      "         66       1.00      0.67      0.80         6\n",
      "         67       1.00      0.17      0.29         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.80      0.67      0.73         6\n",
      "         71       0.86      1.00      0.92         6\n",
      "         72       0.67      0.67      0.67         6\n",
      "         73       0.75      0.50      0.60         6\n",
      "         74       1.00      0.67      0.80         6\n",
      "         75       0.80      0.67      0.73         6\n",
      "         76       0.60      0.50      0.55         6\n",
      "\n",
      "avg / total       0.87      0.84      0.83       462\n",
      "\n",
      "each loop acc 0.839826839827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      1.00      0.92         6\n",
      "          1       1.00      1.00      1.00         6\n",
      "          2       1.00      0.67      0.80         6\n",
      "          3       0.86      1.00      0.92         6\n",
      "          4       0.83      0.83      0.83         6\n",
      "          5       0.86      1.00      0.92         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       1.00      1.00      1.00         6\n",
      "          8       0.38      0.83      0.53         6\n",
      "          9       0.75      1.00      0.86         6\n",
      "         10       0.62      0.83      0.71         6\n",
      "         11       0.86      1.00      0.92         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       0.86      1.00      0.92         6\n",
      "         14       0.80      0.67      0.73         6\n",
      "         15       0.83      0.83      0.83         6\n",
      "         16       0.75      1.00      0.86         6\n",
      "         17       1.00      1.00      1.00         6\n",
      "         18       1.00      0.83      0.91         6\n",
      "         19       1.00      0.83      0.91         6\n",
      "         20       1.00      1.00      1.00         6\n",
      "         21       0.86      1.00      0.92         6\n",
      "         22       0.86      1.00      0.92         6\n",
      "         23       0.86      1.00      0.92         6\n",
      "         24       0.67      1.00      0.80         6\n",
      "         25       1.00      1.00      1.00         6\n",
      "         26       1.00      1.00      1.00         6\n",
      "         27       0.67      0.67      0.67         6\n",
      "         28       0.75      1.00      0.86         6\n",
      "         29       0.80      0.67      0.73         6\n",
      "         30       1.00      1.00      1.00         6\n",
      "         31       0.67      1.00      0.80         6\n",
      "         32       1.00      1.00      1.00         6\n",
      "         33       1.00      0.83      0.91         6\n",
      "         34       1.00      1.00      1.00         6\n",
      "         35       0.50      0.67      0.57         6\n",
      "         36       1.00      1.00      1.00         6\n",
      "         37       0.86      1.00      0.92         6\n",
      "         38       1.00      0.83      0.91         6\n",
      "         39       0.67      0.67      0.67         6\n",
      "         40       1.00      0.83      0.91         6\n",
      "         41       1.00      1.00      1.00         6\n",
      "         42       1.00      0.83      0.91         6\n",
      "         43       1.00      1.00      1.00         6\n",
      "         44       1.00      0.67      0.80         6\n",
      "         45       0.86      1.00      0.92         6\n",
      "         46       1.00      0.83      0.91         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       1.00      1.00      1.00         6\n",
      "         49       1.00      0.50      0.67         6\n",
      "         50       1.00      0.67      0.80         6\n",
      "         51       0.60      0.50      0.55         6\n",
      "         52       1.00      1.00      1.00         6\n",
      "         53       0.83      0.83      0.83         6\n",
      "         54       1.00      1.00      1.00         6\n",
      "         55       1.00      1.00      1.00         6\n",
      "         56       1.00      0.83      0.91         6\n",
      "         57       1.00      1.00      1.00         6\n",
      "         58       1.00      1.00      1.00         6\n",
      "         59       1.00      0.83      0.91         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       1.00      1.00      1.00         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       1.00      1.00      1.00         6\n",
      "         65       1.00      0.33      0.50         6\n",
      "         66       0.38      0.50      0.43         6\n",
      "         67       1.00      0.67      0.80         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.50      0.17      0.25         6\n",
      "         71       0.75      1.00      0.86         6\n",
      "         72       0.60      0.50      0.55         6\n",
      "         73       0.40      0.33      0.36         6\n",
      "         74       1.00      0.67      0.80         6\n",
      "         75       0.25      0.17      0.20         6\n",
      "         76       0.67      0.67      0.67         6\n",
      "\n",
      "avg / total       0.87      0.86      0.85       462\n",
      "\n",
      "each loop acc 0.857142857143\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      1.00      0.86         6\n",
      "          1       0.67      1.00      0.80         6\n",
      "          2       0.62      0.83      0.71         6\n",
      "          3       1.00      1.00      1.00         6\n",
      "          4       0.75      1.00      0.86         6\n",
      "          5       0.86      1.00      0.92         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       0.86      1.00      0.92         6\n",
      "          8       0.60      1.00      0.75         6\n",
      "          9       0.80      0.67      0.73         6\n",
      "         10       0.50      0.83      0.62         6\n",
      "         11       0.86      1.00      0.92         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       0.86      1.00      0.92         6\n",
      "         14       0.86      1.00      0.92         6\n",
      "         15       1.00      0.83      0.91         6\n",
      "         16       0.86      1.00      0.92         6\n",
      "         17       1.00      1.00      1.00         6\n",
      "         18       1.00      1.00      1.00         6\n",
      "         19       1.00      0.67      0.80         6\n",
      "         20       1.00      1.00      1.00         6\n",
      "         21       1.00      1.00      1.00         6\n",
      "         22       0.71      0.83      0.77         6\n",
      "         23       0.86      1.00      0.92         6\n",
      "         24       0.80      0.67      0.73         6\n",
      "         25       0.67      1.00      0.80         6\n",
      "         26       0.86      1.00      0.92         6\n",
      "         27       0.56      0.83      0.67         6\n",
      "         28       1.00      1.00      1.00         6\n",
      "         29       1.00      1.00      1.00         6\n",
      "         30       1.00      1.00      1.00         6\n",
      "         31       0.86      1.00      0.92         6\n",
      "         32       0.83      0.83      0.83         6\n",
      "         33       1.00      1.00      1.00         6\n",
      "         34       1.00      0.83      0.91         6\n",
      "         35       0.80      0.67      0.73         6\n",
      "         36       1.00      0.83      0.91         6\n",
      "         37       0.86      1.00      0.92         6\n",
      "         38       1.00      0.83      0.91         6\n",
      "         39       0.67      0.67      0.67         6\n",
      "         40       1.00      0.67      0.80         6\n",
      "         41       1.00      1.00      1.00         6\n",
      "         42       1.00      0.83      0.91         6\n",
      "         43       1.00      1.00      1.00         6\n",
      "         44       1.00      0.83      0.91         6\n",
      "         45       1.00      1.00      1.00         6\n",
      "         46       0.75      0.50      0.60         6\n",
      "         47       0.75      0.50      0.60         6\n",
      "         48       0.83      0.83      0.83         6\n",
      "         49       1.00      1.00      1.00         6\n",
      "         50       0.75      0.50      0.60         6\n",
      "         51       1.00      0.50      0.67         6\n",
      "         52       0.86      1.00      0.92         6\n",
      "         53       1.00      1.00      1.00         6\n",
      "         54       1.00      1.00      1.00         6\n",
      "         55       1.00      1.00      1.00         6\n",
      "         56       1.00      0.67      0.80         6\n",
      "         57       1.00      0.83      0.91         6\n",
      "         58       1.00      0.83      0.91         6\n",
      "         59       1.00      0.83      0.91         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       1.00      1.00      1.00         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       1.00      1.00      1.00         6\n",
      "         65       1.00      0.83      0.91         6\n",
      "         66       1.00      1.00      1.00         6\n",
      "         67       1.00      0.83      0.91         6\n",
      "         68       1.00      0.83      0.91         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       1.00      0.83      0.91         6\n",
      "         71       1.00      0.67      0.80         6\n",
      "         72       0.62      0.83      0.71         6\n",
      "         73       1.00      1.00      1.00         6\n",
      "         74       1.00      1.00      1.00         6\n",
      "         75       0.80      0.67      0.73         6\n",
      "         76       0.67      0.33      0.44         6\n",
      "\n",
      "avg / total       0.90      0.88      0.88       462\n",
      "\n",
      "each loop acc 0.883116883117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.83      0.71         6\n",
      "          1       0.83      0.83      0.83         6\n",
      "          2       0.86      1.00      0.92         6\n",
      "          3       1.00      1.00      1.00         6\n",
      "          4       1.00      1.00      1.00         6\n",
      "          5       0.75      1.00      0.86         6\n",
      "          6       0.86      1.00      0.92         6\n",
      "          7       0.67      1.00      0.80         6\n",
      "          8       0.43      0.50      0.46         6\n",
      "          9       1.00      0.83      0.91         6\n",
      "         10       0.75      1.00      0.86         6\n",
      "         11       1.00      1.00      1.00         6\n",
      "         12       0.67      1.00      0.80         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       0.83      0.83      0.83         6\n",
      "         15       0.67      0.67      0.67         6\n",
      "         16       1.00      1.00      1.00         6\n",
      "         17       1.00      1.00      1.00         6\n",
      "         18       0.67      1.00      0.80         6\n",
      "         19       0.86      1.00      0.92         6\n",
      "         20       0.75      1.00      0.86         6\n",
      "         21       1.00      0.67      0.80         6\n",
      "         22       0.83      0.83      0.83         6\n",
      "         23       1.00      1.00      1.00         6\n",
      "         24       0.71      0.83      0.77         6\n",
      "         25       1.00      1.00      1.00         6\n",
      "         26       0.86      1.00      0.92         6\n",
      "         27       0.71      0.83      0.77         6\n",
      "         28       1.00      1.00      1.00         6\n",
      "         29       0.86      1.00      0.92         6\n",
      "         30       0.86      1.00      0.92         6\n",
      "         31       1.00      0.83      0.91         6\n",
      "         32       1.00      1.00      1.00         6\n",
      "         33       0.86      1.00      0.92         6\n",
      "         34       1.00      0.83      0.91         6\n",
      "         35       0.83      0.83      0.83         6\n",
      "         36       1.00      0.67      0.80         6\n",
      "         37       1.00      1.00      1.00         6\n",
      "         38       1.00      1.00      1.00         6\n",
      "         39       0.67      0.67      0.67         6\n",
      "         40       1.00      1.00      1.00         6\n",
      "         41       1.00      1.00      1.00         6\n",
      "         42       0.86      1.00      0.92         6\n",
      "         43       0.86      1.00      0.92         6\n",
      "         44       1.00      0.67      0.80         6\n",
      "         45       0.86      1.00      0.92         6\n",
      "         46       1.00      0.67      0.80         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       1.00      0.67      0.80         6\n",
      "         49       1.00      1.00      1.00         6\n",
      "         50       1.00      1.00      1.00         6\n",
      "         51       0.71      0.83      0.77         6\n",
      "         52       0.75      0.50      0.60         6\n",
      "         53       1.00      0.50      0.67         6\n",
      "         54       1.00      0.83      0.91         6\n",
      "         55       1.00      1.00      1.00         6\n",
      "         56       1.00      0.83      0.91         6\n",
      "         57       1.00      1.00      1.00         6\n",
      "         58       1.00      0.83      0.91         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       1.00      0.67      0.80         6\n",
      "         63       1.00      0.83      0.91         6\n",
      "         64       1.00      0.83      0.91         6\n",
      "         65       1.00      0.50      0.67         6\n",
      "         66       1.00      1.00      1.00         6\n",
      "         67       1.00      0.83      0.91         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       1.00      0.83      0.91         6\n",
      "         71       1.00      0.83      0.91         6\n",
      "         72       0.67      0.67      0.67         6\n",
      "         73       1.00      1.00      1.00         6\n",
      "         74       1.00      1.00      1.00         6\n",
      "         75       0.57      0.67      0.62         6\n",
      "         76       0.80      0.67      0.73         6\n",
      "\n",
      "avg / total       0.90      0.89      0.88       462\n",
      "\n",
      "each loop acc 0.885281385281\n",
      "random f1  0.8628842454995403\n",
      "random Accuracy: 0.8666666666666666\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import lsanomaly\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn import utils  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "\n",
    "# import the CSV from http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "# this will return a pandas dataframe.\n",
    "data = pd.read_csv('C:/Users/S/Documents/PY/increased30featureswodevice.csv', low_memory=False)\n",
    "'''data.loc[data['UUID'] == \"RVTNB1502866560357\", \"attack\"] = 1  \n",
    "data.loc[data['UUID'] != \"RVTNB1502866560357\", \"attack\"] = -1\n",
    "df_majority = data[data['attack']==-1]\n",
    "df_minority = data[data['attack']==1]\n",
    "from sklearn.utils import resample\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=830,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#print(data['attack'].value_counts())'''\n",
    "\n",
    "#target=np.array(target)\n",
    "#target = pd.DataFrame(target,columns=['attack'])\n",
    "\n",
    "#data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "categorical_columns=[\"UUID\"]\n",
    "cate_data = data[categorical_columns]\n",
    "\n",
    "#for col in data.columns.values:\n",
    "#    print(col, data[col].unique())\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def label_encode(cate_data, columns):\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        col_values_unique = list(cate_data[col].unique())\n",
    "        le_fitted = le.fit(col_values_unique)\n",
    " \n",
    "        col_values = list(cate_data[col].values)\n",
    "        le.classes_\n",
    "        col_values_transformed = le.transform(col_values)\n",
    "        cate_data[col] = col_values_transformed\n",
    " \n",
    "to_be_encoded_cols = cate_data.columns.values\n",
    "label_encode(cate_data, to_be_encoded_cols)\n",
    "display(cate_data.head())\n",
    "target=cate_data['UUID']\n",
    "target=np.array(target)\n",
    "#target = pd.DataFrame(target)\n",
    "#target=target1.values\n",
    "\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "data=pd.concat([data,cate_data], axis=1)\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "\n",
    "# check the shape for sanity checking.\n",
    "data.shape\n",
    "display(data.head())\n",
    "print(\"initial data info\",data.info())\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"data is\",data.shape)\n",
    "from skfeature.function.information_theoretical_based import LCSI\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "\n",
    "from skfeature.utility.entropy_estimators import *\n",
    "import scipy.io\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#scaled_data=data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaleddata= pd.DataFrame(scaled_data)\n",
    "scaled_data=np.array(scaled_data)\n",
    "\n",
    "print(scaled_data.shape)\n",
    "print(target.shape)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "#display(target.head())\n",
    "#idx=MRMR.mrmr(scaled_data,target,n_selected_features=50)\n",
    "'''from sklearn import cross_validation\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "correct = 0\n",
    "print(\"scaled data details - \",scaled_data.info())\n",
    "print(\"target data details - \",target.info())\n",
    "for train, test in ss:\n",
    "    #print(scaled_data[train])\n",
    "    #print(target[train])\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=50)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:50]]\n",
    "print(features)    '''\n",
    "'''skb= SVC(kernel=\"linear\")\n",
    "rfe = RFE(estimator=skb, n_features_to_select=70)\n",
    "rfe=rfe.fit(scaleddata,target)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "skft = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "for train, test in skft:\n",
    "    X_train,X_test=scaled_data.iloc[train],scaled_data.iloc[test]\n",
    "    Y_train,y_test=target.iloc[train],target.iloc[test]\n",
    "    model1 = svm.OneClassSVM(nu=nu, kernel='rbf', gamma=0.10000000000000001)  \n",
    "    model1.fit(X_train, Y_train)\n",
    "    scores = cross_val_score(model1,X_test,y_test, cv=5, scoring='accuracy')\n",
    "    print(scores)\n",
    "print(scores.mean())'''\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5,random_state=36851234)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "''''from sklearn.model_selection import GridSearchCV\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "clf = svm.SVC(decision_function_shape='ovo',kernel='rbf')    # linear SVM\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=skf)\n",
    "grid.fit(scaled_data, target)\n",
    "print(\"The best parameters are %s with a score of %0.2f\"% (grid.best_params_, grid.best_score_))'''\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "clf=RandomForestClassifier(n_estimators=3,class_weight='balanced')\n",
    "\n",
    "\n",
    "correct = 0\n",
    "fscoreTotal =0\n",
    "\n",
    "results=[]\n",
    "for train, test in skf.split(scaled_data,target):\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=88)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:88]]\n",
    "    \n",
    "        # train a classification model with the selected features on the training dataset\n",
    "    clf.fit(features[train], target[train])\n",
    "    #clf1.fit(scaled_data[train],target[train])\n",
    "    \n",
    "        # predict the class labels of test data\n",
    "    y_predict = clf.predict(features[test])\n",
    "    #y_predict = clf1.predict(scaled_data[test])\n",
    "    \n",
    "    print(\"metrics\")\n",
    "        # obtain the classification accuracy on the test data\n",
    "    acc = accuracy_score(target[test], y_predict)\n",
    "    \n",
    "    correct = correct + acc\n",
    "    \n",
    "    fscore=f1_score(target[test], y_predict,average='weighted')\n",
    "    \n",
    "    fscoreTotal=fscoreTotal+fscore\n",
    "    \n",
    "        #print(\"fsc \",f1_score(target[test], y_predict,average='weighted'))\n",
    "        #print(\"conf mat \",confusion_matrix(target[test],y_predict))\n",
    "        #print(\"ACCURACY: \", (accuracy_score(target[test], y_predict)))\n",
    "    report = classification_report(target[test], y_predict)\n",
    "    print(report)\n",
    "    \n",
    "    print(\"each loop acc\",acc)\n",
    "   \n",
    "score=float(correct)/5\n",
    "\n",
    "results.append(score)\n",
    "\n",
    "print(\"random f1 \",float(fscoreTotal)/5)\n",
    "    # output the average classification accuracy over all 10 folds\n",
    "print(\"random Accuracy:\", float(correct)/5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
