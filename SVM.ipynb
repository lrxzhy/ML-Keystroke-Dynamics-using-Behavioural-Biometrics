{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0    821\n",
      " 1.0     45\n",
      "Name: attack, dtype: int64\n",
      "outliers.shape (821,)\n",
      "outlier fraction 0.9480369515011547\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 866 entries, 0 to 865\n",
      "Data columns (total 9 columns):\n",
      "Touch_Pressure      866 non-null float64\n",
      "Touch_Size          866 non-null float64\n",
      "X_Coordinate        866 non-null float64\n",
      "Y_Coordinate        866 non-null float64\n",
      "X_Precision         866 non-null float64\n",
      "Y_Precision         866 non-null float64\n",
      "Action_Timestamp    866 non-null float64\n",
      "Button              866 non-null float64\n",
      "Action_Type         866 non-null float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 61.0 KB\n",
      "None\n",
      "nu 0.9480369515011547\n",
      "accuracy:  0.897398843931\n",
      "precision:  0.0571428571429\n",
      "recall:  0.05\n",
      "f1:  0.0533333333333\n",
      "area under curve (auc):  0.499693251534\n",
      "accuracy:  0.913793103448\n",
      "precision:  0.0\n",
      "recall:  0.0\n",
      "f1:  0.0\n",
      "area under curve (auc):  0.470414201183\n",
      "(133, 152) (133, 152)\n",
      "0.985590778098\n",
      "(133, 152)\n",
      "0.953890489914\n",
      "(133, 152)\n",
      "0.99711815562\n",
      "(133, 152)\n",
      "0.99711815562\n",
      "(133, 152)\n",
      "0.99711815562\n",
      "(133, 152)\n",
      "0.99711815562\n",
      "(133, 152)\n",
      "0.959654178674\n",
      "(133, 152)\n",
      "0.965417867435\n",
      "(133, 152)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB5AAAAL+CAYAAABrHIg4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n56PqlQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x223fff8b748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import lsanomaly\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn import utils  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=2),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "# import the CSV from http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "# this will return a pandas dataframe.\n",
    "data = pd.read_csv('C:/Users/user/.spyder-py3/loopdata.csv', low_memory=False)\n",
    "\n",
    "\n",
    "#relevant_features = [  \n",
    "#    \"UUID\",\n",
    "#    \"X_Coordinate\",\n",
    "#    \"Y_Coordinate\",\n",
    "#]\n",
    "relevant_features = [  \n",
    "    \"UUID\",\n",
    "    \"Touch_Pressure\",\n",
    "    \"Touch_Size\",\n",
    "    \"Button\",\n",
    "    \"Action_Type\",\n",
    "    \"X_Coordinate\",\n",
    "    \"Y_Coordinate\",\n",
    "    \"X_Precision\",\n",
    "    \"Y_Precision\",\n",
    "    \"Action_Timestamp\",\n",
    "]\n",
    "data = data[relevant_features]\n",
    "data.loc[data['UUID'] == \"AEVXC1499703691514\", \"attack\"] = 1  \n",
    "data.loc[data['UUID'] != \"AEVXC1499703691514\", \"attack\"] = -1\n",
    "\n",
    "target = data['attack']\n",
    "print(data['attack'].value_counts())\n",
    "\n",
    "data.drop([\"UUID\", \"attack\"], axis=1, inplace=True)\n",
    "data=data.loc[:, 'Touch_Pressure':]\n",
    "\n",
    "#data.drop([\"HR_Timestamp\",\"Language\",\"Hardware_Model\",\"SDK_Version\",\"Manufacture\",\"Screen_Size\",\"Time_Zone\",\"Date_Time\",\"Country_Code\",\"Location\",\"Num_of_CPU_Cores\",\"Location_lat\",\"Location_long\"], axis=1, inplace=True)\n",
    "\n",
    "categorical_columns=[\"Button\",\"Action_Type\"]\n",
    "cate_data = data[categorical_columns]\n",
    "\n",
    "#for col in data.columns.values:\n",
    "#    print(col, data[col].unique())\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def label_encode(cate_data, columns):\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        col_values_unique = list(cate_data[col].unique())\n",
    "        le_fitted = le.fit(col_values_unique)\n",
    " \n",
    "        col_values = list(cate_data[col].values)\n",
    "        le.classes_\n",
    "        col_values_transformed = le.transform(col_values)\n",
    "        cate_data[col] = col_values_transformed\n",
    " \n",
    "to_be_encoded_cols = cate_data.columns.values\n",
    "label_encode(cate_data, to_be_encoded_cols)\n",
    "#display(cate_data.head())\n",
    "\n",
    "data.drop([\"Button\",\"Action_Type\"], axis=1, inplace=True)\n",
    "data=pd.concat([data,cate_data], axis=1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "data[\"Touch_Pressure\"] = np.log((data[\"Touch_Pressure\"] + 0.1).astype(float))  \n",
    "data[\"Touch_Size\"] = np.log((data[\"Touch_Size\"] + 0.1).astype(float))  \n",
    "data[\"X_Coordinate\"] = np.log((data[\"X_Coordinate\"] + 0.1).astype(float))\n",
    "data[\"Y_Coordinate\"] = np.log((data[\"Y_Coordinate\"] + 0.1).astype(float)) \n",
    "data[\"X_Precision\"] = np.log((data[\"X_Precision\"] + 0.1).astype(float)) \n",
    "data[\"Y_Precision\"] = np.log((data[\"Y_Precision\"] + 0.1).astype(float)) \n",
    "data[\"Button\"] = np.log((data[\"Button\"] + 0.1).astype(float))\n",
    "data[\"Action_Type\"] = np.log((data[\"Action_Type\"] + 0.1).astype(float))\n",
    "data[\"Action_Timestamp\"] = np.log(((data[\"Y_Precision\"]*0.0000001) + 0.1).astype(float))\n",
    "\n",
    "outliers = target[target == -1]  \n",
    "\n",
    "print(\"outliers.shape\", outliers.shape)  \n",
    "print(\"outlier fraction\", outliers.shape[0]/target.shape[0])\n",
    "\n",
    "# check the shape for sanity checking.\n",
    "data.shape\n",
    "print(data.info())\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "train_data, test_data, train_target, test_target = train_test_split(data, target, train_size = 0.8)  \n",
    "train_data.shape\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "# set nu (which should be the proportion of outliers in our dataset)\n",
    "nu = outliers.shape[0] / target.shape[0]  \n",
    "print(\"nu\", nu)\n",
    "\n",
    "#model=RandomForestClassifier()\n",
    "#model.fit(train_data,train_target) \n",
    "\n",
    "model = svm.OneClassSVM(nu=nu, kernel='rbf', gamma='auto')  \n",
    "model.fit(train_data) \n",
    "\n",
    "from sklearn import metrics  \n",
    "preds = model.predict(train_data)  \n",
    "targs = train_target\n",
    "\n",
    "\n",
    "print(\"accuracy: \", metrics.accuracy_score(targs, preds))  \n",
    "print(\"precision: \", metrics.precision_score(targs, preds))  \n",
    "print(\"recall: \", metrics.recall_score(targs, preds))  \n",
    "print(\"f1: \", metrics.f1_score(targs, preds))  \n",
    "print(\"area under curve (auc): \", metrics.roc_auc_score(targs, preds))\n",
    "\n",
    "preds = model.predict(test_data)  \n",
    "targs = test_target\n",
    "\n",
    "print(\"accuracy: \", metrics.accuracy_score(targs, preds))  \n",
    "print(\"precision: \", metrics.precision_score(targs, preds))  \n",
    "print(\"recall: \", metrics.recall_score(targs, preds))  \n",
    "print(\"f1: \", metrics.f1_score(targs, preds))  \n",
    "print(\"area under curve (auc): \", metrics.roc_auc_score(targs, preds))\n",
    "n_components = 2\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=10)\n",
    "reduced_data= ipca.fit_transform(scaled_data)\n",
    "\n",
    "\n",
    "XYdata=(reduced_data,target)\n",
    "datasets=[XYdata]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    #X, y = ds\n",
    "    #X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reduced_data, target, test_size=.4, random_state=42)\n",
    "\n",
    "    X=np.array(data)\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "    print(xx.shape,yy.shape)\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    #print(len(datasets), len(classifiers))\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,edgecolors='k')\n",
    "        # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4,edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i+=1\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred=clf.predict(X_test)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print(score)\n",
    "            # Plot the decision boundary. For that, we will assign a color to each\n",
    "            # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "            #Z = clf.decision_function(X_test)\n",
    "        #else:\n",
    "            #Z = clf.predict_proba(X_test)\n",
    "        #print(X_test.shape)\n",
    "            # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        print(Z.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "            # Plot also the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,edgecolors='k')\n",
    "            # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,edgecolors='k', alpha=0.4)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(.4, .4, ('%.2f' % score).lstrip('0'),size=15, horizontalalignment='right')\n",
    "        i+=1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "270\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgIAAAHwCAYAAADU9wdDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d7149ebcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, pipeline\n",
    "\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits(n_class=9)\n",
    "\n",
    "# To apply an classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.data)\n",
    "data = digits.data / 16.\n",
    "data -= data.mean(axis=0)\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "data_train, targets_train = (data[:n_samples // 2],\n",
    "                             digits.target[:n_samples // 2])\n",
    "\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "data_test, targets_test = (data[n_samples // 2:],\n",
    "                           digits.target[n_samples // 2:])\n",
    "# data_test = scaler.transform(data_test)\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "kernel_svm = svm.SVC(gamma=.2)\n",
    "linear_svm = svm.LinearSVC()\n",
    "\n",
    "\n",
    "\n",
    "# fit and predict using linear and kernel svm:\n",
    "\n",
    "kernel_svm_time = time()\n",
    "kernel_svm.fit(data_train, targets_train)\n",
    "kernel_svm_score = kernel_svm.score(data_test, targets_test)\n",
    "kernel_svm_time = time() - kernel_svm_time\n",
    "\n",
    "linear_svm_time = time()\n",
    "linear_svm.fit(data_train, targets_train)\n",
    "linear_svm_score = linear_svm.score(data_test, targets_test)\n",
    "linear_svm_time = time() - linear_svm_time\n",
    "\n",
    "sample_sizes = 30 * np.arange(1, 10)\n",
    "\n",
    "\n",
    "# plot the results:\n",
    "plt.figure(figsize=(8, 8))\n",
    "accuracy = plt.subplot(211)\n",
    "# second y axis for timeings\n",
    "timescale = plt.subplot(212)\n",
    "\n",
    "print(sample_sizes[0])\n",
    "print(sample_sizes[-1])\n",
    "\n",
    "# horizontal lines for exact rbf and linear kernels:\n",
    "accuracy.plot([sample_sizes[0], sample_sizes[-1]],\n",
    "              [linear_svm_score, linear_svm_score], label=\"linear svm\")\n",
    "timescale.plot([sample_sizes[0], sample_sizes[-1]],\n",
    "               [linear_svm_time, linear_svm_time], '--', label='linear svm')\n",
    "\n",
    "accuracy.plot([sample_sizes[0], sample_sizes[-1]],\n",
    "              [kernel_svm_score, kernel_svm_score], label=\"rbf svm\")\n",
    "timescale.plot([sample_sizes[0], sample_sizes[-1]],\n",
    "               [kernel_svm_time, kernel_svm_time], '--', label='rbf svm')\n",
    "\n",
    "# vertical line for dataset dimensionality = 64\n",
    "accuracy.plot([64, 64], [0.7, 1], label=\"n_features\")\n",
    "\n",
    "# legends and labels\n",
    "accuracy.set_title(\"Classification accuracy\")\n",
    "timescale.set_title(\"Training times\")\n",
    "accuracy.set_xlim(sample_sizes[0], sample_sizes[-1])\n",
    "accuracy.set_xticks(())\n",
    "accuracy.set_ylim(0.8, 1)\n",
    "timescale.set_xlabel(\"Sampling steps = transformed feature dimension\")\n",
    "accuracy.set_ylabel(\"Classification accuracy\")\n",
    "timescale.set_ylabel(\"Training time in seconds\")\n",
    "accuracy.legend(loc='best')\n",
    "timescale.legend(loc='best')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "      <th>Language</th>\n",
       "      <th>Hardware_Model</th>\n",
       "      <th>SDK_Version</th>\n",
       "      <th>Manufacture</th>\n",
       "      <th>Screen_Size</th>\n",
       "      <th>Time_Zone</th>\n",
       "      <th>Country_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UUID  Language  Hardware_Model  SDK_Version  Manufacture  Screen_Size  \\\n",
       "0    48         0              40            3           18           23   \n",
       "1    48         0              40            3           18           23   \n",
       "2    48         0              40            3           18           23   \n",
       "3    48         0              40            3           18           23   \n",
       "4    48         0              40            3           18           23   \n",
       "\n",
       "   Time_Zone  Country_Code  \n",
       "0          7             1  \n",
       "1          7             1  \n",
       "2          7             1  \n",
       "3          7             1  \n",
       "4          7             1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num_of_CPU_Cores</th>\n",
       "      <th>pLN1</th>\n",
       "      <th>p.2</th>\n",
       "      <th>pLN3</th>\n",
       "      <th>pt4</th>\n",
       "      <th>pi5</th>\n",
       "      <th>pe6</th>\n",
       "      <th>pLN7</th>\n",
       "      <th>p58</th>\n",
       "      <th>pLN9</th>\n",
       "      <th>...</th>\n",
       "      <th>avdu2</th>\n",
       "      <th>avgp</th>\n",
       "      <th>avga</th>\n",
       "      <th>Language</th>\n",
       "      <th>Hardware_Model</th>\n",
       "      <th>SDK_Version</th>\n",
       "      <th>Manufacture</th>\n",
       "      <th>Screen_Size</th>\n",
       "      <th>Time_Zone</th>\n",
       "      <th>Country_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>88.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>575.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>466.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Num_of_CPU_Cores  pLN1  p.2  pLN3  pt4  pi5  pe6  pLN7  p58  pLN9  \\\n",
       "0               8.0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0   \n",
       "1               8.0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0   \n",
       "2               8.0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0   \n",
       "3               8.0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0   \n",
       "4               8.0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0   \n",
       "\n",
       "       ...            avdu2  avgp      avga  Language  Hardware_Model  \\\n",
       "0      ...        88.200000   1.0  0.004412         0              40   \n",
       "1      ...        95.400000   1.0  0.004167         0              40   \n",
       "2      ...       575.333333   1.0  0.008333         0              40   \n",
       "3      ...       466.400000   1.0  0.008211         0              40   \n",
       "4      ...       121.800000   1.0  0.009804         0              40   \n",
       "\n",
       "   SDK_Version  Manufacture  Screen_Size  Time_Zone  Country_Code  \n",
       "0            3           18           23          7             1  \n",
       "1            3           18           23          7             1  \n",
       "2            3           18           23          7             1  \n",
       "3            3           18           23          7             1  \n",
       "4            3           18           23          7             1  \n",
       "\n",
       "[5 rows x 155 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 770 entries, 0 to 769\n",
      "Columns: 155 entries, Num_of_CPU_Cores to Country_Code\n",
      "dtypes: float64(148), int64(7)\n",
      "memory usage: 932.5 KB\n",
      "initial data info None\n",
      "data is (770, 155)\n",
      "(770, 155)\n",
      "(770,)\n",
      "metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "f1  0.7183384869099154\n",
      "Accuracy: 0.7636363636363637\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import lsanomaly\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn import utils  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "\n",
    "# import the CSV from http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "# this will return a pandas dataframe.\n",
    "data = pd.read_csv('C:/Users/user/.spyder-py3/features1.csv', low_memory=False)\n",
    "'''data.loc[data['UUID'] == \"RVTNB1502866560357\", \"attack\"] = 1  \n",
    "data.loc[data['UUID'] != \"RVTNB1502866560357\", \"attack\"] = -1\n",
    "df_majority = data[data['attack']==-1]\n",
    "df_minority = data[data['attack']==1]\n",
    "from sklearn.utils import resample\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=830,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#print(data['attack'].value_counts())'''\n",
    "\n",
    "#target=np.array(target)\n",
    "#target = pd.DataFrame(target,columns=['attack'])\n",
    "\n",
    "#data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "categorical_columns=[\"UUID\",\"Language\",\"Hardware_Model\",\"SDK_Version\",\"Manufacture\",\"Screen_Size\",\"Time_Zone\",\"Country_Code\"]\n",
    "cate_data = data[categorical_columns]\n",
    "\n",
    "#for col in data.columns.values:\n",
    "#    print(col, data[col].unique())\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def label_encode(cate_data, columns):\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        col_values_unique = list(cate_data[col].unique())\n",
    "        le_fitted = le.fit(col_values_unique)\n",
    " \n",
    "        col_values = list(cate_data[col].values)\n",
    "        le.classes_\n",
    "        col_values_transformed = le.transform(col_values)\n",
    "        cate_data[col] = col_values_transformed\n",
    " \n",
    "to_be_encoded_cols = cate_data.columns.values\n",
    "label_encode(cate_data, to_be_encoded_cols)\n",
    "display(cate_data.head())\n",
    "target=cate_data['UUID']\n",
    "target=np.array(target)\n",
    "#target = pd.DataFrame(target)\n",
    "#target=target1.values\n",
    "\n",
    "data.drop([\"UUID\",\"Language\",\"Hardware_Model\",\"SDK_Version\",\"Manufacture\",\"Screen_Size\",\"Time_Zone\",\"Country_Code\"], axis=1, inplace=True)\n",
    "data=pd.concat([data,cate_data], axis=1)\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "\n",
    "# check the shape for sanity checking.\n",
    "data.shape\n",
    "display(data.head())\n",
    "print(\"initial data info\",data.info())\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"data is\",data.shape)\n",
    "from skfeature.function.information_theoretical_based import LCSI\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "\n",
    "from skfeature.utility.entropy_estimators import *\n",
    "import scipy.io\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#scaled_data=data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaleddata= pd.DataFrame(scaled_data)\n",
    "scaled_data=np.array(scaled_data)\n",
    "\n",
    "print(scaled_data.shape)\n",
    "print(target.shape)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "#display(target.head())\n",
    "#idx=MRMR.mrmr(scaled_data,target,n_selected_features=50)\n",
    "'''from sklearn import cross_validation\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "correct = 0\n",
    "print(\"scaled data details - \",scaled_data.info())\n",
    "print(\"target data details - \",target.info())\n",
    "for train, test in ss:\n",
    "    #print(scaled_data[train])\n",
    "    #print(target[train])\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=50)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:50]]\n",
    "print(features)    '''\n",
    "'''skb= SVC(kernel=\"linear\")\n",
    "rfe = RFE(estimator=skb, n_features_to_select=70)\n",
    "rfe=rfe.fit(scaleddata,target)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "skft = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "for train, test in skft:\n",
    "    X_train,X_test=scaled_data.iloc[train],scaled_data.iloc[test]\n",
    "    Y_train,y_test=target.iloc[train],target.iloc[test]\n",
    "    model1 = svm.OneClassSVM(nu=nu, kernel='rbf', gamma=0.10000000000000001)  \n",
    "    model1.fit(X_train, Y_train)\n",
    "    scores = cross_val_score(model1,X_test,y_test, cv=5, scoring='accuracy')\n",
    "    print(scores)\n",
    "print(scores.mean())'''\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5,random_state=36851234)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "'''\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "cval = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cval)\n",
    "grid.fit(scaled_data, target)\n",
    "print(\"The best parameters are %s with a score of %0.2f\"% (grid.best_params_, grid.best_score_))\n",
    "'''\n",
    "clf = svm.SVC(decision_function_shape='ovo',gamma=0.000001,kernel='rbf')    # linear SVM\n",
    "correct = 0\n",
    "fscoreTotal =0\n",
    "for train, test in skf.split(scaled_data,target):\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=67)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:67]]\n",
    "        #print(target[train])\n",
    "        # train a classification model with the selected features on the training dataset\n",
    "    clf.fit(features[train], target[train])\n",
    "\n",
    "        # predict the class labels of test data\n",
    "    y_predict = clf.predict(features[test])\n",
    "    print(\"metrics\")\n",
    "        # obtain the classification accuracy on the test data\n",
    "    acc = accuracy_score(target[test], y_predict)\n",
    "    correct = correct + acc\n",
    "    fscore=f1_score(target[test], y_predict,average='weighted')\n",
    "    fscoreTotal=fscoreTotal+fscore\n",
    "        #print(\"fsc \",f1_score(target[test], y_predict,average='weighted'))\n",
    "        #print(\"conf mat \",confusion_matrix(target[test],y_predict))\n",
    "        #print(\"ACCURACY: \", (accuracy_score(target[test], y_predict)))\n",
    "        #report = classification_report(target[test], y_predict)\n",
    "        #print(report)\n",
    "print(\"f1 \",float(fscoreTotal)/5)\n",
    "# output the average classification accuracy over all 10 folds\n",
    "print(\"Accuracy:\", float(correct)/5)\n",
    "    \n",
    "##svc=SelectKBest(mutual_info_classif, k=50).fit_transform(data,target)\n",
    "#svc = SVC(kernel=\"linear\")\n",
    "#rfe = RFE(estimator=svc, n_features_to_select=10)\n",
    "#rfe.fit(data, target)\n",
    "print(\"here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Button</th>\n",
       "      <th>Touch_Pressure</th>\n",
       "      <th>Touch_Size</th>\n",
       "      <th>X_Coordinate</th>\n",
       "      <th>Y_Coordinate</th>\n",
       "      <th>X_Precision</th>\n",
       "      <th>Y_Precision</th>\n",
       "      <th>Action_Type</th>\n",
       "      <th>Action_Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145304</td>\n",
       "      <td>SHIFT</td>\n",
       "      <td>0.180392</td>\n",
       "      <td>0.180392</td>\n",
       "      <td>72.0</td>\n",
       "      <td>977.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>27554780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145305</td>\n",
       "      <td>NUMBERS</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Down</td>\n",
       "      <td>27555580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>145306</td>\n",
       "      <td>LETTERS</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>27555680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>145307</td>\n",
       "      <td>.</td>\n",
       "      <td>0.180392</td>\n",
       "      <td>0.180392</td>\n",
       "      <td>172.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Down</td>\n",
       "      <td>27556523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145308</td>\n",
       "      <td>z</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>166.0</td>\n",
       "      <td>968.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Down</td>\n",
       "      <td>27556523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID   Button  Touch_Pressure  Touch_Size  X_Coordinate  Y_Coordinate  \\\n",
       "0  145304    SHIFT        0.180392    0.180392          72.0         977.0   \n",
       "1  145305  NUMBERS        0.184314    0.184314          97.0        1072.0   \n",
       "2  145306  LETTERS        0.184314    0.184314          97.0        1072.0   \n",
       "3  145307        .        0.180392    0.180392         172.0         972.0   \n",
       "4  145308        z        0.184314    0.184314         166.0         968.5   \n",
       "\n",
       "   X_Precision  Y_Precision Action_Type  Action_Timestamp  \n",
       "0          1.0          1.0          Up          27554780  \n",
       "1          1.0          1.0        Down          27555580  \n",
       "2          1.0          1.0          Up          27555680  \n",
       "3          1.0          1.0        Down          27556523  \n",
       "4          1.0          1.0        Down          27556523  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Button</th>\n",
       "      <th>Action_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Button  Action_Type\n",
       "0       9            1\n",
       "1       6            0\n",
       "2       5            1\n",
       "3       0            0\n",
       "4      19            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import lsanomaly\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn import utils  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.core.display import display\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=2),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "# import the CSV from http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "# this will return a pandas dataframe.\n",
    "data = pd.read_csv('C:/Users/user/.spyder-py3/loopdata.csv', low_memory=False)\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "data.loc[data['UUID'] == \"AEVXC1499703691514\", \"attack\"] = 1  \n",
    "data.loc[data['UUID'] != \"AEVXC1499703691514\", \"attack\"] = -1\n",
    "\n",
    "target = data['attack']\n",
    "data.drop([\"UUID\", \"attack\"], axis=1, inplace=True)\n",
    "data.drop([\"HR_Timestamp\",\"Language\",\"Hardware_Model\",\"SDK_Version\",\"Manufacture\",\"Screen_Size\",\"Time_Zone\",\"Date_Time\",\"Country_Code\",\"Location\",\"Num_of_CPU_Cores\",\"Location_lat\",\"Location_long\"], axis=1, inplace=True)\n",
    "display(data.head())\n",
    "\n",
    "categorical_columns=[\"Button\",\"Action_Type\"]\n",
    "cate_data = data[categorical_columns]\n",
    "\n",
    "#for col in data.columns.values:\n",
    "#    print(col, data[col].unique())\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def label_encode(cate_data, columns):\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        col_values_unique = list(cate_data[col].unique())\n",
    "        le_fitted = le.fit(col_values_unique)\n",
    " \n",
    "        col_values = list(cate_data[col].values)\n",
    "        le.classes_\n",
    "        col_values_transformed = le.transform(col_values)\n",
    "        cate_data[col] = col_values_transformed\n",
    " \n",
    "to_be_encoded_cols = cate_data.columns.values\n",
    "label_encode(cate_data, to_be_encoded_cols)\n",
    "#display(cate_data.head())\n",
    "\n",
    "data.drop([\"Button\",\"Action_Type\"], axis=1, inplace=True)\n",
    "data=pd.concat([data,cate_data], axis=1)\n",
    "\n",
    "\n",
    "#display(data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "      <th>Language</th>\n",
       "      <th>Hardware_Model</th>\n",
       "      <th>SDK_Version</th>\n",
       "      <th>Manufacture</th>\n",
       "      <th>Screen_Size</th>\n",
       "      <th>Time_Zone</th>\n",
       "      <th>Country_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UUID  Language  Hardware_Model  SDK_Version  Manufacture  Screen_Size  \\\n",
       "0    48         0              40            3           18           23   \n",
       "1    48         0              40            3           18           23   \n",
       "2    48         0              40            3           18           23   \n",
       "3    48         0              40            3           18           23   \n",
       "4    48         0              40            3           18           23   \n",
       "\n",
       "   Time_Zone  Country_Code  \n",
       "0          7             1  \n",
       "1          7             1  \n",
       "2          7             1  \n",
       "3          7             1  \n",
       "4          7             1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num_of_CPU_Cores</th>\n",
       "      <th>pLN1</th>\n",
       "      <th>p.2</th>\n",
       "      <th>pLN3</th>\n",
       "      <th>pt4</th>\n",
       "      <th>pi5</th>\n",
       "      <th>pe6</th>\n",
       "      <th>pLN7</th>\n",
       "      <th>p58</th>\n",
       "      <th>pLN9</th>\n",
       "      <th>...</th>\n",
       "      <th>avdu2</th>\n",
       "      <th>avgp</th>\n",
       "      <th>avga</th>\n",
       "      <th>Language</th>\n",
       "      <th>Hardware_Model</th>\n",
       "      <th>SDK_Version</th>\n",
       "      <th>Manufacture</th>\n",
       "      <th>Screen_Size</th>\n",
       "      <th>Time_Zone</th>\n",
       "      <th>Country_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>88.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>575.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>466.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Num_of_CPU_Cores  pLN1  p.2  pLN3  pt4  pi5  pe6  pLN7  p58  pLN9  \\\n",
       "0               8.0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0   \n",
       "1               8.0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0   \n",
       "2               8.0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0   \n",
       "3               8.0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0   \n",
       "4               8.0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0   \n",
       "\n",
       "       ...            avdu2  avgp      avga  Language  Hardware_Model  \\\n",
       "0      ...        88.200000   1.0  0.004412         0              40   \n",
       "1      ...        95.400000   1.0  0.004167         0              40   \n",
       "2      ...       575.333333   1.0  0.008333         0              40   \n",
       "3      ...       466.400000   1.0  0.008211         0              40   \n",
       "4      ...       121.800000   1.0  0.009804         0              40   \n",
       "\n",
       "   SDK_Version  Manufacture  Screen_Size  Time_Zone  Country_Code  \n",
       "0            3           18           23          7             1  \n",
       "1            3           18           23          7             1  \n",
       "2            3           18           23          7             1  \n",
       "3            3           18           23          7             1  \n",
       "4            3           18           23          7             1  \n",
       "\n",
       "[5 rows x 155 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 770 entries, 0 to 769\n",
      "Columns: 155 entries, Num_of_CPU_Cores to Country_Code\n",
      "dtypes: float64(148), int64(7)\n",
      "memory usage: 932.5 KB\n",
      "initial data info None\n",
      "data is (770, 155)\n",
      "(770, 155)\n",
      "(770,)\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         2\n",
      "          1       1.00      1.00      1.00         2\n",
      "          2       1.00      1.00      1.00         2\n",
      "          3       1.00      1.00      1.00         2\n",
      "          4       1.00      1.00      1.00         2\n",
      "          5       1.00      1.00      1.00         2\n",
      "          6       1.00      1.00      1.00         2\n",
      "          7       1.00      1.00      1.00         2\n",
      "          8       1.00      1.00      1.00         2\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       0.00      0.00      0.00         2\n",
      "         11       1.00      1.00      1.00         2\n",
      "         12       1.00      1.00      1.00         2\n",
      "         13       1.00      1.00      1.00         2\n",
      "         14       1.00      1.00      1.00         2\n",
      "         15       1.00      1.00      1.00         2\n",
      "         16       1.00      1.00      1.00         2\n",
      "         17       1.00      1.00      1.00         2\n",
      "         18       1.00      1.00      1.00         2\n",
      "         19       1.00      1.00      1.00         2\n",
      "         20       1.00      1.00      1.00         2\n",
      "         21       1.00      1.00      1.00         2\n",
      "         22       1.00      1.00      1.00         2\n",
      "         23       1.00      1.00      1.00         2\n",
      "         24       1.00      1.00      1.00         2\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00         2\n",
      "         27       1.00      0.50      0.67         2\n",
      "         28       0.67      1.00      0.80         2\n",
      "         29       1.00      0.50      0.67         2\n",
      "         30       1.00      1.00      1.00         2\n",
      "         31       0.67      1.00      0.80         2\n",
      "         32       1.00      1.00      1.00         2\n",
      "         33       1.00      1.00      1.00         2\n",
      "         34       1.00      1.00      1.00         2\n",
      "         35       1.00      1.00      1.00         2\n",
      "         36       1.00      1.00      1.00         2\n",
      "         37       1.00      1.00      1.00         2\n",
      "         38       1.00      1.00      1.00         2\n",
      "         39       0.67      1.00      0.80         2\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       1.00      1.00      1.00         2\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         2\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00         2\n",
      "         46       0.50      1.00      0.67         2\n",
      "         47       1.00      1.00      1.00         2\n",
      "         48       1.00      1.00      1.00         2\n",
      "         49       1.00      1.00      1.00         2\n",
      "         50       1.00      1.00      1.00         2\n",
      "         51       1.00      1.00      1.00         2\n",
      "         52       1.00      1.00      1.00         2\n",
      "         53       1.00      1.00      1.00         2\n",
      "         54       1.00      1.00      1.00         2\n",
      "         55       1.00      0.50      0.67         2\n",
      "         56       1.00      1.00      1.00         2\n",
      "         57       1.00      1.00      1.00         2\n",
      "         58       1.00      1.00      1.00         2\n",
      "         59       1.00      1.00      1.00         2\n",
      "         60       1.00      1.00      1.00         2\n",
      "         61       1.00      1.00      1.00         2\n",
      "         62       1.00      1.00      1.00         2\n",
      "         63       1.00      1.00      1.00         2\n",
      "         64       1.00      1.00      1.00         2\n",
      "         65       1.00      1.00      1.00         2\n",
      "         66       0.67      1.00      0.80         2\n",
      "         67       1.00      1.00      1.00         2\n",
      "         68       1.00      1.00      1.00         2\n",
      "         69       1.00      1.00      1.00         2\n",
      "         70       1.00      1.00      1.00         2\n",
      "         71       1.00      1.00      1.00         2\n",
      "         72       1.00      0.50      0.67         2\n",
      "         73       1.00      1.00      1.00         2\n",
      "         74       1.00      1.00      1.00         2\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       0.96      0.96      0.95       154\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         2\n",
      "          1       1.00      1.00      1.00         2\n",
      "          2       1.00      1.00      1.00         2\n",
      "          3       1.00      1.00      1.00         2\n",
      "          4       1.00      1.00      1.00         2\n",
      "          5       1.00      1.00      1.00         2\n",
      "          6       1.00      1.00      1.00         2\n",
      "          7       1.00      1.00      1.00         2\n",
      "          8       0.67      1.00      0.80         2\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       0.00      0.00      0.00         2\n",
      "         11       1.00      1.00      1.00         2\n",
      "         12       1.00      1.00      1.00         2\n",
      "         13       1.00      1.00      1.00         2\n",
      "         14       1.00      1.00      1.00         2\n",
      "         15       1.00      1.00      1.00         2\n",
      "         16       1.00      1.00      1.00         2\n",
      "         17       1.00      1.00      1.00         2\n",
      "         18       1.00      1.00      1.00         2\n",
      "         19       1.00      1.00      1.00         2\n",
      "         20       1.00      1.00      1.00         2\n",
      "         21       1.00      1.00      1.00         2\n",
      "         22       1.00      1.00      1.00         2\n",
      "         23       1.00      1.00      1.00         2\n",
      "         24       1.00      1.00      1.00         2\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00         2\n",
      "         27       0.00      0.00      0.00         2\n",
      "         28       0.67      1.00      0.80         2\n",
      "         29       1.00      0.50      0.67         2\n",
      "         30       1.00      1.00      1.00         2\n",
      "         31       0.67      1.00      0.80         2\n",
      "         32       1.00      1.00      1.00         2\n",
      "         33       1.00      1.00      1.00         2\n",
      "         34       1.00      1.00      1.00         2\n",
      "         35       1.00      1.00      1.00         2\n",
      "         36       1.00      1.00      1.00         2\n",
      "         37       1.00      1.00      1.00         2\n",
      "         38       1.00      1.00      1.00         2\n",
      "         39       0.67      1.00      0.80         2\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       1.00      1.00      1.00         2\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         2\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00         2\n",
      "         46       0.50      1.00      0.67         2\n",
      "         47       1.00      1.00      1.00         2\n",
      "         48       1.00      1.00      1.00         2\n",
      "         49       1.00      1.00      1.00         2\n",
      "         50       1.00      1.00      1.00         2\n",
      "         51       1.00      1.00      1.00         2\n",
      "         52       1.00      1.00      1.00         2\n",
      "         53       1.00      1.00      1.00         2\n",
      "         54       1.00      1.00      1.00         2\n",
      "         55       1.00      0.50      0.67         2\n",
      "         56       1.00      1.00      1.00         2\n",
      "         57       1.00      1.00      1.00         2\n",
      "         58       1.00      1.00      1.00         2\n",
      "         59       1.00      1.00      1.00         2\n",
      "         60       1.00      1.00      1.00         2\n",
      "         61       1.00      1.00      1.00         2\n",
      "         62       1.00      1.00      1.00         2\n",
      "         63       1.00      1.00      1.00         2\n",
      "         64       1.00      1.00      1.00         2\n",
      "         65       1.00      1.00      1.00         2\n",
      "         66       0.67      1.00      0.80         2\n",
      "         67       1.00      1.00      1.00         2\n",
      "         68       1.00      1.00      1.00         2\n",
      "         69       1.00      1.00      1.00         2\n",
      "         70       1.00      1.00      1.00         2\n",
      "         71       1.00      1.00      1.00         2\n",
      "         72       1.00      0.50      0.67         2\n",
      "         73       1.00      1.00      1.00         2\n",
      "         74       1.00      1.00      1.00         2\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       0.95      0.95      0.94       154\n",
      "\n",
      "each loop acc 0.961038961039\n",
      "each loop rbf acc 0.954545454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67         2\n",
      "          1       1.00      1.00      1.00         2\n",
      "          2       1.00      1.00      1.00         2\n",
      "          3       1.00      1.00      1.00         2\n",
      "          4       1.00      1.00      1.00         2\n",
      "          5       1.00      1.00      1.00         2\n",
      "          6       1.00      1.00      1.00         2\n",
      "          7       1.00      1.00      1.00         2\n",
      "          8       1.00      0.50      0.67         2\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       1.00      1.00      1.00         2\n",
      "         11       1.00      1.00      1.00         2\n",
      "         12       1.00      1.00      1.00         2\n",
      "         13       1.00      1.00      1.00         2\n",
      "         14       1.00      1.00      1.00         2\n",
      "         15       1.00      1.00      1.00         2\n",
      "         16       1.00      1.00      1.00         2\n",
      "         17       1.00      1.00      1.00         2\n",
      "         18       1.00      1.00      1.00         2\n",
      "         19       1.00      1.00      1.00         2\n",
      "         20       1.00      1.00      1.00         2\n",
      "         21       1.00      1.00      1.00         2\n",
      "         22       1.00      1.00      1.00         2\n",
      "         23       1.00      1.00      1.00         2\n",
      "         24       1.00      1.00      1.00         2\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00         2\n",
      "         27       0.67      1.00      0.80         2\n",
      "         28       1.00      1.00      1.00         2\n",
      "         29       1.00      1.00      1.00         2\n",
      "         30       1.00      1.00      1.00         2\n",
      "         31       1.00      1.00      1.00         2\n",
      "         32       1.00      1.00      1.00         2\n",
      "         33       1.00      1.00      1.00         2\n",
      "         34       1.00      1.00      1.00         2\n",
      "         35       1.00      1.00      1.00         2\n",
      "         36       1.00      1.00      1.00         2\n",
      "         37       1.00      1.00      1.00         2\n",
      "         38       1.00      1.00      1.00         2\n",
      "         39       1.00      1.00      1.00         2\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       1.00      1.00      1.00         2\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       0.67      1.00      0.80         2\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00         2\n",
      "         46       1.00      1.00      1.00         2\n",
      "         47       1.00      1.00      1.00         2\n",
      "         48       1.00      1.00      1.00         2\n",
      "         49       1.00      1.00      1.00         2\n",
      "         50       1.00      1.00      1.00         2\n",
      "         51       1.00      1.00      1.00         2\n",
      "         52       1.00      1.00      1.00         2\n",
      "         53       1.00      1.00      1.00         2\n",
      "         54       1.00      1.00      1.00         2\n",
      "         55       1.00      1.00      1.00         2\n",
      "         56       1.00      1.00      1.00         2\n",
      "         57       1.00      1.00      1.00         2\n",
      "         58       1.00      1.00      1.00         2\n",
      "         59       1.00      1.00      1.00         2\n",
      "         60       1.00      1.00      1.00         2\n",
      "         61       1.00      1.00      1.00         2\n",
      "         62       1.00      1.00      1.00         2\n",
      "         63       1.00      1.00      1.00         2\n",
      "         64       1.00      1.00      1.00         2\n",
      "         65       1.00      1.00      1.00         2\n",
      "         66       1.00      1.00      1.00         2\n",
      "         67       1.00      1.00      1.00         2\n",
      "         68       1.00      1.00      1.00         2\n",
      "         69       1.00      1.00      1.00         2\n",
      "         70       1.00      1.00      1.00         2\n",
      "         71       1.00      1.00      1.00         2\n",
      "         72       1.00      1.00      1.00         2\n",
      "         73       1.00      1.00      1.00         2\n",
      "         74       1.00      1.00      1.00         2\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       0.99      0.99      0.99       154\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67         2\n",
      "          1       1.00      1.00      1.00         2\n",
      "          2       1.00      1.00      1.00         2\n",
      "          3       1.00      1.00      1.00         2\n",
      "          4       1.00      1.00      1.00         2\n",
      "          5       1.00      1.00      1.00         2\n",
      "          6       1.00      1.00      1.00         2\n",
      "          7       1.00      1.00      1.00         2\n",
      "          8       1.00      0.50      0.67         2\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       1.00      1.00      1.00         2\n",
      "         11       1.00      1.00      1.00         2\n",
      "         12       1.00      1.00      1.00         2\n",
      "         13       1.00      1.00      1.00         2\n",
      "         14       1.00      1.00      1.00         2\n",
      "         15       1.00      1.00      1.00         2\n",
      "         16       1.00      1.00      1.00         2\n",
      "         17       1.00      1.00      1.00         2\n",
      "         18       1.00      1.00      1.00         2\n",
      "         19       1.00      1.00      1.00         2\n",
      "         20       1.00      1.00      1.00         2\n",
      "         21       1.00      1.00      1.00         2\n",
      "         22       1.00      1.00      1.00         2\n",
      "         23       1.00      1.00      1.00         2\n",
      "         24       1.00      0.50      0.67         2\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00         2\n",
      "         27       0.67      1.00      0.80         2\n",
      "         28       1.00      1.00      1.00         2\n",
      "         29       1.00      1.00      1.00         2\n",
      "         30       1.00      1.00      1.00         2\n",
      "         31       1.00      1.00      1.00         2\n",
      "         32       1.00      1.00      1.00         2\n",
      "         33       1.00      1.00      1.00         2\n",
      "         34       1.00      1.00      1.00         2\n",
      "         35       1.00      1.00      1.00         2\n",
      "         36       1.00      1.00      1.00         2\n",
      "         37       1.00      1.00      1.00         2\n",
      "         38       1.00      1.00      1.00         2\n",
      "         39       1.00      1.00      1.00         2\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       1.00      1.00      1.00         2\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       0.67      1.00      0.80         2\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00         2\n",
      "         46       1.00      1.00      1.00         2\n",
      "         47       1.00      1.00      1.00         2\n",
      "         48       1.00      1.00      1.00         2\n",
      "         49       1.00      1.00      1.00         2\n",
      "         50       1.00      1.00      1.00         2\n",
      "         51       1.00      1.00      1.00         2\n",
      "         52       1.00      1.00      1.00         2\n",
      "         53       1.00      1.00      1.00         2\n",
      "         54       0.67      1.00      0.80         2\n",
      "         55       1.00      1.00      1.00         2\n",
      "         56       1.00      1.00      1.00         2\n",
      "         57       1.00      1.00      1.00         2\n",
      "         58       1.00      1.00      1.00         2\n",
      "         59       1.00      1.00      1.00         2\n",
      "         60       1.00      1.00      1.00         2\n",
      "         61       1.00      1.00      1.00         2\n",
      "         62       1.00      1.00      1.00         2\n",
      "         63       1.00      1.00      1.00         2\n",
      "         64       1.00      1.00      1.00         2\n",
      "         65       1.00      1.00      1.00         2\n",
      "         66       1.00      1.00      1.00         2\n",
      "         67       1.00      1.00      1.00         2\n",
      "         68       1.00      1.00      1.00         2\n",
      "         69       1.00      1.00      1.00         2\n",
      "         70       1.00      1.00      1.00         2\n",
      "         71       1.00      1.00      1.00         2\n",
      "         72       1.00      1.00      1.00         2\n",
      "         73       1.00      1.00      1.00         2\n",
      "         74       1.00      1.00      1.00         2\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       0.99      0.98      0.98       154\n",
      "\n",
      "each loop acc 0.987012987013\n",
      "each loop rbf acc 0.980519480519\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         2\n",
      "          1       1.00      1.00      1.00         2\n",
      "          2       1.00      1.00      1.00         2\n",
      "          3       1.00      1.00      1.00         2\n",
      "          4       1.00      1.00      1.00         2\n",
      "          5       1.00      1.00      1.00         2\n",
      "          6       1.00      1.00      1.00         2\n",
      "          7       1.00      1.00      1.00         2\n",
      "          8       0.00      0.00      0.00         2\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       1.00      0.50      0.67         2\n",
      "         11       1.00      1.00      1.00         2\n",
      "         12       1.00      1.00      1.00         2\n",
      "         13       1.00      1.00      1.00         2\n",
      "         14       1.00      1.00      1.00         2\n",
      "         15       1.00      1.00      1.00         2\n",
      "         16       1.00      1.00      1.00         2\n",
      "         17       1.00      1.00      1.00         2\n",
      "         18       1.00      1.00      1.00         2\n",
      "         19       1.00      1.00      1.00         2\n",
      "         20       1.00      1.00      1.00         2\n",
      "         21       1.00      1.00      1.00         2\n",
      "         22       1.00      1.00      1.00         2\n",
      "         23       1.00      1.00      1.00         2\n",
      "         24       1.00      1.00      1.00         2\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00         2\n",
      "         27       0.33      0.50      0.40         2\n",
      "         28       1.00      1.00      1.00         2\n",
      "         29       1.00      1.00      1.00         2\n",
      "         30       1.00      1.00      1.00         2\n",
      "         31       1.00      1.00      1.00         2\n",
      "         32       1.00      1.00      1.00         2\n",
      "         33       1.00      1.00      1.00         2\n",
      "         34       1.00      1.00      1.00         2\n",
      "         35       1.00      1.00      1.00         2\n",
      "         36       1.00      1.00      1.00         2\n",
      "         37       1.00      1.00      1.00         2\n",
      "         38       1.00      1.00      1.00         2\n",
      "         39       1.00      1.00      1.00         2\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       1.00      1.00      1.00         2\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         2\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00         2\n",
      "         46       0.50      0.50      0.50         2\n",
      "         47       1.00      1.00      1.00         2\n",
      "         48       1.00      1.00      1.00         2\n",
      "         49       1.00      1.00      1.00         2\n",
      "         50       0.67      1.00      0.80         2\n",
      "         51       1.00      1.00      1.00         2\n",
      "         52       1.00      1.00      1.00         2\n",
      "         53       1.00      1.00      1.00         2\n",
      "         54       1.00      1.00      1.00         2\n",
      "         55       1.00      1.00      1.00         2\n",
      "         56       1.00      1.00      1.00         2\n",
      "         57       1.00      1.00      1.00         2\n",
      "         58       1.00      1.00      1.00         2\n",
      "         59       1.00      1.00      1.00         2\n",
      "         60       1.00      1.00      1.00         2\n",
      "         61       1.00      1.00      1.00         2\n",
      "         62       1.00      1.00      1.00         2\n",
      "         63       1.00      1.00      1.00         2\n",
      "         64       1.00      1.00      1.00         2\n",
      "         65       1.00      1.00      1.00         2\n",
      "         66       1.00      1.00      1.00         2\n",
      "         67       1.00      1.00      1.00         2\n",
      "         68       1.00      1.00      1.00         2\n",
      "         69       1.00      1.00      1.00         2\n",
      "         70       1.00      1.00      1.00         2\n",
      "         71       1.00      1.00      1.00         2\n",
      "         72       1.00      1.00      1.00         2\n",
      "         73       1.00      1.00      1.00         2\n",
      "         74       1.00      1.00      1.00         2\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       0.97      0.97      0.97       154\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         2\n",
      "          1       1.00      1.00      1.00         2\n",
      "          2       1.00      1.00      1.00         2\n",
      "          3       1.00      1.00      1.00         2\n",
      "          4       1.00      1.00      1.00         2\n",
      "          5       1.00      1.00      1.00         2\n",
      "          6       1.00      1.00      1.00         2\n",
      "          7       1.00      1.00      1.00         2\n",
      "          8       0.00      0.00      0.00         2\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       1.00      0.50      0.67         2\n",
      "         11       1.00      1.00      1.00         2\n",
      "         12       1.00      1.00      1.00         2\n",
      "         13       1.00      1.00      1.00         2\n",
      "         14       1.00      1.00      1.00         2\n",
      "         15       1.00      1.00      1.00         2\n",
      "         16       1.00      1.00      1.00         2\n",
      "         17       1.00      1.00      1.00         2\n",
      "         18       1.00      1.00      1.00         2\n",
      "         19       1.00      1.00      1.00         2\n",
      "         20       1.00      1.00      1.00         2\n",
      "         21       1.00      1.00      1.00         2\n",
      "         22       1.00      1.00      1.00         2\n",
      "         23       1.00      1.00      1.00         2\n",
      "         24       1.00      1.00      1.00         2\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00         2\n",
      "         27       0.50      1.00      0.67         2\n",
      "         28       1.00      1.00      1.00         2\n",
      "         29       1.00      1.00      1.00         2\n",
      "         30       1.00      1.00      1.00         2\n",
      "         31       1.00      1.00      1.00         2\n",
      "         32       1.00      1.00      1.00         2\n",
      "         33       1.00      1.00      1.00         2\n",
      "         34       1.00      1.00      1.00         2\n",
      "         35       1.00      1.00      1.00         2\n",
      "         36       1.00      1.00      1.00         2\n",
      "         37       1.00      1.00      1.00         2\n",
      "         38       1.00      1.00      1.00         2\n",
      "         39       1.00      0.50      0.67         2\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       1.00      1.00      1.00         2\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         2\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00         2\n",
      "         46       0.50      0.50      0.50         2\n",
      "         47       1.00      1.00      1.00         2\n",
      "         48       1.00      1.00      1.00         2\n",
      "         49       1.00      1.00      1.00         2\n",
      "         50       0.67      1.00      0.80         2\n",
      "         51       1.00      1.00      1.00         2\n",
      "         52       1.00      1.00      1.00         2\n",
      "         53       1.00      1.00      1.00         2\n",
      "         54       1.00      1.00      1.00         2\n",
      "         55       1.00      1.00      1.00         2\n",
      "         56       1.00      1.00      1.00         2\n",
      "         57       1.00      1.00      1.00         2\n",
      "         58       1.00      1.00      1.00         2\n",
      "         59       1.00      1.00      1.00         2\n",
      "         60       1.00      1.00      1.00         2\n",
      "         61       1.00      1.00      1.00         2\n",
      "         62       1.00      1.00      1.00         2\n",
      "         63       1.00      1.00      1.00         2\n",
      "         64       1.00      1.00      1.00         2\n",
      "         65       1.00      1.00      1.00         2\n",
      "         66       1.00      1.00      1.00         2\n",
      "         67       1.00      1.00      1.00         2\n",
      "         68       1.00      1.00      1.00         2\n",
      "         69       1.00      1.00      1.00         2\n",
      "         70       1.00      1.00      1.00         2\n",
      "         71       1.00      1.00      1.00         2\n",
      "         72       1.00      1.00      1.00         2\n",
      "         73       1.00      1.00      1.00         2\n",
      "         74       1.00      1.00      1.00         2\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       0.97      0.97      0.96       154\n",
      "\n",
      "each loop acc 0.967532467532\n",
      "each loop rbf acc 0.967532467532\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         2\n",
      "          1       1.00      1.00      1.00         2\n",
      "          2       1.00      1.00      1.00         2\n",
      "          3       1.00      1.00      1.00         2\n",
      "          4       1.00      1.00      1.00         2\n",
      "          5       1.00      1.00      1.00         2\n",
      "          6       1.00      1.00      1.00         2\n",
      "          7       1.00      1.00      1.00         2\n",
      "          8       0.67      1.00      0.80         2\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       1.00      1.00      1.00         2\n",
      "         11       1.00      1.00      1.00         2\n",
      "         12       1.00      1.00      1.00         2\n",
      "         13       1.00      1.00      1.00         2\n",
      "         14       1.00      1.00      1.00         2\n",
      "         15       1.00      1.00      1.00         2\n",
      "         16       1.00      1.00      1.00         2\n",
      "         17       1.00      1.00      1.00         2\n",
      "         18       1.00      1.00      1.00         2\n",
      "         19       1.00      1.00      1.00         2\n",
      "         20       1.00      1.00      1.00         2\n",
      "         21       1.00      1.00      1.00         2\n",
      "         22       1.00      1.00      1.00         2\n",
      "         23       1.00      1.00      1.00         2\n",
      "         24       1.00      1.00      1.00         2\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00         2\n",
      "         27       0.00      0.00      0.00         2\n",
      "         28       1.00      1.00      1.00         2\n",
      "         29       0.67      1.00      0.80         2\n",
      "         30       1.00      1.00      1.00         2\n",
      "         31       1.00      0.50      0.67         2\n",
      "         32       1.00      1.00      1.00         2\n",
      "         33       1.00      1.00      1.00         2\n",
      "         34       1.00      1.00      1.00         2\n",
      "         35       1.00      1.00      1.00         2\n",
      "         36       1.00      1.00      1.00         2\n",
      "         37       1.00      1.00      1.00         2\n",
      "         38       1.00      1.00      1.00         2\n",
      "         39       0.67      1.00      0.80         2\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       1.00      1.00      1.00         2\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         2\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00         2\n",
      "         46       1.00      1.00      1.00         2\n",
      "         47       1.00      1.00      1.00         2\n",
      "         48       1.00      1.00      1.00         2\n",
      "         49       1.00      1.00      1.00         2\n",
      "         50       1.00      1.00      1.00         2\n",
      "         51       1.00      1.00      1.00         2\n",
      "         52       1.00      1.00      1.00         2\n",
      "         53       1.00      1.00      1.00         2\n",
      "         54       1.00      1.00      1.00         2\n",
      "         55       1.00      1.00      1.00         2\n",
      "         56       1.00      1.00      1.00         2\n",
      "         57       1.00      1.00      1.00         2\n",
      "         58       1.00      1.00      1.00         2\n",
      "         59       1.00      1.00      1.00         2\n",
      "         60       1.00      1.00      1.00         2\n",
      "         61       1.00      1.00      1.00         2\n",
      "         62       1.00      1.00      1.00         2\n",
      "         63       1.00      1.00      1.00         2\n",
      "         64       1.00      1.00      1.00         2\n",
      "         65       1.00      1.00      1.00         2\n",
      "         66       1.00      1.00      1.00         2\n",
      "         67       1.00      1.00      1.00         2\n",
      "         68       1.00      1.00      1.00         2\n",
      "         69       1.00      1.00      1.00         2\n",
      "         70       1.00      1.00      1.00         2\n",
      "         71       1.00      1.00      1.00         2\n",
      "         72       1.00      1.00      1.00         2\n",
      "         73       1.00      1.00      1.00         2\n",
      "         74       1.00      1.00      1.00         2\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       0.97      0.98      0.97       154\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         2\n",
      "          1       1.00      1.00      1.00         2\n",
      "          2       1.00      1.00      1.00         2\n",
      "          3       1.00      1.00      1.00         2\n",
      "          4       1.00      1.00      1.00         2\n",
      "          5       1.00      1.00      1.00         2\n",
      "          6       1.00      1.00      1.00         2\n",
      "          7       1.00      1.00      1.00         2\n",
      "          8       1.00      1.00      1.00         2\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       1.00      1.00      1.00         2\n",
      "         11       1.00      1.00      1.00         2\n",
      "         12       1.00      1.00      1.00         2\n",
      "         13       1.00      1.00      1.00         2\n",
      "         14       1.00      1.00      1.00         2\n",
      "         15       1.00      1.00      1.00         2\n",
      "         16       1.00      1.00      1.00         2\n",
      "         17       1.00      1.00      1.00         2\n",
      "         18       1.00      1.00      1.00         2\n",
      "         19       1.00      1.00      1.00         2\n",
      "         20       1.00      1.00      1.00         2\n",
      "         21       1.00      1.00      1.00         2\n",
      "         22       1.00      1.00      1.00         2\n",
      "         23       1.00      1.00      1.00         2\n",
      "         24       1.00      1.00      1.00         2\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00         2\n",
      "         27       1.00      1.00      1.00         2\n",
      "         28       1.00      1.00      1.00         2\n",
      "         29       0.67      1.00      0.80         2\n",
      "         30       1.00      1.00      1.00         2\n",
      "         31       1.00      0.50      0.67         2\n",
      "         32       1.00      1.00      1.00         2\n",
      "         33       1.00      1.00      1.00         2\n",
      "         34       1.00      1.00      1.00         2\n",
      "         35       1.00      1.00      1.00         2\n",
      "         36       1.00      1.00      1.00         2\n",
      "         37       1.00      1.00      1.00         2\n",
      "         38       1.00      1.00      1.00         2\n",
      "         39       1.00      1.00      1.00         2\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       1.00      1.00      1.00         2\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         2\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00         2\n",
      "         46       1.00      1.00      1.00         2\n",
      "         47       1.00      1.00      1.00         2\n",
      "         48       1.00      1.00      1.00         2\n",
      "         49       1.00      1.00      1.00         2\n",
      "         50       1.00      1.00      1.00         2\n",
      "         51       1.00      1.00      1.00         2\n",
      "         52       1.00      1.00      1.00         2\n",
      "         53       1.00      1.00      1.00         2\n",
      "         54       1.00      1.00      1.00         2\n",
      "         55       1.00      1.00      1.00         2\n",
      "         56       1.00      1.00      1.00         2\n",
      "         57       1.00      1.00      1.00         2\n",
      "         58       1.00      1.00      1.00         2\n",
      "         59       1.00      1.00      1.00         2\n",
      "         60       1.00      1.00      1.00         2\n",
      "         61       1.00      1.00      1.00         2\n",
      "         62       1.00      1.00      1.00         2\n",
      "         63       1.00      1.00      1.00         2\n",
      "         64       1.00      1.00      1.00         2\n",
      "         65       1.00      1.00      1.00         2\n",
      "         66       1.00      1.00      1.00         2\n",
      "         67       1.00      1.00      1.00         2\n",
      "         68       1.00      1.00      1.00         2\n",
      "         69       1.00      1.00      1.00         2\n",
      "         70       1.00      1.00      1.00         2\n",
      "         71       1.00      1.00      1.00         2\n",
      "         72       1.00      1.00      1.00         2\n",
      "         73       1.00      1.00      1.00         2\n",
      "         74       1.00      1.00      1.00         2\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       1.00      0.99      0.99       154\n",
      "\n",
      "each loop acc 0.980519480519\n",
      "each loop rbf acc 0.993506493506\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         2\n",
      "          1       1.00      1.00      1.00         2\n",
      "          2       1.00      1.00      1.00         2\n",
      "          3       1.00      1.00      1.00         2\n",
      "          4       1.00      1.00      1.00         2\n",
      "          5       1.00      1.00      1.00         2\n",
      "          6       1.00      1.00      1.00         2\n",
      "          7       1.00      1.00      1.00         2\n",
      "          8       0.33      0.50      0.40         2\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       1.00      1.00      1.00         2\n",
      "         11       1.00      1.00      1.00         2\n",
      "         12       1.00      1.00      1.00         2\n",
      "         13       1.00      1.00      1.00         2\n",
      "         14       1.00      1.00      1.00         2\n",
      "         15       1.00      1.00      1.00         2\n",
      "         16       1.00      1.00      1.00         2\n",
      "         17       1.00      1.00      1.00         2\n",
      "         18       1.00      1.00      1.00         2\n",
      "         19       1.00      1.00      1.00         2\n",
      "         20       1.00      1.00      1.00         2\n",
      "         21       1.00      1.00      1.00         2\n",
      "         22       1.00      1.00      1.00         2\n",
      "         23       1.00      1.00      1.00         2\n",
      "         24       1.00      1.00      1.00         2\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00         2\n",
      "         27       0.50      0.50      0.50         2\n",
      "         28       1.00      1.00      1.00         2\n",
      "         29       0.67      1.00      0.80         2\n",
      "         30       1.00      1.00      1.00         2\n",
      "         31       1.00      0.50      0.67         2\n",
      "         32       1.00      1.00      1.00         2\n",
      "         33       1.00      1.00      1.00         2\n",
      "         34       1.00      1.00      1.00         2\n",
      "         35       1.00      1.00      1.00         2\n",
      "         36       1.00      1.00      1.00         2\n",
      "         37       1.00      1.00      1.00         2\n",
      "         38       1.00      1.00      1.00         2\n",
      "         39       0.50      0.50      0.50         2\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       1.00      1.00      1.00         2\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         2\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00         2\n",
      "         46       1.00      1.00      1.00         2\n",
      "         47       1.00      1.00      1.00         2\n",
      "         48       1.00      1.00      1.00         2\n",
      "         49       1.00      1.00      1.00         2\n",
      "         50       1.00      1.00      1.00         2\n",
      "         51       1.00      1.00      1.00         2\n",
      "         52       1.00      1.00      1.00         2\n",
      "         53       1.00      1.00      1.00         2\n",
      "         54       1.00      1.00      1.00         2\n",
      "         55       1.00      1.00      1.00         2\n",
      "         56       1.00      1.00      1.00         2\n",
      "         57       1.00      1.00      1.00         2\n",
      "         58       1.00      1.00      1.00         2\n",
      "         59       1.00      1.00      1.00         2\n",
      "         60       1.00      1.00      1.00         2\n",
      "         61       1.00      0.50      0.67         2\n",
      "         62       1.00      1.00      1.00         2\n",
      "         63       1.00      1.00      1.00         2\n",
      "         64       1.00      1.00      1.00         2\n",
      "         65       1.00      1.00      1.00         2\n",
      "         66       1.00      1.00      1.00         2\n",
      "         67       1.00      1.00      1.00         2\n",
      "         68       1.00      1.00      1.00         2\n",
      "         69       1.00      1.00      1.00         2\n",
      "         70       1.00      1.00      1.00         2\n",
      "         71       1.00      1.00      1.00         2\n",
      "         72       1.00      1.00      1.00         2\n",
      "         73       1.00      1.00      1.00         2\n",
      "         74       1.00      1.00      1.00         2\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       0.97      0.97      0.97       154\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         2\n",
      "          1       1.00      1.00      1.00         2\n",
      "          2       1.00      1.00      1.00         2\n",
      "          3       1.00      1.00      1.00         2\n",
      "          4       1.00      1.00      1.00         2\n",
      "          5       1.00      1.00      1.00         2\n",
      "          6       1.00      1.00      1.00         2\n",
      "          7       1.00      1.00      1.00         2\n",
      "          8       1.00      1.00      1.00         2\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       1.00      1.00      1.00         2\n",
      "         11       1.00      1.00      1.00         2\n",
      "         12       1.00      1.00      1.00         2\n",
      "         13       1.00      1.00      1.00         2\n",
      "         14       1.00      1.00      1.00         2\n",
      "         15       1.00      1.00      1.00         2\n",
      "         16       1.00      1.00      1.00         2\n",
      "         17       1.00      1.00      1.00         2\n",
      "         18       1.00      1.00      1.00         2\n",
      "         19       1.00      1.00      1.00         2\n",
      "         20       1.00      1.00      1.00         2\n",
      "         21       1.00      1.00      1.00         2\n",
      "         22       1.00      1.00      1.00         2\n",
      "         23       1.00      1.00      1.00         2\n",
      "         24       1.00      1.00      1.00         2\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00         2\n",
      "         27       1.00      1.00      1.00         2\n",
      "         28       1.00      1.00      1.00         2\n",
      "         29       0.50      1.00      0.67         2\n",
      "         30       1.00      1.00      1.00         2\n",
      "         31       0.00      0.00      0.00         2\n",
      "         32       1.00      1.00      1.00         2\n",
      "         33       1.00      1.00      1.00         2\n",
      "         34       1.00      0.50      0.67         2\n",
      "         35       1.00      1.00      1.00         2\n",
      "         36       1.00      1.00      1.00         2\n",
      "         37       1.00      1.00      1.00         2\n",
      "         38       1.00      1.00      1.00         2\n",
      "         39       0.67      1.00      0.80         2\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       1.00      1.00      1.00         2\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         2\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00         2\n",
      "         46       1.00      1.00      1.00         2\n",
      "         47       1.00      1.00      1.00         2\n",
      "         48       1.00      1.00      1.00         2\n",
      "         49       1.00      1.00      1.00         2\n",
      "         50       1.00      1.00      1.00         2\n",
      "         51       1.00      1.00      1.00         2\n",
      "         52       1.00      1.00      1.00         2\n",
      "         53       1.00      1.00      1.00         2\n",
      "         54       1.00      1.00      1.00         2\n",
      "         55       1.00      1.00      1.00         2\n",
      "         56       1.00      1.00      1.00         2\n",
      "         57       1.00      1.00      1.00         2\n",
      "         58       1.00      1.00      1.00         2\n",
      "         59       1.00      1.00      1.00         2\n",
      "         60       1.00      1.00      1.00         2\n",
      "         61       1.00      0.50      0.67         2\n",
      "         62       1.00      1.00      1.00         2\n",
      "         63       1.00      1.00      1.00         2\n",
      "         64       1.00      1.00      1.00         2\n",
      "         65       1.00      1.00      1.00         2\n",
      "         66       1.00      1.00      1.00         2\n",
      "         67       1.00      1.00      1.00         2\n",
      "         68       1.00      1.00      1.00         2\n",
      "         69       1.00      1.00      1.00         2\n",
      "         70       1.00      1.00      1.00         2\n",
      "         71       1.00      1.00      1.00         2\n",
      "         72       0.67      1.00      0.80         2\n",
      "         73       1.00      1.00      1.00         2\n",
      "         74       1.00      1.00      1.00         2\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       0.97      0.97      0.97       154\n",
      "\n",
      "each loop acc 0.967532467532\n",
      "each loop rbf acc 0.974025974026\n",
      "f1  0.9699567099567099\n",
      "Accuracy: 0.9727272727272727\n",
      "f1  0.9699567099567101\n",
      "Accuracy: 0.974025974025974\n",
      "here\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAADqCAYAAAClduxYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28d90d30a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import lsanomaly\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn import utils  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "\n",
    "# import the CSV from http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "# this will return a pandas dataframe.\n",
    "data = pd.read_csv('C:/Users/user/.spyder-py3/features1.csv', low_memory=False)\n",
    "'''data.loc[data['UUID'] == \"RVTNB1502866560357\", \"attack\"] = 1  \n",
    "data.loc[data['UUID'] != \"RVTNB1502866560357\", \"attack\"] = -1\n",
    "df_majority = data[data['attack']==-1]\n",
    "df_minority = data[data['attack']==1]\n",
    "from sklearn.utils import resample\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=830,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#print(data['attack'].value_counts())'''\n",
    "\n",
    "#target=np.array(target)\n",
    "#target = pd.DataFrame(target,columns=['attack'])\n",
    "\n",
    "#data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "categorical_columns=[\"UUID\",\"Language\",\"Hardware_Model\",\"SDK_Version\",\"Manufacture\",\"Screen_Size\",\"Time_Zone\",\"Country_Code\"]\n",
    "cate_data = data[categorical_columns]\n",
    "\n",
    "#for col in data.columns.values:\n",
    "#    print(col, data[col].unique())\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def label_encode(cate_data, columns):\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        col_values_unique = list(cate_data[col].unique())\n",
    "        le_fitted = le.fit(col_values_unique)\n",
    " \n",
    "        col_values = list(cate_data[col].values)\n",
    "        le.classes_\n",
    "        col_values_transformed = le.transform(col_values)\n",
    "        cate_data[col] = col_values_transformed\n",
    " \n",
    "to_be_encoded_cols = cate_data.columns.values\n",
    "label_encode(cate_data, to_be_encoded_cols)\n",
    "display(cate_data.head())\n",
    "target=cate_data['UUID']\n",
    "target=np.array(target)\n",
    "#target = pd.DataFrame(target)\n",
    "#target=target1.values\n",
    "\n",
    "data.drop([\"UUID\",\"Language\",\"Hardware_Model\",\"SDK_Version\",\"Manufacture\",\"Screen_Size\",\"Time_Zone\",\"Country_Code\"], axis=1, inplace=True)\n",
    "data=pd.concat([data,cate_data], axis=1)\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "\n",
    "# check the shape for sanity checking.\n",
    "data.shape\n",
    "display(data.head())\n",
    "print(\"initial data info\",data.info())\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"data is\",data.shape)\n",
    "from skfeature.function.information_theoretical_based import LCSI\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "\n",
    "from skfeature.utility.entropy_estimators import *\n",
    "import scipy.io\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#scaled_data=data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaleddata= pd.DataFrame(scaled_data)\n",
    "scaled_data=np.array(scaled_data)\n",
    "\n",
    "print(scaled_data.shape)\n",
    "print(target.shape)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "#display(target.head())\n",
    "#idx=MRMR.mrmr(scaled_data,target,n_selected_features=50)\n",
    "'''from sklearn import cross_validation\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "correct = 0\n",
    "print(\"scaled data details - \",scaled_data.info())\n",
    "print(\"target data details - \",target.info())\n",
    "for train, test in ss:\n",
    "    #print(scaled_data[train])\n",
    "    #print(target[train])\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=50)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:50]]\n",
    "print(features)    '''\n",
    "'''skb= SVC(kernel=\"linear\")\n",
    "rfe = RFE(estimator=skb, n_features_to_select=70)\n",
    "rfe=rfe.fit(scaleddata,target)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "skft = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "for train, test in skft:\n",
    "    X_train,X_test=scaled_data.iloc[train],scaled_data.iloc[test]\n",
    "    Y_train,y_test=target.iloc[train],target.iloc[test]\n",
    "    model1 = svm.OneClassSVM(nu=nu, kernel='rbf', gamma=0.10000000000000001)  \n",
    "    model1.fit(X_train, Y_train)\n",
    "    scores = cross_val_score(model1,X_test,y_test, cv=5, scoring='accuracy')\n",
    "    print(scores)\n",
    "print(scores.mean())'''\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5,random_state=36851234)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "''''from sklearn.model_selection import GridSearchCV\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "clf = svm.SVC(decision_function_shape='ovo',kernel='rbf')    # linear SVM\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=skf)\n",
    "grid.fit(scaled_data, target)\n",
    "print(\"The best parameters are %s with a score of %0.2f\"% (grid.best_params_, grid.best_score_))'''\n",
    "plt.figure(figsize=(8, 8))\n",
    "accuracy = plt.subplot(211)\n",
    "box=plt.subplot(211)\n",
    "from sklearn.svm import LinearSVC\n",
    "clf=SVC(kernel='linear')\n",
    "#clf=LinearSVC()\n",
    "rbf=SVC(decision_function_shape='ovo',gamma=0.001,C=10000.0)\n",
    "correct = 0\n",
    "fscoreTotal =0\n",
    "rbf_correct = 0\n",
    "rbf_fscoreTotal =0\n",
    "results=[]\n",
    "for train, test in skf.split(scaled_data,target):\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=36)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:36]]\n",
    "        #print(target[train])\n",
    "        # train a classification model with the selected features on the training dataset\n",
    "    clf.fit(features[train], target[train])\n",
    "    #clf1.fit(scaled_data[train],target[train])\n",
    "    rbf.fit(features[train], target[train])\n",
    "        # predict the class labels of test data\n",
    "    y_predict = clf.predict(features[test])\n",
    "    #y_predict = clf1.predict(scaled_data[test])\n",
    "    rbf_y_predict = rbf.predict(features[test])\n",
    "    print(\"metrics\")\n",
    "        # obtain the classification accuracy on the test data\n",
    "    acc = accuracy_score(target[test], y_predict)\n",
    "    rbf_acc = accuracy_score(target[test], rbf_y_predict)\n",
    "    correct = correct + acc\n",
    "    rbf_correct = rbf_correct + rbf_acc\n",
    "    fscore=f1_score(target[test], y_predict,average='weighted')\n",
    "    rbf_fscore=f1_score(target[test], rbf_y_predict,average='weighted')\n",
    "    fscoreTotal=fscoreTotal+fscore\n",
    "    rbf_fscoreTotal=rbf_fscoreTotal+rbf_fscore\n",
    "        #print(\"fsc \",f1_score(target[test], y_predict,average='weighted'))\n",
    "        #print(\"conf mat \",confusion_matrix(target[test],y_predict))\n",
    "        #print(\"ACCURACY: \", (accuracy_score(target[test], y_predict)))\n",
    "    report = classification_report(target[test], y_predict)\n",
    "    print(report)\n",
    "    rbreport = classification_report(target[test], rbf_y_predict)\n",
    "    print(rbreport)\n",
    "    print(\"each loop acc\",acc)\n",
    "    print(\"each loop rbf acc\",rbf_acc)\n",
    "score=float(correct)/5\n",
    "rbfscore=float(rbf_correct)/5\n",
    "results.append(score)\n",
    "results.append(rbfscore)\n",
    "print(\"f1 \",float(fscoreTotal)/5)\n",
    "    # output the average classification accuracy over all 10 folds\n",
    "print(\"Accuracy:\", float(correct)/5)\n",
    "print(\"f1 \",float(rbf_fscoreTotal)/5)\n",
    "    # output the average classification accuracy over all 10 folds\n",
    "print(\"Accuracy:\", float(rbf_correct)/5)\n",
    "\n",
    "accuracy.plot([scaled_data.shape[1], scaled_data.shape[0]],\n",
    "              [score, score], label=\"linear svm\")\n",
    "accuracy.plot([scaled_data.shape[1], scaled_data.shape[0]],\n",
    "              [rbfscore, rbfscore], label=\"rbf svm with grid search C=10000 and gamma=0.001\")\n",
    "accuracy.set_title(\"Classification accuracy\")\n",
    "accuracy.set_xlim(scaled_data.shape[1], scaled_data.shape[0])\n",
    "accuracy.set_xticks(())\n",
    "accuracy.set_ylim(0.94, 1)\n",
    "accuracy.set_ylabel(\"Classification accuracy\")\n",
    "accuracy.legend(loc='best')\n",
    "##svc=SelectKBest(mutual_info_classif, k=50).fit_transform(data,target)\n",
    "#svc = SVC(kernel=\"linear\")\n",
    "#rfe = RFE(estimator=svc, n_features_to_select=10)\n",
    "#rfe.fit(data, target)\n",
    "print(\"here\")\n",
    "box.boxplot(results)\n",
    "box.set_title(\"Classification accuracy\")\n",
    "box.set_xlim(scaled_data.shape[1], scaled_data.shape[0])\n",
    "box.set_xticks(())\n",
    "box.set_ylim(0.94, 1)\n",
    "box.set_ylabel(\"Classification accuracy\")\n",
    "box.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UUID\n",
       "0    48\n",
       "1    48\n",
       "2    48\n",
       "3    48\n",
       "4    48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pLN1</th>\n",
       "      <th>p.2</th>\n",
       "      <th>pLN3</th>\n",
       "      <th>pt4</th>\n",
       "      <th>pi5</th>\n",
       "      <th>pe6</th>\n",
       "      <th>pLN7</th>\n",
       "      <th>p58</th>\n",
       "      <th>pLN9</th>\n",
       "      <th>pSH10</th>\n",
       "      <th>...</th>\n",
       "      <th>du2a13</th>\n",
       "      <th>du2n14</th>\n",
       "      <th>du2n15</th>\n",
       "      <th>avgdu</th>\n",
       "      <th>avgud</th>\n",
       "      <th>avgdd</th>\n",
       "      <th>avguu</th>\n",
       "      <th>avdu2</th>\n",
       "      <th>avgp</th>\n",
       "      <th>avga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>37.875</td>\n",
       "      <td>24.466667</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>88.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>670740857</td>\n",
       "      <td>3015</td>\n",
       "      <td>1081</td>\n",
       "      <td>37.625</td>\n",
       "      <td>31.933333</td>\n",
       "      <td>64.066667</td>\n",
       "      <td>63.266667</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2361</td>\n",
       "      <td>1918</td>\n",
       "      <td>884</td>\n",
       "      <td>64.125</td>\n",
       "      <td>453.733333</td>\n",
       "      <td>515.933333</td>\n",
       "      <td>513.133333</td>\n",
       "      <td>575.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1918</td>\n",
       "      <td>1438</td>\n",
       "      <td>827</td>\n",
       "      <td>63.250</td>\n",
       "      <td>347.733333</td>\n",
       "      <td>407.733333</td>\n",
       "      <td>406.400000</td>\n",
       "      <td>466.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>69.375</td>\n",
       "      <td>-9.133333</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pLN1  p.2  pLN3  pt4  pi5  pe6  pLN7  p58  pLN9  pSH10    ...     \\\n",
       "0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "1   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "2   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "3   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "4   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "\n",
       "      du2a13     du2n14  du2n15   avgdu       avgud       avgdd       avguu  \\\n",
       "0       2302  670740857     973  37.875   24.466667   56.800000   55.866667   \n",
       "1  670740857       3015    1081  37.625   31.933333   64.066667   63.266667   \n",
       "2       2361       1918     884  64.125  453.733333  515.933333  513.133333   \n",
       "3       1918       1438     827  63.250  347.733333  407.733333  406.400000   \n",
       "4       2302  670740857     973  69.375   -9.133333   56.800000   55.866667   \n",
       "\n",
       "        avdu2  avgp      avga  \n",
       "0   88.200000   1.0  0.004412  \n",
       "1   95.400000   1.0  0.004167  \n",
       "2  575.333333   1.0  0.008333  \n",
       "3  466.400000   1.0  0.008211  \n",
       "4  121.800000   1.0  0.009804  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7700 entries, 0 to 7699\n",
      "Columns: 147 entries, pLN1 to avga\n",
      "dtypes: float64(71), int64(76)\n",
      "memory usage: 8.6 MB\n",
      "initial data info None\n",
      "data is (7700, 147)\n",
      "(7700, 147)\n",
      "(7700,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.85      0.79        20\n",
      "          1       0.79      0.95      0.86        20\n",
      "          2       0.95      0.95      0.95        20\n",
      "          3       0.95      0.95      0.95        20\n",
      "          4       0.68      0.65      0.67        20\n",
      "          5       1.00      1.00      1.00        20\n",
      "          6       1.00      0.95      0.97        20\n",
      "          7       0.71      0.85      0.77        20\n",
      "          8       0.25      0.40      0.31        20\n",
      "          9       0.50      0.60      0.55        20\n",
      "         10       0.40      0.30      0.34        20\n",
      "         11       0.95      1.00      0.98        20\n",
      "         12       0.82      0.70      0.76        20\n",
      "         13       0.75      0.45      0.56        20\n",
      "         14       0.89      0.80      0.84        20\n",
      "         15       0.29      0.50      0.37        20\n",
      "         16       1.00      0.90      0.95        20\n",
      "         17       1.00      0.95      0.97        20\n",
      "         18       0.13      0.10      0.11        20\n",
      "         19       0.90      0.90      0.90        20\n",
      "         20       0.72      0.65      0.68        20\n",
      "         21       0.81      0.85      0.83        20\n",
      "         22       0.95      0.95      0.95        20\n",
      "         23       0.35      0.40      0.37        20\n",
      "         24       0.60      0.15      0.24        20\n",
      "         25       0.20      0.10      0.13        20\n",
      "         26       0.67      0.70      0.68        20\n",
      "         27       0.18      0.25      0.21        20\n",
      "         28       0.93      0.70      0.80        20\n",
      "         29       0.72      0.90      0.80        20\n",
      "         30       0.86      0.60      0.71        20\n",
      "         31       0.58      0.35      0.44        20\n",
      "         32       0.90      0.95      0.93        20\n",
      "         33       0.65      0.75      0.70        20\n",
      "         34       0.52      0.65      0.58        20\n",
      "         35       0.71      0.60      0.65        20\n",
      "         36       0.79      0.75      0.77        20\n",
      "         37       0.67      0.50      0.57        20\n",
      "         38       1.00      1.00      1.00        20\n",
      "         39       0.64      0.80      0.71        20\n",
      "         40       0.65      0.65      0.65        20\n",
      "         41       0.37      0.65      0.47        20\n",
      "         42       0.67      0.60      0.63        20\n",
      "         43       0.87      1.00      0.93        20\n",
      "         44       0.78      0.70      0.74        20\n",
      "         45       0.48      0.60      0.53        20\n",
      "         46       0.33      0.45      0.38        20\n",
      "         47       1.00      1.00      1.00        20\n",
      "         48       0.55      0.55      0.55        20\n",
      "         49       1.00      0.95      0.97        20\n",
      "         50       0.38      0.50      0.43        20\n",
      "         51       0.13      0.10      0.11        20\n",
      "         52       0.35      0.30      0.32        20\n",
      "         53       0.64      0.70      0.67        20\n",
      "         54       0.86      0.90      0.88        20\n",
      "         55       0.73      0.95      0.83        20\n",
      "         56       0.90      0.90      0.90        20\n",
      "         57       0.91      1.00      0.95        20\n",
      "         58       0.72      0.65      0.68        20\n",
      "         59       0.78      0.90      0.84        20\n",
      "         60       1.00      1.00      1.00        20\n",
      "         61       0.88      0.70      0.78        20\n",
      "         62       0.54      0.65      0.59        20\n",
      "         63       0.90      0.95      0.93        20\n",
      "         64       0.70      0.80      0.74        20\n",
      "         65       0.00      0.00      0.00        20\n",
      "         66       0.59      0.80      0.68        20\n",
      "         67       0.30      0.15      0.20        20\n",
      "         68       0.90      0.90      0.90        20\n",
      "         69       1.00      1.00      1.00        20\n",
      "         70       0.65      0.75      0.70        20\n",
      "         71       1.00      0.95      0.97        20\n",
      "         72       0.54      0.65      0.59        20\n",
      "         73       0.67      0.40      0.50        20\n",
      "         74       1.00      1.00      1.00        20\n",
      "         75       0.17      0.15      0.16        20\n",
      "         76       0.67      0.50      0.57        20\n",
      "\n",
      "avg / total       0.69      0.69      0.68      1540\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.95      0.86        20\n",
      "          1       0.95      1.00      0.98        20\n",
      "          2       0.95      0.90      0.92        20\n",
      "          3       0.95      0.95      0.95        20\n",
      "          4       0.87      1.00      0.93        20\n",
      "          5       1.00      1.00      1.00        20\n",
      "          6       1.00      1.00      1.00        20\n",
      "          7       0.77      0.85      0.81        20\n",
      "          8       0.41      0.55      0.47        20\n",
      "          9       0.95      1.00      0.98        20\n",
      "         10       0.42      0.40      0.41        20\n",
      "         11       0.95      1.00      0.98        20\n",
      "         12       0.94      0.85      0.89        20\n",
      "         13       0.50      0.55      0.52        20\n",
      "         14       1.00      0.95      0.97        20\n",
      "         15       0.38      0.30      0.33        20\n",
      "         16       1.00      1.00      1.00        20\n",
      "         17       1.00      1.00      1.00        20\n",
      "         18       0.50      0.45      0.47        20\n",
      "         19       0.95      1.00      0.98        20\n",
      "         20       0.80      0.80      0.80        20\n",
      "         21       0.89      0.80      0.84        20\n",
      "         22       0.95      0.90      0.92        20\n",
      "         23       0.75      0.90      0.82        20\n",
      "         24       0.44      0.55      0.49        20\n",
      "         25       0.42      0.40      0.41        20\n",
      "         26       0.83      0.95      0.88        20\n",
      "         27       0.50      0.35      0.41        20\n",
      "         28       0.95      0.95      0.95        20\n",
      "         29       0.74      0.85      0.79        20\n",
      "         30       0.93      0.65      0.76        20\n",
      "         31       0.62      0.50      0.56        20\n",
      "         32       0.95      1.00      0.98        20\n",
      "         33       0.71      0.60      0.65        20\n",
      "         34       0.73      0.80      0.76        20\n",
      "         35       0.75      0.75      0.75        20\n",
      "         36       0.90      0.95      0.93        20\n",
      "         37       0.59      0.65      0.62        20\n",
      "         38       0.95      0.95      0.95        20\n",
      "         39       0.79      0.95      0.86        20\n",
      "         40       1.00      0.75      0.86        20\n",
      "         41       0.58      0.55      0.56        20\n",
      "         42       0.80      0.80      0.80        20\n",
      "         43       0.91      1.00      0.95        20\n",
      "         44       0.73      0.95      0.83        20\n",
      "         45       0.70      0.70      0.70        20\n",
      "         46       0.63      0.60      0.62        20\n",
      "         47       1.00      1.00      1.00        20\n",
      "         48       0.74      0.85      0.79        20\n",
      "         49       1.00      0.95      0.97        20\n",
      "         50       0.48      0.50      0.49        20\n",
      "         51       0.20      0.10      0.13        20\n",
      "         52       0.71      0.60      0.65        20\n",
      "         53       0.80      0.80      0.80        20\n",
      "         54       0.90      0.90      0.90        20\n",
      "         55       0.90      0.95      0.93        20\n",
      "         56       0.89      0.80      0.84        20\n",
      "         57       1.00      1.00      1.00        20\n",
      "         58       0.62      0.75      0.68        20\n",
      "         59       1.00      1.00      1.00        20\n",
      "         60       1.00      1.00      1.00        20\n",
      "         61       1.00      0.95      0.97        20\n",
      "         62       0.76      0.80      0.78        20\n",
      "         63       0.95      1.00      0.98        20\n",
      "         64       0.77      0.85      0.81        20\n",
      "         65       0.61      0.55      0.58        20\n",
      "         66       0.87      1.00      0.93        20\n",
      "         67       0.79      0.55      0.65        20\n",
      "         68       1.00      0.95      0.97        20\n",
      "         69       1.00      1.00      1.00        20\n",
      "         70       0.75      0.60      0.67        20\n",
      "         71       1.00      0.95      0.97        20\n",
      "         72       0.71      0.75      0.73        20\n",
      "         73       0.90      0.90      0.90        20\n",
      "         74       1.00      1.00      1.00        20\n",
      "         75       0.19      0.20      0.20        20\n",
      "         76       0.74      0.70      0.72        20\n",
      "\n",
      "avg / total       0.79      0.80      0.79      1540\n",
      "\n",
      "each loop acc 0.685064935065\n",
      "each loop rbf acc 0.795454545455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.90      0.92        20\n",
      "          1       0.94      0.80      0.86        20\n",
      "          2       1.00      1.00      1.00        20\n",
      "          3       1.00      0.90      0.95        20\n",
      "          4       0.60      0.75      0.67        20\n",
      "          5       1.00      0.95      0.97        20\n",
      "          6       1.00      1.00      1.00        20\n",
      "          7       0.90      0.90      0.90        20\n",
      "          8       0.24      0.40      0.30        20\n",
      "          9       0.69      0.90      0.78        20\n",
      "         10       0.30      0.35      0.33        20\n",
      "         11       0.86      0.95      0.90        20\n",
      "         12       1.00      1.00      1.00        20\n",
      "         13       0.95      1.00      0.98        20\n",
      "         14       0.95      0.95      0.95        20\n",
      "         15       0.52      0.55      0.54        20\n",
      "         16       1.00      0.90      0.95        20\n",
      "         17       1.00      0.95      0.97        20\n",
      "         18       1.00      1.00      1.00        20\n",
      "         19       0.87      1.00      0.93        20\n",
      "         20       0.78      0.70      0.74        20\n",
      "         21       1.00      0.95      0.97        20\n",
      "         22       0.95      0.95      0.95        20\n",
      "         23       0.60      0.60      0.60        20\n",
      "         24       0.50      0.20      0.29        20\n",
      "         25       0.44      0.20      0.28        20\n",
      "         26       0.71      0.75      0.73        20\n",
      "         27       0.31      0.40      0.35        20\n",
      "         28       0.83      0.75      0.79        20\n",
      "         29       0.94      0.80      0.86        20\n",
      "         30       1.00      1.00      1.00        20\n",
      "         31       0.74      0.85      0.79        20\n",
      "         32       0.91      1.00      0.95        20\n",
      "         33       0.62      0.65      0.63        20\n",
      "         34       0.67      0.60      0.63        20\n",
      "         35       0.71      0.75      0.73        20\n",
      "         36       1.00      0.90      0.95        20\n",
      "         37       0.52      0.55      0.54        20\n",
      "         38       1.00      1.00      1.00        20\n",
      "         39       0.48      0.80      0.60        20\n",
      "         40       0.67      0.50      0.57        20\n",
      "         41       0.43      0.65      0.52        20\n",
      "         42       0.65      0.85      0.74        20\n",
      "         43       0.90      0.95      0.93        20\n",
      "         44       1.00      0.85      0.92        20\n",
      "         45       0.50      0.65      0.57        20\n",
      "         46       0.35      0.55      0.43        20\n",
      "         47       1.00      1.00      1.00        20\n",
      "         48       0.47      0.35      0.40        20\n",
      "         49       1.00      1.00      1.00        20\n",
      "         50       0.47      0.40      0.43        20\n",
      "         51       0.00      0.00      0.00        20\n",
      "         52       1.00      0.85      0.92        20\n",
      "         53       0.80      0.60      0.69        20\n",
      "         54       0.95      1.00      0.98        20\n",
      "         55       0.77      0.85      0.81        20\n",
      "         56       0.83      0.95      0.88        20\n",
      "         57       0.86      0.90      0.88        20\n",
      "         58       0.63      0.60      0.62        20\n",
      "         59       1.00      1.00      1.00        20\n",
      "         60       1.00      1.00      1.00        20\n",
      "         61       1.00      1.00      1.00        20\n",
      "         62       1.00      0.95      0.97        20\n",
      "         63       0.95      1.00      0.98        20\n",
      "         64       0.73      0.80      0.76        20\n",
      "         65       0.00      0.00      0.00        20\n",
      "         66       0.69      0.90      0.78        20\n",
      "         67       0.39      0.35      0.37        20\n",
      "         68       1.00      0.95      0.97        20\n",
      "         69       1.00      1.00      1.00        20\n",
      "         70       0.78      0.70      0.74        20\n",
      "         71       1.00      1.00      1.00        20\n",
      "         72       0.53      0.80      0.64        20\n",
      "         73       0.93      0.70      0.80        20\n",
      "         74       1.00      1.00      1.00        20\n",
      "         75       0.33      0.35      0.34        20\n",
      "         76       0.73      0.40      0.52        20\n",
      "\n",
      "avg / total       0.76      0.77      0.76      1540\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.90      0.92        20\n",
      "          1       0.91      1.00      0.95        20\n",
      "          2       1.00      1.00      1.00        20\n",
      "          3       1.00      1.00      1.00        20\n",
      "          4       0.87      1.00      0.93        20\n",
      "          5       1.00      0.95      0.97        20\n",
      "          6       1.00      1.00      1.00        20\n",
      "          7       1.00      0.95      0.97        20\n",
      "          8       0.41      0.70      0.52        20\n",
      "          9       0.80      1.00      0.89        20\n",
      "         10       0.57      0.65      0.60        20\n",
      "         11       0.95      1.00      0.98        20\n",
      "         12       1.00      1.00      1.00        20\n",
      "         13       1.00      1.00      1.00        20\n",
      "         14       0.95      1.00      0.98        20\n",
      "         15       0.73      0.80      0.76        20\n",
      "         16       1.00      1.00      1.00        20\n",
      "         17       1.00      1.00      1.00        20\n",
      "         18       1.00      1.00      1.00        20\n",
      "         19       0.95      1.00      0.98        20\n",
      "         20       0.85      0.85      0.85        20\n",
      "         21       1.00      0.95      0.97        20\n",
      "         22       1.00      0.95      0.97        20\n",
      "         23       0.83      0.75      0.79        20\n",
      "         24       0.47      0.45      0.46        20\n",
      "         25       0.41      0.45      0.43        20\n",
      "         26       0.91      1.00      0.95        20\n",
      "         27       0.70      0.70      0.70        20\n",
      "         28       0.94      0.85      0.89        20\n",
      "         29       0.85      0.85      0.85        20\n",
      "         30       1.00      1.00      1.00        20\n",
      "         31       0.81      0.85      0.83        20\n",
      "         32       1.00      1.00      1.00        20\n",
      "         33       0.71      0.85      0.77        20\n",
      "         34       0.71      0.85      0.77        20\n",
      "         35       0.80      0.80      0.80        20\n",
      "         36       1.00      1.00      1.00        20\n",
      "         37       0.53      0.45      0.49        20\n",
      "         38       1.00      1.00      1.00        20\n",
      "         39       0.64      0.80      0.71        20\n",
      "         40       1.00      0.85      0.92        20\n",
      "         41       0.59      0.65      0.62        20\n",
      "         42       0.89      0.85      0.87        20\n",
      "         43       0.90      0.95      0.93        20\n",
      "         44       0.95      0.90      0.92        20\n",
      "         45       0.82      0.70      0.76        20\n",
      "         46       0.50      0.45      0.47        20\n",
      "         47       1.00      0.95      0.97        20\n",
      "         48       0.74      0.85      0.79        20\n",
      "         49       1.00      1.00      1.00        20\n",
      "         50       0.62      0.50      0.56        20\n",
      "         51       0.17      0.05      0.08        20\n",
      "         52       1.00      0.90      0.95        20\n",
      "         53       0.82      0.90      0.86        20\n",
      "         54       0.95      0.95      0.95        20\n",
      "         55       0.86      0.95      0.90        20\n",
      "         56       0.87      1.00      0.93        20\n",
      "         57       0.95      1.00      0.98        20\n",
      "         58       0.81      0.65      0.72        20\n",
      "         59       1.00      1.00      1.00        20\n",
      "         60       1.00      1.00      1.00        20\n",
      "         61       1.00      1.00      1.00        20\n",
      "         62       1.00      1.00      1.00        20\n",
      "         63       0.95      1.00      0.98        20\n",
      "         64       0.85      0.85      0.85        20\n",
      "         65       0.36      0.20      0.26        20\n",
      "         66       0.85      0.85      0.85        20\n",
      "         67       0.87      0.65      0.74        20\n",
      "         68       1.00      0.90      0.95        20\n",
      "         69       1.00      1.00      1.00        20\n",
      "         70       0.84      0.80      0.82        20\n",
      "         71       1.00      1.00      1.00        20\n",
      "         72       0.62      0.50      0.56        20\n",
      "         73       0.89      0.85      0.87        20\n",
      "         74       1.00      1.00      1.00        20\n",
      "         75       0.30      0.35      0.33        20\n",
      "         76       0.59      0.65      0.62        20\n",
      "\n",
      "avg / total       0.84      0.84      0.84      1540\n",
      "\n",
      "each loop acc 0.765584415584\n",
      "each loop rbf acc 0.844155844156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.85      0.87        20\n",
      "          1       0.94      0.75      0.83        20\n",
      "          2       1.00      1.00      1.00        20\n",
      "          3       1.00      0.95      0.97        20\n",
      "          4       0.47      0.45      0.46        20\n",
      "          5       1.00      1.00      1.00        20\n",
      "          6       1.00      1.00      1.00        20\n",
      "          7       0.83      1.00      0.91        20\n",
      "          8       0.20      0.10      0.13        20\n",
      "          9       0.78      0.90      0.84        20\n",
      "         10       0.53      0.50      0.51        20\n",
      "         11       1.00      1.00      1.00        20\n",
      "         12       1.00      1.00      1.00        20\n",
      "         13       0.95      0.95      0.95        20\n",
      "         14       1.00      1.00      1.00        20\n",
      "         15       0.38      0.50      0.43        20\n",
      "         16       0.95      0.90      0.92        20\n",
      "         17       0.95      0.95      0.95        20\n",
      "         18       1.00      1.00      1.00        20\n",
      "         19       0.85      0.85      0.85        20\n",
      "         20       0.69      0.55      0.61        20\n",
      "         21       0.89      0.85      0.87        20\n",
      "         22       0.89      0.85      0.87        20\n",
      "         23       0.61      0.55      0.58        20\n",
      "         24       0.38      0.25      0.30        20\n",
      "         25       0.48      0.55      0.51        20\n",
      "         26       0.80      0.80      0.80        20\n",
      "         27       0.29      0.25      0.27        20\n",
      "         28       0.71      0.75      0.73        20\n",
      "         29       0.79      0.95      0.86        20\n",
      "         30       1.00      1.00      1.00        20\n",
      "         31       0.92      0.55      0.69        20\n",
      "         32       0.95      0.95      0.95        20\n",
      "         33       0.62      0.80      0.70        20\n",
      "         34       0.58      0.75      0.65        20\n",
      "         35       0.82      0.90      0.86        20\n",
      "         36       0.95      0.95      0.95        20\n",
      "         37       0.68      0.65      0.67        20\n",
      "         38       1.00      1.00      1.00        20\n",
      "         39       0.56      0.75      0.64        20\n",
      "         40       0.48      0.50      0.49        20\n",
      "         41       0.44      0.40      0.42        20\n",
      "         42       0.79      0.75      0.77        20\n",
      "         43       0.86      0.90      0.88        20\n",
      "         44       0.79      0.95      0.86        20\n",
      "         45       0.48      0.75      0.59        20\n",
      "         46       0.33      0.40      0.36        20\n",
      "         47       1.00      1.00      1.00        20\n",
      "         48       0.64      0.45      0.53        20\n",
      "         49       1.00      1.00      1.00        20\n",
      "         50       0.50      0.45      0.47        20\n",
      "         51       0.14      0.15      0.14        20\n",
      "         52       0.95      1.00      0.98        20\n",
      "         53       0.76      0.80      0.78        20\n",
      "         54       1.00      0.90      0.95        20\n",
      "         55       0.74      0.70      0.72        20\n",
      "         56       0.86      0.90      0.88        20\n",
      "         57       0.90      0.95      0.93        20\n",
      "         58       0.71      0.50      0.59        20\n",
      "         59       1.00      1.00      1.00        20\n",
      "         60       1.00      1.00      1.00        20\n",
      "         61       1.00      1.00      1.00        20\n",
      "         62       0.95      0.95      0.95        20\n",
      "         63       1.00      0.95      0.97        20\n",
      "         64       0.62      0.75      0.68        20\n",
      "         65       0.12      0.15      0.13        20\n",
      "         66       0.88      0.75      0.81        20\n",
      "         67       0.42      0.25      0.31        20\n",
      "         68       0.90      0.95      0.93        20\n",
      "         69       1.00      1.00      1.00        20\n",
      "         70       0.88      0.75      0.81        20\n",
      "         71       1.00      0.95      0.97        20\n",
      "         72       0.59      0.80      0.68        20\n",
      "         73       0.77      0.85      0.81        20\n",
      "         74       1.00      1.00      1.00        20\n",
      "         75       0.40      0.30      0.34        20\n",
      "         76       0.72      0.65      0.68        20\n",
      "\n",
      "avg / total       0.77      0.76      0.76      1540\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        20\n",
      "          1       0.95      0.95      0.95        20\n",
      "          2       1.00      1.00      1.00        20\n",
      "          3       0.95      0.95      0.95        20\n",
      "          4       0.95      0.90      0.92        20\n",
      "          5       1.00      1.00      1.00        20\n",
      "          6       1.00      1.00      1.00        20\n",
      "          7       0.91      1.00      0.95        20\n",
      "          8       0.33      0.35      0.34        20\n",
      "          9       0.91      1.00      0.95        20\n",
      "         10       0.68      0.65      0.67        20\n",
      "         11       1.00      1.00      1.00        20\n",
      "         12       1.00      1.00      1.00        20\n",
      "         13       0.95      1.00      0.98        20\n",
      "         14       1.00      1.00      1.00        20\n",
      "         15       0.68      0.65      0.67        20\n",
      "         16       1.00      1.00      1.00        20\n",
      "         17       0.95      1.00      0.98        20\n",
      "         18       1.00      1.00      1.00        20\n",
      "         19       0.90      0.90      0.90        20\n",
      "         20       0.78      0.90      0.84        20\n",
      "         21       0.85      0.85      0.85        20\n",
      "         22       0.95      0.90      0.92        20\n",
      "         23       0.86      0.90      0.88        20\n",
      "         24       0.64      0.45      0.53        20\n",
      "         25       0.46      0.60      0.52        20\n",
      "         26       0.80      1.00      0.89        20\n",
      "         27       0.46      0.65      0.54        20\n",
      "         28       0.91      1.00      0.95        20\n",
      "         29       0.78      0.70      0.74        20\n",
      "         30       1.00      1.00      1.00        20\n",
      "         31       0.70      0.70      0.70        20\n",
      "         32       1.00      1.00      1.00        20\n",
      "         33       0.75      0.90      0.82        20\n",
      "         34       0.73      0.80      0.76        20\n",
      "         35       0.79      0.95      0.86        20\n",
      "         36       1.00      1.00      1.00        20\n",
      "         37       0.79      0.55      0.65        20\n",
      "         38       1.00      1.00      1.00        20\n",
      "         39       0.71      0.75      0.73        20\n",
      "         40       0.90      0.95      0.93        20\n",
      "         41       0.50      0.50      0.50        20\n",
      "         42       0.94      0.80      0.86        20\n",
      "         43       0.95      1.00      0.98        20\n",
      "         44       0.90      0.95      0.93        20\n",
      "         45       0.86      0.95      0.90        20\n",
      "         46       0.61      0.85      0.71        20\n",
      "         47       1.00      1.00      1.00        20\n",
      "         48       0.94      0.85      0.89        20\n",
      "         49       1.00      1.00      1.00        20\n",
      "         50       0.65      0.65      0.65        20\n",
      "         51       0.38      0.40      0.39        20\n",
      "         52       0.95      1.00      0.98        20\n",
      "         53       0.83      0.95      0.88        20\n",
      "         54       0.95      0.95      0.95        20\n",
      "         55       1.00      0.90      0.95        20\n",
      "         56       0.95      0.95      0.95        20\n",
      "         57       1.00      1.00      1.00        20\n",
      "         58       0.88      0.70      0.78        20\n",
      "         59       1.00      1.00      1.00        20\n",
      "         60       1.00      1.00      1.00        20\n",
      "         61       1.00      1.00      1.00        20\n",
      "         62       1.00      1.00      1.00        20\n",
      "         63       1.00      1.00      1.00        20\n",
      "         64       0.88      0.75      0.81        20\n",
      "         65       0.40      0.30      0.34        20\n",
      "         66       0.90      0.95      0.93        20\n",
      "         67       0.83      0.50      0.62        20\n",
      "         68       1.00      0.95      0.97        20\n",
      "         69       1.00      1.00      1.00        20\n",
      "         70       0.93      0.65      0.76        20\n",
      "         71       1.00      0.95      0.97        20\n",
      "         72       0.75      0.75      0.75        20\n",
      "         73       0.95      0.90      0.92        20\n",
      "         74       1.00      1.00      1.00        20\n",
      "         75       0.46      0.30      0.36        20\n",
      "         76       0.75      0.75      0.75        20\n",
      "\n",
      "avg / total       0.86      0.86      0.86      1540\n",
      "\n",
      "each loop acc 0.763636363636\n",
      "each loop rbf acc 0.857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.90      0.86        20\n",
      "          1       0.93      0.70      0.80        20\n",
      "          2       0.83      0.95      0.88        20\n",
      "          3       0.95      0.90      0.92        20\n",
      "          4       0.67      0.90      0.77        20\n",
      "          5       1.00      1.00      1.00        20\n",
      "          6       1.00      1.00      1.00        20\n",
      "          7       0.88      0.70      0.78        20\n",
      "          8       0.26      0.30      0.28        20\n",
      "          9       0.64      0.80      0.71        20\n",
      "         10       0.22      0.30      0.26        20\n",
      "         11       1.00      0.95      0.97        20\n",
      "         12       0.67      0.80      0.73        20\n",
      "         13       0.58      0.35      0.44        20\n",
      "         14       0.90      0.95      0.93        20\n",
      "         15       0.35      0.55      0.43        20\n",
      "         16       1.00      0.85      0.92        20\n",
      "         17       0.95      0.95      0.95        20\n",
      "         18       0.12      0.10      0.11        20\n",
      "         19       0.83      0.95      0.88        20\n",
      "         20       0.71      0.60      0.65        20\n",
      "         21       0.80      0.60      0.69        20\n",
      "         22       1.00      0.90      0.95        20\n",
      "         23       0.38      0.30      0.33        20\n",
      "         24       0.42      0.25      0.31        20\n",
      "         25       0.38      0.25      0.30        20\n",
      "         26       0.68      0.65      0.67        20\n",
      "         27       0.38      0.25      0.30        20\n",
      "         28       0.70      0.70      0.70        20\n",
      "         29       0.82      0.90      0.86        20\n",
      "         30       0.67      0.40      0.50        20\n",
      "         31       0.67      0.70      0.68        20\n",
      "         32       0.81      0.85      0.83        20\n",
      "         33       0.69      0.55      0.61        20\n",
      "         34       0.35      0.55      0.43        20\n",
      "         35       0.69      0.55      0.61        20\n",
      "         36       0.53      0.50      0.51        20\n",
      "         37       0.59      0.50      0.54        20\n",
      "         38       1.00      1.00      1.00        20\n",
      "         39       0.47      0.80      0.59        20\n",
      "         40       0.75      0.45      0.56        20\n",
      "         41       0.47      0.70      0.56        20\n",
      "         42       0.67      0.80      0.73        20\n",
      "         43       0.61      0.85      0.71        20\n",
      "         44       0.56      0.50      0.53        20\n",
      "         45       0.40      0.40      0.40        20\n",
      "         46       0.31      0.45      0.37        20\n",
      "         47       1.00      1.00      1.00        20\n",
      "         48       0.43      0.50      0.47        20\n",
      "         49       1.00      1.00      1.00        20\n",
      "         50       0.25      0.20      0.22        20\n",
      "         51       0.17      0.10      0.12        20\n",
      "         52       0.47      0.35      0.40        20\n",
      "         53       0.76      0.65      0.70        20\n",
      "         54       0.76      0.95      0.84        20\n",
      "         55       0.61      0.70      0.65        20\n",
      "         56       0.81      0.85      0.83        20\n",
      "         57       0.91      1.00      0.95        20\n",
      "         58       0.62      0.75      0.68        20\n",
      "         59       0.80      0.80      0.80        20\n",
      "         60       1.00      1.00      1.00        20\n",
      "         61       0.79      0.75      0.77        20\n",
      "         62       0.73      0.55      0.63        20\n",
      "         63       0.90      0.95      0.93        20\n",
      "         64       0.62      0.65      0.63        20\n",
      "         65       0.33      0.10      0.15        20\n",
      "         66       0.59      0.80      0.68        20\n",
      "         67       0.38      0.30      0.33        20\n",
      "         68       0.73      0.55      0.63        20\n",
      "         69       1.00      1.00      1.00        20\n",
      "         70       0.68      0.75      0.71        20\n",
      "         71       0.95      0.90      0.92        20\n",
      "         72       0.50      0.65      0.57        20\n",
      "         73       0.76      0.65      0.70        20\n",
      "         74       0.95      1.00      0.98        20\n",
      "         75       0.20      0.25      0.22        20\n",
      "         76       0.67      0.60      0.63        20\n",
      "\n",
      "avg / total       0.67      0.66      0.66      1540\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.85      0.85        20\n",
      "          1       0.95      1.00      0.98        20\n",
      "          2       0.86      0.95      0.90        20\n",
      "          3       1.00      0.90      0.95        20\n",
      "          4       0.90      0.90      0.90        20\n",
      "          5       1.00      1.00      1.00        20\n",
      "          6       1.00      1.00      1.00        20\n",
      "          7       0.89      0.85      0.87        20\n",
      "          8       0.33      0.35      0.34        20\n",
      "          9       0.82      0.90      0.86        20\n",
      "         10       0.41      0.45      0.43        20\n",
      "         11       1.00      1.00      1.00        20\n",
      "         12       0.73      0.95      0.83        20\n",
      "         13       0.53      0.45      0.49        20\n",
      "         14       1.00      1.00      1.00        20\n",
      "         15       0.57      0.65      0.60        20\n",
      "         16       1.00      1.00      1.00        20\n",
      "         17       0.95      1.00      0.98        20\n",
      "         18       0.69      0.45      0.55        20\n",
      "         19       0.95      1.00      0.98        20\n",
      "         20       0.82      0.90      0.86        20\n",
      "         21       0.79      0.75      0.77        20\n",
      "         22       0.86      0.90      0.88        20\n",
      "         23       0.79      0.55      0.65        20\n",
      "         24       0.47      0.35      0.40        20\n",
      "         25       0.39      0.60      0.47        20\n",
      "         26       0.83      0.95      0.88        20\n",
      "         27       0.57      0.60      0.59        20\n",
      "         28       0.91      1.00      0.95        20\n",
      "         29       0.86      0.90      0.88        20\n",
      "         30       0.71      0.75      0.73        20\n",
      "         31       0.74      0.70      0.72        20\n",
      "         32       1.00      1.00      1.00        20\n",
      "         33       0.80      0.60      0.69        20\n",
      "         34       0.56      0.75      0.64        20\n",
      "         35       0.63      0.60      0.62        20\n",
      "         36       0.71      0.60      0.65        20\n",
      "         37       0.53      0.45      0.49        20\n",
      "         38       1.00      1.00      1.00        20\n",
      "         39       0.68      0.85      0.76        20\n",
      "         40       0.95      0.95      0.95        20\n",
      "         41       0.58      0.35      0.44        20\n",
      "         42       0.77      0.85      0.81        20\n",
      "         43       0.85      0.85      0.85        20\n",
      "         44       0.83      0.75      0.79        20\n",
      "         45       0.85      0.85      0.85        20\n",
      "         46       0.54      0.65      0.59        20\n",
      "         47       1.00      1.00      1.00        20\n",
      "         48       0.95      1.00      0.98        20\n",
      "         49       1.00      1.00      1.00        20\n",
      "         50       0.50      0.50      0.50        20\n",
      "         51       0.25      0.20      0.22        20\n",
      "         52       0.67      0.80      0.73        20\n",
      "         53       0.83      0.75      0.79        20\n",
      "         54       0.90      0.95      0.93        20\n",
      "         55       0.95      0.90      0.92        20\n",
      "         56       0.86      0.90      0.88        20\n",
      "         57       1.00      1.00      1.00        20\n",
      "         58       0.68      0.85      0.76        20\n",
      "         59       1.00      1.00      1.00        20\n",
      "         60       1.00      1.00      1.00        20\n",
      "         61       1.00      1.00      1.00        20\n",
      "         62       0.67      0.60      0.63        20\n",
      "         63       1.00      0.90      0.95        20\n",
      "         64       0.88      0.70      0.78        20\n",
      "         65       0.38      0.30      0.33        20\n",
      "         66       0.67      0.90      0.77        20\n",
      "         67       0.73      0.55      0.63        20\n",
      "         68       1.00      0.75      0.86        20\n",
      "         69       1.00      1.00      1.00        20\n",
      "         70       0.64      0.70      0.67        20\n",
      "         71       0.95      0.90      0.92        20\n",
      "         72       0.56      0.50      0.53        20\n",
      "         73       0.84      0.80      0.82        20\n",
      "         74       1.00      1.00      1.00        20\n",
      "         75       0.40      0.40      0.40        20\n",
      "         76       0.62      0.65      0.63        20\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1540\n",
      "\n",
      "each loop acc 0.664285714286\n",
      "each loop rbf acc 0.781818181818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95        20\n",
      "          1       0.95      1.00      0.98        20\n",
      "          2       1.00      1.00      1.00        20\n",
      "          3       0.95      0.90      0.92        20\n",
      "          4       0.58      0.70      0.64        20\n",
      "          5       1.00      1.00      1.00        20\n",
      "          6       1.00      1.00      1.00        20\n",
      "          7       0.86      0.90      0.88        20\n",
      "          8       0.34      0.50      0.41        20\n",
      "          9       0.64      0.90      0.75        20\n",
      "         10       0.33      0.35      0.34        20\n",
      "         11       1.00      0.95      0.97        20\n",
      "         12       0.91      1.00      0.95        20\n",
      "         13       0.95      0.95      0.95        20\n",
      "         14       0.83      0.95      0.88        20\n",
      "         15       0.44      0.60      0.51        20\n",
      "         16       0.86      0.95      0.90        20\n",
      "         17       0.95      0.90      0.92        20\n",
      "         18       1.00      1.00      1.00        20\n",
      "         19       0.95      0.90      0.92        20\n",
      "         20       0.57      0.60      0.59        20\n",
      "         21       0.95      0.95      0.95        20\n",
      "         22       0.95      1.00      0.98        20\n",
      "         23       0.48      0.50      0.49        20\n",
      "         24       0.45      0.25      0.32        20\n",
      "         25       0.46      0.30      0.36        20\n",
      "         26       0.67      0.50      0.57        20\n",
      "         27       0.26      0.25      0.26        20\n",
      "         28       0.75      0.60      0.67        20\n",
      "         29       0.74      0.85      0.79        20\n",
      "         30       0.95      1.00      0.98        20\n",
      "         31       0.79      0.55      0.65        20\n",
      "         32       0.90      0.95      0.93        20\n",
      "         33       0.63      0.60      0.62        20\n",
      "         34       0.68      0.75      0.71        20\n",
      "         35       0.81      0.65      0.72        20\n",
      "         36       1.00      0.85      0.92        20\n",
      "         37       0.38      0.45      0.41        20\n",
      "         38       1.00      1.00      1.00        20\n",
      "         39       0.47      0.80      0.59        20\n",
      "         40       0.59      0.50      0.54        20\n",
      "         41       0.30      0.35      0.33        20\n",
      "         42       0.70      0.70      0.70        20\n",
      "         43       0.90      0.95      0.93        20\n",
      "         44       0.79      0.75      0.77        20\n",
      "         45       0.57      0.65      0.60        20\n",
      "         46       0.35      0.40      0.37        20\n",
      "         47       1.00      1.00      1.00        20\n",
      "         48       0.56      0.45      0.50        20\n",
      "         49       0.95      1.00      0.98        20\n",
      "         50       0.31      0.20      0.24        20\n",
      "         51       0.00      0.00      0.00        20\n",
      "         52       0.91      1.00      0.95        20\n",
      "         53       0.70      0.70      0.70        20\n",
      "         54       0.78      0.90      0.84        20\n",
      "         55       0.67      0.80      0.73        20\n",
      "         56       1.00      0.90      0.95        20\n",
      "         57       0.94      0.85      0.89        20\n",
      "         58       0.62      0.65      0.63        20\n",
      "         59       1.00      1.00      1.00        20\n",
      "         60       1.00      0.95      0.97        20\n",
      "         61       1.00      1.00      1.00        20\n",
      "         62       1.00      1.00      1.00        20\n",
      "         63       0.95      1.00      0.98        20\n",
      "         64       0.56      0.50      0.53        20\n",
      "         65       0.50      0.15      0.23        20\n",
      "         66       0.64      0.80      0.71        20\n",
      "         67       0.23      0.15      0.18        20\n",
      "         68       0.95      0.90      0.92        20\n",
      "         69       1.00      1.00      1.00        20\n",
      "         70       0.71      0.85      0.77        20\n",
      "         71       0.95      1.00      0.98        20\n",
      "         72       0.50      0.55      0.52        20\n",
      "         73       0.82      0.45      0.58        20\n",
      "         74       1.00      0.90      0.95        20\n",
      "         75       0.22      0.40      0.28        20\n",
      "         76       0.53      0.50      0.51        20\n",
      "\n",
      "avg / total       0.73      0.74      0.73      1540\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        20\n",
      "          1       0.95      1.00      0.98        20\n",
      "          2       1.00      1.00      1.00        20\n",
      "          3       0.90      0.95      0.93        20\n",
      "          4       0.89      0.80      0.84        20\n",
      "          5       1.00      1.00      1.00        20\n",
      "          6       1.00      1.00      1.00        20\n",
      "          7       0.95      0.95      0.95        20\n",
      "          8       0.48      0.55      0.51        20\n",
      "          9       0.74      1.00      0.85        20\n",
      "         10       0.57      0.60      0.59        20\n",
      "         11       1.00      1.00      1.00        20\n",
      "         12       0.95      0.95      0.95        20\n",
      "         13       0.95      0.95      0.95        20\n",
      "         14       0.90      0.95      0.93        20\n",
      "         15       0.68      0.65      0.67        20\n",
      "         16       0.95      0.95      0.95        20\n",
      "         17       1.00      0.90      0.95        20\n",
      "         18       1.00      1.00      1.00        20\n",
      "         19       0.95      0.95      0.95        20\n",
      "         20       0.68      0.75      0.71        20\n",
      "         21       0.95      0.95      0.95        20\n",
      "         22       0.87      1.00      0.93        20\n",
      "         23       0.94      0.85      0.89        20\n",
      "         24       0.50      0.50      0.50        20\n",
      "         25       0.33      0.50      0.40        20\n",
      "         26       0.84      0.80      0.82        20\n",
      "         27       0.52      0.55      0.54        20\n",
      "         28       0.90      0.90      0.90        20\n",
      "         29       0.84      0.80      0.82        20\n",
      "         30       1.00      1.00      1.00        20\n",
      "         31       0.85      0.85      0.85        20\n",
      "         32       1.00      1.00      1.00        20\n",
      "         33       0.90      0.90      0.90        20\n",
      "         34       0.88      0.70      0.78        20\n",
      "         35       0.82      0.90      0.86        20\n",
      "         36       1.00      0.95      0.97        20\n",
      "         37       0.71      0.25      0.37        20\n",
      "         38       0.91      1.00      0.95        20\n",
      "         39       0.64      0.80      0.71        20\n",
      "         40       0.78      0.90      0.84        20\n",
      "         41       0.57      0.65      0.60        20\n",
      "         42       0.71      0.75      0.73        20\n",
      "         43       0.91      1.00      0.95        20\n",
      "         44       0.83      0.75      0.79        20\n",
      "         45       0.89      0.80      0.84        20\n",
      "         46       0.67      0.60      0.63        20\n",
      "         47       1.00      1.00      1.00        20\n",
      "         48       0.80      0.80      0.80        20\n",
      "         49       1.00      1.00      1.00        20\n",
      "         50       0.60      0.60      0.60        20\n",
      "         51       0.30      0.15      0.20        20\n",
      "         52       0.95      1.00      0.98        20\n",
      "         53       0.74      0.70      0.72        20\n",
      "         54       0.86      0.95      0.90        20\n",
      "         55       0.90      0.90      0.90        20\n",
      "         56       1.00      0.95      0.97        20\n",
      "         57       0.95      0.90      0.92        20\n",
      "         58       0.84      0.80      0.82        20\n",
      "         59       1.00      1.00      1.00        20\n",
      "         60       1.00      0.95      0.97        20\n",
      "         61       1.00      1.00      1.00        20\n",
      "         62       1.00      1.00      1.00        20\n",
      "         63       1.00      1.00      1.00        20\n",
      "         64       0.71      0.60      0.65        20\n",
      "         65       0.38      0.30      0.33        20\n",
      "         66       0.95      0.90      0.92        20\n",
      "         67       0.64      0.70      0.67        20\n",
      "         68       1.00      0.85      0.92        20\n",
      "         69       1.00      1.00      1.00        20\n",
      "         70       0.89      0.85      0.87        20\n",
      "         71       0.95      1.00      0.98        20\n",
      "         72       0.61      0.55      0.58        20\n",
      "         73       0.90      0.95      0.93        20\n",
      "         74       1.00      0.90      0.95        20\n",
      "         75       0.29      0.40      0.33        20\n",
      "         76       0.59      0.65      0.62        20\n",
      "\n",
      "avg / total       0.83      0.83      0.83      1540\n",
      "\n",
      "each loop acc 0.735714285714\n",
      "each loop rbf acc 0.829220779221\n",
      "f1  0.7168702733587476\n",
      "Accuracy: 0.7228571428571429\n",
      "f1  0.8187182080494229\n",
      "Accuracy: 0.8215584415584415\n",
      "here\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAADqCAYAAAClduxYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKdmLiIikOCV7ERGRFPf/ATxv6uEBPusbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xda19b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import lsanomaly\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn import utils  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "\n",
    "# import the CSV from http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "# this will return a pandas dataframe.\n",
    "data = pd.read_csv('C:/Users/S/Documents/PY/increased100features.csv', low_memory=False)\n",
    "'''data.loc[data['UUID'] == \"RVTNB1502866560357\", \"attack\"] = 1  \n",
    "data.loc[data['UUID'] != \"RVTNB1502866560357\", \"attack\"] = -1\n",
    "df_majority = data[data['attack']==-1]\n",
    "df_minority = data[data['attack']==1]\n",
    "from sklearn.utils import resample\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=830,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#print(data['attack'].value_counts())'''\n",
    "\n",
    "#target=np.array(target)\n",
    "#target = pd.DataFrame(target,columns=['attack'])\n",
    "\n",
    "#data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "categorical_columns=[\"UUID\"]\n",
    "cate_data = data[categorical_columns]\n",
    "\n",
    "#for col in data.columns.values:\n",
    "#    print(col, data[col].unique())\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def label_encode(cate_data, columns):\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        col_values_unique = list(cate_data[col].unique())\n",
    "        le_fitted = le.fit(col_values_unique)\n",
    " \n",
    "        col_values = list(cate_data[col].values)\n",
    "        le.classes_\n",
    "        col_values_transformed = le.transform(col_values)\n",
    "        cate_data[col] = col_values_transformed\n",
    " \n",
    "to_be_encoded_cols = cate_data.columns.values\n",
    "label_encode(cate_data, to_be_encoded_cols)\n",
    "display(cate_data.head())\n",
    "target=cate_data['UUID']\n",
    "target=np.array(target)\n",
    "#target = pd.DataFrame(target)\n",
    "#target=target1.values\n",
    "\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "data=pd.concat([data,cate_data], axis=1)\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "\n",
    "# check the shape for sanity checking.\n",
    "data.shape\n",
    "display(data.head())\n",
    "print(\"initial data info\",data.info())\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"data is\",data.shape)\n",
    "from skfeature.function.information_theoretical_based import LCSI\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "\n",
    "from skfeature.utility.entropy_estimators import *\n",
    "import scipy.io\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#scaled_data=data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaleddata= pd.DataFrame(scaled_data)\n",
    "scaled_data=np.array(scaled_data)\n",
    "\n",
    "print(scaled_data.shape)\n",
    "print(target.shape)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "#display(target.head())\n",
    "#idx=MRMR.mrmr(scaled_data,target,n_selected_features=50)\n",
    "'''from sklearn import cross_validation\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "correct = 0\n",
    "print(\"scaled data details - \",scaled_data.info())\n",
    "print(\"target data details - \",target.info())\n",
    "for train, test in ss:\n",
    "    #print(scaled_data[train])\n",
    "    #print(target[train])\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=50)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:50]]\n",
    "print(features)    '''\n",
    "'''skb= SVC(kernel=\"linear\")\n",
    "rfe = RFE(estimator=skb, n_features_to_select=70)\n",
    "rfe=rfe.fit(scaleddata,target)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "skft = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "for train, test in skft:\n",
    "    X_train,X_test=scaled_data.iloc[train],scaled_data.iloc[test]\n",
    "    Y_train,y_test=target.iloc[train],target.iloc[test]\n",
    "    model1 = svm.OneClassSVM(nu=nu, kernel='rbf', gamma=0.10000000000000001)  \n",
    "    model1.fit(X_train, Y_train)\n",
    "    scores = cross_val_score(model1,X_test,y_test, cv=5, scoring='accuracy')\n",
    "    print(scores)\n",
    "print(scores.mean())'''\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5,random_state=36851234)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "'''clf = svm.SVC(decision_function_shape='ovo',kernel='rbf')    # linear SVM\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=skf)\n",
    "grid.fit(scaled_data, target)\n",
    "print(\"The best parameters are %s with a score of %0.2f\"% (grid.best_params_, grid.best_score_))'''\n",
    "plt.figure(figsize=(8, 8))\n",
    "accuracy = plt.subplot(211)\n",
    "box=plt.subplot(211)\n",
    "from sklearn.svm import LinearSVC\n",
    "clf=SVC(kernel='linear')\n",
    "#clf=LinearSVC()\n",
    "rbf=SVC(decision_function_shape='ovo',gamma=0.001,C=1000000.0)\n",
    "correct = 0\n",
    "fscoreTotal =0\n",
    "rbf_correct = 0\n",
    "rbf_fscoreTotal =0\n",
    "results=[]\n",
    "for train, test in skf.split(scaled_data,target):\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=36)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:79]]\n",
    "        #print(target[train])\n",
    "        # train a classification model with the selected features on the training dataset\n",
    "    clf.fit(features[train], target[train])\n",
    "    #clf1.fit(scaled_data[train],target[train])\n",
    "    rbf.fit(features[train], target[train])\n",
    "        # predict the class labels of test data\n",
    "    y_predict = clf.predict(features[test])\n",
    "    #y_predict = clf1.predict(scaled_data[test])\n",
    "    rbf_y_predict = rbf.predict(features[test])\n",
    "    print(\"metrics\")\n",
    "        # obtain the classification accuracy on the test data\n",
    "    acc = accuracy_score(target[test], y_predict)\n",
    "    rbf_acc = accuracy_score(target[test], rbf_y_predict)\n",
    "    correct = correct + acc\n",
    "    rbf_correct = rbf_correct + rbf_acc\n",
    "    fscore=f1_score(target[test], y_predict,average='weighted')\n",
    "    rbf_fscore=f1_score(target[test], rbf_y_predict,average='weighted')\n",
    "    fscoreTotal=fscoreTotal+fscore\n",
    "    rbf_fscoreTotal=rbf_fscoreTotal+rbf_fscore\n",
    "        #print(\"fsc \",f1_score(target[test], y_predict,average='weighted'))\n",
    "        #print(\"conf mat \",confusion_matrix(target[test],y_predict))\n",
    "        #print(\"ACCURACY: \", (accuracy_score(target[test], y_predict)))\n",
    "    report = classification_report(target[test], y_predict)\n",
    "    print(report)\n",
    "    rbreport = classification_report(target[test], rbf_y_predict)\n",
    "    print(rbreport)\n",
    "    print(\"each loop acc\",acc)\n",
    "    print(\"each loop rbf acc\",rbf_acc)\n",
    "score=float(correct)/5\n",
    "rbfscore=float(rbf_correct)/5\n",
    "results.append(score)\n",
    "results.append(rbfscore)\n",
    "print(\"f1 \",float(fscoreTotal)/5)\n",
    "    # output the average classification accuracy over all 10 folds\n",
    "print(\"Accuracy:\", float(correct)/5)\n",
    "print(\"f1 \",float(rbf_fscoreTotal)/5)\n",
    "    # output the average classification accuracy over all 10 folds\n",
    "print(\"Accuracy:\", float(rbf_correct)/5)\n",
    "\n",
    "accuracy.plot([scaled_data.shape[1], scaled_data.shape[0]],\n",
    "              [score, score], label=\"linear svm\")\n",
    "accuracy.plot([scaled_data.shape[1], scaled_data.shape[0]],\n",
    "              [rbfscore, rbfscore], label=\"rbf svm with grid search C=1000000 and gamma=0.001\")\n",
    "accuracy.set_title(\"Classification accuracy\")\n",
    "accuracy.set_xlim(scaled_data.shape[1], scaled_data.shape[0])\n",
    "accuracy.set_xticks(())\n",
    "accuracy.set_ylim(0.80, 0.90)\n",
    "accuracy.set_ylabel(\"Classification accuracy\")\n",
    "accuracy.legend(loc='best')\n",
    "##svc=SelectKBest(mutual_info_classif, k=50).fit_transform(data,target)\n",
    "#svc = SVC(kernel=\"linear\")\n",
    "#rfe = RFE(estimator=svc, n_features_to_select=10)\n",
    "#rfe.fit(data, target)\n",
    "print(\"here\")\n",
    "box.boxplot(results)\n",
    "box.set_title(\"Classification accuracy\")\n",
    "box.set_xlim(scaled_data.shape[1], scaled_data.shape[0])\n",
    "box.set_xticks(())\n",
    "box.set_ylim(0.80, 0.90)\n",
    "box.set_ylabel(\"Classification accuracy\")\n",
    "box.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "      <th>Language</th>\n",
       "      <th>Hardware_Model</th>\n",
       "      <th>SDK_Version</th>\n",
       "      <th>Manufacture</th>\n",
       "      <th>Screen_Size</th>\n",
       "      <th>Time_Zone</th>\n",
       "      <th>Country_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UUID  Language  Hardware_Model  SDK_Version  Manufacture  Screen_Size  \\\n",
       "0    48         0              40            3           18           23   \n",
       "1    48         0              40            3           18           23   \n",
       "2    48         0              40            3           18           23   \n",
       "3    48         0              40            3           18           23   \n",
       "4    48         0              40            3           18           23   \n",
       "\n",
       "   Time_Zone  Country_Code  \n",
       "0          7             1  \n",
       "1          7             1  \n",
       "2          7             1  \n",
       "3          7             1  \n",
       "4          7             1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num_of_CPU_Cores</th>\n",
       "      <th>pLN1</th>\n",
       "      <th>p.2</th>\n",
       "      <th>pLN3</th>\n",
       "      <th>pt4</th>\n",
       "      <th>pi5</th>\n",
       "      <th>pe6</th>\n",
       "      <th>pLN7</th>\n",
       "      <th>p58</th>\n",
       "      <th>pLN9</th>\n",
       "      <th>...</th>\n",
       "      <th>avdu2</th>\n",
       "      <th>avgp</th>\n",
       "      <th>avga</th>\n",
       "      <th>Language</th>\n",
       "      <th>Hardware_Model</th>\n",
       "      <th>SDK_Version</th>\n",
       "      <th>Manufacture</th>\n",
       "      <th>Screen_Size</th>\n",
       "      <th>Time_Zone</th>\n",
       "      <th>Country_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>88.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>575.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>466.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Num_of_CPU_Cores  pLN1  p.2  pLN3  pt4  pi5  pe6  pLN7  p58  pLN9  \\\n",
       "0               8.0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0   \n",
       "1               8.0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0   \n",
       "2               8.0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0   \n",
       "3               8.0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0   \n",
       "4               8.0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0   \n",
       "\n",
       "       ...            avdu2  avgp      avga  Language  Hardware_Model  \\\n",
       "0      ...        88.200000   1.0  0.004412         0              40   \n",
       "1      ...        95.400000   1.0  0.004167         0              40   \n",
       "2      ...       575.333333   1.0  0.008333         0              40   \n",
       "3      ...       466.400000   1.0  0.008211         0              40   \n",
       "4      ...       121.800000   1.0  0.009804         0              40   \n",
       "\n",
       "   SDK_Version  Manufacture  Screen_Size  Time_Zone  Country_Code  \n",
       "0            3           18           23          7             1  \n",
       "1            3           18           23          7             1  \n",
       "2            3           18           23          7             1  \n",
       "3            3           18           23          7             1  \n",
       "4            3           18           23          7             1  \n",
       "\n",
       "[5 rows x 155 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 770 entries, 0 to 769\n",
      "Columns: 155 entries, Num_of_CPU_Cores to Country_Code\n",
      "dtypes: float64(148), int64(7)\n",
      "memory usage: 932.5 KB\n",
      "initial data info None\n",
      "data is (770, 155)\n",
      "(770, 155)\n",
      "(770,)\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         2\n",
      "          1       1.00      1.00      1.00         2\n",
      "          2       1.00      1.00      1.00         2\n",
      "          3       1.00      1.00      1.00         2\n",
      "          4       0.67      1.00      0.80         2\n",
      "          5       1.00      1.00      1.00         2\n",
      "          6       1.00      1.00      1.00         2\n",
      "          7       1.00      1.00      1.00         2\n",
      "          8       0.67      1.00      0.80         2\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       1.00      1.00      1.00         2\n",
      "         11       1.00      1.00      1.00         2\n",
      "         12       1.00      1.00      1.00         2\n",
      "         13       1.00      1.00      1.00         2\n",
      "         14       1.00      1.00      1.00         2\n",
      "         15       1.00      1.00      1.00         2\n",
      "         16       1.00      1.00      1.00         2\n",
      "         17       1.00      1.00      1.00         2\n",
      "         18       1.00      1.00      1.00         2\n",
      "         19       0.67      1.00      0.80         2\n",
      "         20       1.00      1.00      1.00         2\n",
      "         21       1.00      1.00      1.00         2\n",
      "         22       1.00      1.00      1.00         2\n",
      "         23       1.00      1.00      1.00         2\n",
      "         24       1.00      1.00      1.00         2\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00         2\n",
      "         27       1.00      0.50      0.67         2\n",
      "         28       1.00      1.00      1.00         2\n",
      "         29       1.00      1.00      1.00         2\n",
      "         30       1.00      1.00      1.00         2\n",
      "         31       1.00      1.00      1.00         2\n",
      "         32       1.00      1.00      1.00         2\n",
      "         33       1.00      1.00      1.00         2\n",
      "         34       1.00      1.00      1.00         2\n",
      "         35       1.00      1.00      1.00         2\n",
      "         36       1.00      1.00      1.00         2\n",
      "         37       1.00      1.00      1.00         2\n",
      "         38       1.00      1.00      1.00         2\n",
      "         39       1.00      1.00      1.00         2\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       1.00      1.00      1.00         2\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         2\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00         2\n",
      "         46       1.00      1.00      1.00         2\n",
      "         47       1.00      1.00      1.00         2\n",
      "         48       1.00      1.00      1.00         2\n",
      "         49       1.00      1.00      1.00         2\n",
      "         50       1.00      1.00      1.00         2\n",
      "         51       1.00      1.00      1.00         2\n",
      "         52       1.00      1.00      1.00         2\n",
      "         53       1.00      1.00      1.00         2\n",
      "         54       1.00      1.00      1.00         2\n",
      "         55       1.00      1.00      1.00         2\n",
      "         56       1.00      1.00      1.00         2\n",
      "         57       1.00      1.00      1.00         2\n",
      "         58       1.00      1.00      1.00         2\n",
      "         59       1.00      1.00      1.00         2\n",
      "         60       1.00      1.00      1.00         2\n",
      "         61       1.00      1.00      1.00         2\n",
      "         62       1.00      1.00      1.00         2\n",
      "         63       1.00      1.00      1.00         2\n",
      "         64       1.00      1.00      1.00         2\n",
      "         65       1.00      1.00      1.00         2\n",
      "         66       1.00      0.50      0.67         2\n",
      "         67       1.00      0.50      0.67         2\n",
      "         68       1.00      1.00      1.00         2\n",
      "         69       1.00      1.00      1.00         2\n",
      "         70       1.00      1.00      1.00         2\n",
      "         71       1.00      1.00      1.00         2\n",
      "         72       1.00      1.00      1.00         2\n",
      "         73       1.00      1.00      1.00         2\n",
      "         74       1.00      1.00      1.00         2\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       0.99      0.98      0.98       154\n",
      "\n",
      "each loop acc 0.980519480519\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         2\n",
      "          1       1.00      1.00      1.00         2\n",
      "          2       1.00      1.00      1.00         2\n",
      "          3       1.00      1.00      1.00         2\n",
      "          4       1.00      1.00      1.00         2\n",
      "          5       1.00      1.00      1.00         2\n",
      "          6       1.00      1.00      1.00         2\n",
      "          7       1.00      1.00      1.00         2\n",
      "          8       1.00      1.00      1.00         2\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       1.00      1.00      1.00         2\n",
      "         11       1.00      1.00      1.00         2\n",
      "         12       1.00      1.00      1.00         2\n",
      "         13       1.00      1.00      1.00         2\n",
      "         14       1.00      1.00      1.00         2\n",
      "         15       1.00      1.00      1.00         2\n",
      "         16       1.00      1.00      1.00         2\n",
      "         17       1.00      1.00      1.00         2\n",
      "         18       1.00      1.00      1.00         2\n",
      "         19       1.00      1.00      1.00         2\n",
      "         20       1.00      1.00      1.00         2\n",
      "         21       1.00      1.00      1.00         2\n",
      "         22       1.00      1.00      1.00         2\n",
      "         23       1.00      1.00      1.00         2\n",
      "         24       1.00      1.00      1.00         2\n",
      "         25       0.67      1.00      0.80         2\n",
      "         26       1.00      1.00      1.00         2\n",
      "         27       1.00      1.00      1.00         2\n",
      "         28       1.00      1.00      1.00         2\n",
      "         29       1.00      0.50      0.67         2\n",
      "         30       1.00      1.00      1.00         2\n",
      "         31       0.67      1.00      0.80         2\n",
      "         32       1.00      1.00      1.00         2\n",
      "         33       1.00      1.00      1.00         2\n",
      "         34       1.00      1.00      1.00         2\n",
      "         35       0.67      1.00      0.80         2\n",
      "         36       1.00      1.00      1.00         2\n",
      "         37       1.00      1.00      1.00         2\n",
      "         38       1.00      1.00      1.00         2\n",
      "         39       1.00      1.00      1.00         2\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       1.00      0.50      0.67         2\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         2\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00         2\n",
      "         46       1.00      1.00      1.00         2\n",
      "         47       1.00      1.00      1.00         2\n",
      "         48       1.00      1.00      1.00         2\n",
      "         49       1.00      1.00      1.00         2\n",
      "         50       1.00      1.00      1.00         2\n",
      "         51       1.00      1.00      1.00         2\n",
      "         52       1.00      1.00      1.00         2\n",
      "         53       1.00      1.00      1.00         2\n",
      "         54       1.00      1.00      1.00         2\n",
      "         55       1.00      1.00      1.00         2\n",
      "         56       1.00      1.00      1.00         2\n",
      "         57       1.00      1.00      1.00         2\n",
      "         58       1.00      1.00      1.00         2\n",
      "         59       1.00      1.00      1.00         2\n",
      "         60       1.00      1.00      1.00         2\n",
      "         61       1.00      1.00      1.00         2\n",
      "         62       1.00      1.00      1.00         2\n",
      "         63       1.00      1.00      1.00         2\n",
      "         64       1.00      1.00      1.00         2\n",
      "         65       1.00      1.00      1.00         2\n",
      "         66       1.00      1.00      1.00         2\n",
      "         67       1.00      1.00      1.00         2\n",
      "         68       1.00      1.00      1.00         2\n",
      "         69       1.00      1.00      1.00         2\n",
      "         70       1.00      0.50      0.67         2\n",
      "         71       1.00      1.00      1.00         2\n",
      "         72       1.00      0.50      0.67         2\n",
      "         73       1.00      1.00      1.00         2\n",
      "         74       1.00      1.00      1.00         2\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       0.67      1.00      0.80         2\n",
      "\n",
      "avg / total       0.98      0.97      0.97       154\n",
      "\n",
      "each loop acc 0.974025974026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         2\n",
      "          1       0.67      1.00      0.80         2\n",
      "          2       1.00      1.00      1.00         2\n",
      "          3       1.00      1.00      1.00         2\n",
      "          4       1.00      1.00      1.00         2\n",
      "          5       1.00      1.00      1.00         2\n",
      "          6       1.00      1.00      1.00         2\n",
      "          7       1.00      1.00      1.00         2\n",
      "          8       1.00      1.00      1.00         2\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       1.00      1.00      1.00         2\n",
      "         11       1.00      1.00      1.00         2\n",
      "         12       1.00      1.00      1.00         2\n",
      "         13       1.00      1.00      1.00         2\n",
      "         14       1.00      1.00      1.00         2\n",
      "         15       1.00      1.00      1.00         2\n",
      "         16       1.00      1.00      1.00         2\n",
      "         17       1.00      1.00      1.00         2\n",
      "         18       1.00      1.00      1.00         2\n",
      "         19       1.00      1.00      1.00         2\n",
      "         20       1.00      1.00      1.00         2\n",
      "         21       1.00      1.00      1.00         2\n",
      "         22       1.00      1.00      1.00         2\n",
      "         23       1.00      1.00      1.00         2\n",
      "         24       0.67      1.00      0.80         2\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00         2\n",
      "         27       1.00      1.00      1.00         2\n",
      "         28       1.00      1.00      1.00         2\n",
      "         29       1.00      1.00      1.00         2\n",
      "         30       1.00      1.00      1.00         2\n",
      "         31       1.00      1.00      1.00         2\n",
      "         32       1.00      1.00      1.00         2\n",
      "         33       1.00      1.00      1.00         2\n",
      "         34       1.00      1.00      1.00         2\n",
      "         35       1.00      1.00      1.00         2\n",
      "         36       1.00      1.00      1.00         2\n",
      "         37       1.00      1.00      1.00         2\n",
      "         38       1.00      1.00      1.00         2\n",
      "         39       1.00      1.00      1.00         2\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       1.00      1.00      1.00         2\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         2\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00         2\n",
      "         46       1.00      0.50      0.67         2\n",
      "         47       1.00      1.00      1.00         2\n",
      "         48       1.00      1.00      1.00         2\n",
      "         49       1.00      1.00      1.00         2\n",
      "         50       1.00      1.00      1.00         2\n",
      "         51       1.00      1.00      1.00         2\n",
      "         52       1.00      1.00      1.00         2\n",
      "         53       1.00      1.00      1.00         2\n",
      "         54       1.00      1.00      1.00         2\n",
      "         55       1.00      1.00      1.00         2\n",
      "         56       1.00      1.00      1.00         2\n",
      "         57       1.00      1.00      1.00         2\n",
      "         58       1.00      1.00      1.00         2\n",
      "         59       1.00      1.00      1.00         2\n",
      "         60       1.00      1.00      1.00         2\n",
      "         61       1.00      1.00      1.00         2\n",
      "         62       1.00      1.00      1.00         2\n",
      "         63       1.00      1.00      1.00         2\n",
      "         64       1.00      1.00      1.00         2\n",
      "         65       1.00      1.00      1.00         2\n",
      "         66       1.00      1.00      1.00         2\n",
      "         67       1.00      1.00      1.00         2\n",
      "         68       1.00      1.00      1.00         2\n",
      "         69       1.00      1.00      1.00         2\n",
      "         70       1.00      1.00      1.00         2\n",
      "         71       1.00      1.00      1.00         2\n",
      "         72       1.00      0.50      0.67         2\n",
      "         73       1.00      1.00      1.00         2\n",
      "         74       1.00      1.00      1.00         2\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       0.99      0.99      0.99       154\n",
      "\n",
      "each loop acc 0.987012987013\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         2\n",
      "          1       1.00      1.00      1.00         2\n",
      "          2       1.00      1.00      1.00         2\n",
      "          3       1.00      1.00      1.00         2\n",
      "          4       1.00      1.00      1.00         2\n",
      "          5       1.00      1.00      1.00         2\n",
      "          6       1.00      1.00      1.00         2\n",
      "          7       1.00      1.00      1.00         2\n",
      "          8       1.00      1.00      1.00         2\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       1.00      1.00      1.00         2\n",
      "         11       1.00      1.00      1.00         2\n",
      "         12       1.00      1.00      1.00         2\n",
      "         13       1.00      1.00      1.00         2\n",
      "         14       1.00      1.00      1.00         2\n",
      "         15       1.00      1.00      1.00         2\n",
      "         16       1.00      1.00      1.00         2\n",
      "         17       1.00      1.00      1.00         2\n",
      "         18       1.00      1.00      1.00         2\n",
      "         19       1.00      1.00      1.00         2\n",
      "         20       1.00      1.00      1.00         2\n",
      "         21       1.00      1.00      1.00         2\n",
      "         22       1.00      1.00      1.00         2\n",
      "         23       1.00      1.00      1.00         2\n",
      "         24       1.00      1.00      1.00         2\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00         2\n",
      "         27       1.00      1.00      1.00         2\n",
      "         28       1.00      1.00      1.00         2\n",
      "         29       1.00      1.00      1.00         2\n",
      "         30       1.00      1.00      1.00         2\n",
      "         31       1.00      1.00      1.00         2\n",
      "         32       1.00      1.00      1.00         2\n",
      "         33       1.00      1.00      1.00         2\n",
      "         34       1.00      1.00      1.00         2\n",
      "         35       0.67      1.00      0.80         2\n",
      "         36       1.00      1.00      1.00         2\n",
      "         37       1.00      1.00      1.00         2\n",
      "         38       1.00      1.00      1.00         2\n",
      "         39       1.00      1.00      1.00         2\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       1.00      1.00      1.00         2\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         2\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00         2\n",
      "         46       1.00      1.00      1.00         2\n",
      "         47       1.00      1.00      1.00         2\n",
      "         48       1.00      1.00      1.00         2\n",
      "         49       1.00      1.00      1.00         2\n",
      "         50       1.00      1.00      1.00         2\n",
      "         51       1.00      1.00      1.00         2\n",
      "         52       1.00      1.00      1.00         2\n",
      "         53       1.00      1.00      1.00         2\n",
      "         54       1.00      1.00      1.00         2\n",
      "         55       1.00      1.00      1.00         2\n",
      "         56       1.00      1.00      1.00         2\n",
      "         57       1.00      1.00      1.00         2\n",
      "         58       1.00      1.00      1.00         2\n",
      "         59       1.00      1.00      1.00         2\n",
      "         60       1.00      1.00      1.00         2\n",
      "         61       1.00      1.00      1.00         2\n",
      "         62       1.00      1.00      1.00         2\n",
      "         63       1.00      1.00      1.00         2\n",
      "         64       1.00      1.00      1.00         2\n",
      "         65       1.00      1.00      1.00         2\n",
      "         66       1.00      1.00      1.00         2\n",
      "         67       1.00      1.00      1.00         2\n",
      "         68       1.00      1.00      1.00         2\n",
      "         69       1.00      1.00      1.00         2\n",
      "         70       1.00      0.50      0.67         2\n",
      "         71       1.00      1.00      1.00         2\n",
      "         72       1.00      1.00      1.00         2\n",
      "         73       1.00      1.00      1.00         2\n",
      "         74       1.00      1.00      1.00         2\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       1.00      0.99      0.99       154\n",
      "\n",
      "each loop acc 0.993506493506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         2\n",
      "          1       1.00      1.00      1.00         2\n",
      "          2       1.00      1.00      1.00         2\n",
      "          3       1.00      1.00      1.00         2\n",
      "          4       1.00      1.00      1.00         2\n",
      "          5       1.00      1.00      1.00         2\n",
      "          6       1.00      1.00      1.00         2\n",
      "          7       1.00      1.00      1.00         2\n",
      "          8       1.00      1.00      1.00         2\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       1.00      1.00      1.00         2\n",
      "         11       1.00      1.00      1.00         2\n",
      "         12       1.00      1.00      1.00         2\n",
      "         13       1.00      1.00      1.00         2\n",
      "         14       1.00      1.00      1.00         2\n",
      "         15       1.00      0.50      0.67         2\n",
      "         16       1.00      1.00      1.00         2\n",
      "         17       1.00      1.00      1.00         2\n",
      "         18       1.00      1.00      1.00         2\n",
      "         19       1.00      1.00      1.00         2\n",
      "         20       1.00      1.00      1.00         2\n",
      "         21       1.00      1.00      1.00         2\n",
      "         22       1.00      1.00      1.00         2\n",
      "         23       1.00      1.00      1.00         2\n",
      "         24       1.00      1.00      1.00         2\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00         2\n",
      "         27       1.00      1.00      1.00         2\n",
      "         28       1.00      1.00      1.00         2\n",
      "         29       1.00      1.00      1.00         2\n",
      "         30       1.00      1.00      1.00         2\n",
      "         31       1.00      1.00      1.00         2\n",
      "         32       1.00      1.00      1.00         2\n",
      "         33       1.00      1.00      1.00         2\n",
      "         34       0.67      1.00      0.80         2\n",
      "         35       1.00      1.00      1.00         2\n",
      "         36       1.00      1.00      1.00         2\n",
      "         37       1.00      1.00      1.00         2\n",
      "         38       1.00      1.00      1.00         2\n",
      "         39       1.00      1.00      1.00         2\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       1.00      1.00      1.00         2\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         2\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00         2\n",
      "         46       1.00      1.00      1.00         2\n",
      "         47       1.00      1.00      1.00         2\n",
      "         48       1.00      1.00      1.00         2\n",
      "         49       1.00      1.00      1.00         2\n",
      "         50       1.00      1.00      1.00         2\n",
      "         51       1.00      1.00      1.00         2\n",
      "         52       1.00      1.00      1.00         2\n",
      "         53       1.00      1.00      1.00         2\n",
      "         54       1.00      1.00      1.00         2\n",
      "         55       1.00      1.00      1.00         2\n",
      "         56       1.00      0.50      0.67         2\n",
      "         57       1.00      1.00      1.00         2\n",
      "         58       1.00      1.00      1.00         2\n",
      "         59       1.00      1.00      1.00         2\n",
      "         60       1.00      1.00      1.00         2\n",
      "         61       1.00      1.00      1.00         2\n",
      "         62       0.67      1.00      0.80         2\n",
      "         63       1.00      1.00      1.00         2\n",
      "         64       1.00      1.00      1.00         2\n",
      "         65       1.00      1.00      1.00         2\n",
      "         66       1.00      1.00      1.00         2\n",
      "         67       1.00      1.00      1.00         2\n",
      "         68       1.00      1.00      1.00         2\n",
      "         69       1.00      1.00      1.00         2\n",
      "         70       1.00      1.00      1.00         2\n",
      "         71       1.00      1.00      1.00         2\n",
      "         72       1.00      1.00      1.00         2\n",
      "         73       1.00      1.00      1.00         2\n",
      "         74       1.00      1.00      1.00         2\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       0.99      0.99      0.99       154\n",
      "\n",
      "each loop acc 0.987012987013\n",
      "random f1  0.9833766233766236\n",
      "random Accuracy: 0.9844155844155844\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import lsanomaly\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn import utils  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "\n",
    "# import the CSV from http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "# this will return a pandas dataframe.\n",
    "data = pd.read_csv('C:/Users/S/Documents/PY/features1.csv', low_memory=False)\n",
    "'''data.loc[data['UUID'] == \"RVTNB1502866560357\", \"attack\"] = 1  \n",
    "data.loc[data['UUID'] != \"RVTNB1502866560357\", \"attack\"] = -1\n",
    "df_majority = data[data['attack']==-1]\n",
    "df_minority = data[data['attack']==1]\n",
    "from sklearn.utils import resample\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=830,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#print(data['attack'].value_counts())'''\n",
    "\n",
    "#target=np.array(target)\n",
    "#target = pd.DataFrame(target,columns=['attack'])\n",
    "\n",
    "#data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "categorical_columns=[\"UUID\",\"Language\",\"Hardware_Model\",\"SDK_Version\",\"Manufacture\",\"Screen_Size\",\"Time_Zone\",\"Country_Code\"]\n",
    "cate_data = data[categorical_columns]\n",
    "\n",
    "#for col in data.columns.values:\n",
    "#    print(col, data[col].unique())\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def label_encode(cate_data, columns):\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        col_values_unique = list(cate_data[col].unique())\n",
    "        le_fitted = le.fit(col_values_unique)\n",
    " \n",
    "        col_values = list(cate_data[col].values)\n",
    "        le.classes_\n",
    "        col_values_transformed = le.transform(col_values)\n",
    "        cate_data[col] = col_values_transformed\n",
    " \n",
    "to_be_encoded_cols = cate_data.columns.values\n",
    "label_encode(cate_data, to_be_encoded_cols)\n",
    "display(cate_data.head())\n",
    "target=cate_data['UUID']\n",
    "target=np.array(target)\n",
    "#target = pd.DataFrame(target)\n",
    "#target=target1.values\n",
    "\n",
    "data.drop([\"UUID\",\"Language\",\"Hardware_Model\",\"SDK_Version\",\"Manufacture\",\"Screen_Size\",\"Time_Zone\",\"Country_Code\"], axis=1, inplace=True)\n",
    "data=pd.concat([data,cate_data], axis=1)\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "\n",
    "# check the shape for sanity checking.\n",
    "data.shape\n",
    "display(data.head())\n",
    "print(\"initial data info\",data.info())\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"data is\",data.shape)\n",
    "from skfeature.function.information_theoretical_based import LCSI\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "\n",
    "from skfeature.utility.entropy_estimators import *\n",
    "import scipy.io\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#scaled_data=data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaleddata= pd.DataFrame(scaled_data)\n",
    "scaled_data=np.array(scaled_data)\n",
    "\n",
    "print(scaled_data.shape)\n",
    "print(target.shape)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "#display(target.head())\n",
    "#idx=MRMR.mrmr(scaled_data,target,n_selected_features=50)\n",
    "'''from sklearn import cross_validation\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "correct = 0\n",
    "print(\"scaled data details - \",scaled_data.info())\n",
    "print(\"target data details - \",target.info())\n",
    "for train, test in ss:\n",
    "    #print(scaled_data[train])\n",
    "    #print(target[train])\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=50)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:50]]\n",
    "print(features)    '''\n",
    "'''skb= SVC(kernel=\"linear\")\n",
    "rfe = RFE(estimator=skb, n_features_to_select=70)\n",
    "rfe=rfe.fit(scaleddata,target)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "skft = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "for train, test in skft:\n",
    "    X_train,X_test=scaled_data.iloc[train],scaled_data.iloc[test]\n",
    "    Y_train,y_test=target.iloc[train],target.iloc[test]\n",
    "    model1 = svm.OneClassSVM(nu=nu, kernel='rbf', gamma=0.10000000000000001)  \n",
    "    model1.fit(X_train, Y_train)\n",
    "    scores = cross_val_score(model1,X_test,y_test, cv=5, scoring='accuracy')\n",
    "    print(scores)\n",
    "print(scores.mean())'''\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5,random_state=36851234)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "''''from sklearn.model_selection import GridSearchCV\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "clf = svm.SVC(decision_function_shape='ovo',kernel='rbf')    # linear SVM\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=skf)\n",
    "grid.fit(scaled_data, target)\n",
    "print(\"The best parameters are %s with a score of %0.2f\"% (grid.best_params_, grid.best_score_))'''\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "clf=RandomForestClassifier(n_estimators=5,class_weight='balanced')\n",
    "\n",
    "\n",
    "correct = 0\n",
    "fscoreTotal =0\n",
    "\n",
    "results=[]\n",
    "for train, test in skf.split(scaled_data,target):\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=36)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:36]]\n",
    "    \n",
    "        # train a classification model with the selected features on the training dataset\n",
    "    clf.fit(features[train], target[train])\n",
    "    #clf1.fit(scaled_data[train],target[train])\n",
    "    \n",
    "        # predict the class labels of test data\n",
    "    y_predict = clf.predict(features[test])\n",
    "    #y_predict = clf1.predict(scaled_data[test])\n",
    "    \n",
    "    print(\"metrics\")\n",
    "        # obtain the classification accuracy on the test data\n",
    "    acc = accuracy_score(target[test], y_predict)\n",
    "    \n",
    "    correct = correct + acc\n",
    "    \n",
    "    fscore=f1_score(target[test], y_predict,average='weighted')\n",
    "    \n",
    "    fscoreTotal=fscoreTotal+fscore\n",
    "    \n",
    "        #print(\"fsc \",f1_score(target[test], y_predict,average='weighted'))\n",
    "        #print(\"conf mat \",confusion_matrix(target[test],y_predict))\n",
    "        #print(\"ACCURACY: \", (accuracy_score(target[test], y_predict)))\n",
    "    report = classification_report(target[test], y_predict)\n",
    "    print(report)\n",
    "    \n",
    "    print(\"each loop acc\",acc)\n",
    "   \n",
    "score=float(correct)/5\n",
    "\n",
    "results.append(score)\n",
    "\n",
    "print(\"random f1 \",float(fscoreTotal)/5)\n",
    "    # output the average classification accuracy over all 10 folds\n",
    "print(\"random Accuracy:\", float(correct)/5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UUID\n",
       "0    48\n",
       "1    48\n",
       "2    48\n",
       "3    48\n",
       "4    48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pLN1</th>\n",
       "      <th>p.2</th>\n",
       "      <th>pLN3</th>\n",
       "      <th>pt4</th>\n",
       "      <th>pi5</th>\n",
       "      <th>pe6</th>\n",
       "      <th>pLN7</th>\n",
       "      <th>p58</th>\n",
       "      <th>pLN9</th>\n",
       "      <th>pSH10</th>\n",
       "      <th>...</th>\n",
       "      <th>du2a13</th>\n",
       "      <th>du2n14</th>\n",
       "      <th>du2n15</th>\n",
       "      <th>avgdu</th>\n",
       "      <th>avgud</th>\n",
       "      <th>avgdd</th>\n",
       "      <th>avguu</th>\n",
       "      <th>avdu2</th>\n",
       "      <th>avgp</th>\n",
       "      <th>avga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>37.875</td>\n",
       "      <td>24.466667</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>88.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>670740857</td>\n",
       "      <td>3015</td>\n",
       "      <td>1081</td>\n",
       "      <td>37.625</td>\n",
       "      <td>31.933333</td>\n",
       "      <td>64.066667</td>\n",
       "      <td>63.266667</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2361</td>\n",
       "      <td>1918</td>\n",
       "      <td>884</td>\n",
       "      <td>64.125</td>\n",
       "      <td>453.733333</td>\n",
       "      <td>515.933333</td>\n",
       "      <td>513.133333</td>\n",
       "      <td>575.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1918</td>\n",
       "      <td>1438</td>\n",
       "      <td>827</td>\n",
       "      <td>63.250</td>\n",
       "      <td>347.733333</td>\n",
       "      <td>407.733333</td>\n",
       "      <td>406.400000</td>\n",
       "      <td>466.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>69.375</td>\n",
       "      <td>-9.133333</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pLN1  p.2  pLN3  pt4  pi5  pe6  pLN7  p58  pLN9  pSH10    ...     \\\n",
       "0   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "1   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "2   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "3   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "4   1.0  1.0   1.0  1.0  1.0  1.0   1.0  1.0   1.0    1.0    ...      \n",
       "\n",
       "      du2a13     du2n14  du2n15   avgdu       avgud       avgdd       avguu  \\\n",
       "0       2302  670740857     973  37.875   24.466667   56.800000   55.866667   \n",
       "1  670740857       3015    1081  37.625   31.933333   64.066667   63.266667   \n",
       "2       2361       1918     884  64.125  453.733333  515.933333  513.133333   \n",
       "3       1918       1438     827  63.250  347.733333  407.733333  406.400000   \n",
       "4       2302  670740857     973  69.375   -9.133333   56.800000   55.866667   \n",
       "\n",
       "        avdu2  avgp      avga  \n",
       "0   88.200000   1.0  0.004412  \n",
       "1   95.400000   1.0  0.004167  \n",
       "2  575.333333   1.0  0.008333  \n",
       "3  466.400000   1.0  0.008211  \n",
       "4  121.800000   1.0  0.009804  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7700 entries, 0 to 7699\n",
      "Columns: 147 entries, pLN1 to avga\n",
      "dtypes: float64(71), int64(76)\n",
      "memory usage: 8.6 MB\n",
      "initial data info None\n",
      "data is (7700, 147)\n",
      "(7700, 147)\n",
      "(7700,)\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      1.00      0.89        20\n",
      "          1       0.91      1.00      0.95        20\n",
      "          2       0.95      1.00      0.98        20\n",
      "          3       1.00      1.00      1.00        20\n",
      "          4       1.00      1.00      1.00        20\n",
      "          5       1.00      1.00      1.00        20\n",
      "          6       1.00      1.00      1.00        20\n",
      "          7       1.00      1.00      1.00        20\n",
      "          8       0.78      0.90      0.84        20\n",
      "          9       1.00      0.95      0.97        20\n",
      "         10       0.80      1.00      0.89        20\n",
      "         11       1.00      1.00      1.00        20\n",
      "         12       1.00      1.00      1.00        20\n",
      "         13       1.00      1.00      1.00        20\n",
      "         14       1.00      1.00      1.00        20\n",
      "         15       0.95      0.90      0.92        20\n",
      "         16       1.00      1.00      1.00        20\n",
      "         17       1.00      1.00      1.00        20\n",
      "         18       1.00      0.95      0.97        20\n",
      "         19       1.00      0.95      0.97        20\n",
      "         20       1.00      0.95      0.97        20\n",
      "         21       1.00      1.00      1.00        20\n",
      "         22       0.83      1.00      0.91        20\n",
      "         23       0.86      0.95      0.90        20\n",
      "         24       0.81      0.85      0.83        20\n",
      "         25       0.89      0.85      0.87        20\n",
      "         26       1.00      1.00      1.00        20\n",
      "         27       0.94      0.85      0.89        20\n",
      "         28       1.00      0.95      0.97        20\n",
      "         29       0.91      1.00      0.95        20\n",
      "         30       1.00      0.85      0.92        20\n",
      "         31       0.95      0.95      0.95        20\n",
      "         32       1.00      1.00      1.00        20\n",
      "         33       1.00      0.90      0.95        20\n",
      "         34       1.00      0.95      0.97        20\n",
      "         35       0.89      0.85      0.87        20\n",
      "         36       1.00      1.00      1.00        20\n",
      "         37       0.82      0.90      0.86        20\n",
      "         38       1.00      1.00      1.00        20\n",
      "         39       0.90      0.95      0.93        20\n",
      "         40       0.95      1.00      0.98        20\n",
      "         41       1.00      0.95      0.97        20\n",
      "         42       0.95      0.95      0.95        20\n",
      "         43       1.00      0.90      0.95        20\n",
      "         44       0.95      0.90      0.92        20\n",
      "         45       0.91      1.00      0.95        20\n",
      "         46       0.89      0.85      0.87        20\n",
      "         47       1.00      1.00      1.00        20\n",
      "         48       1.00      0.95      0.97        20\n",
      "         49       1.00      1.00      1.00        20\n",
      "         50       1.00      0.90      0.95        20\n",
      "         51       0.85      0.85      0.85        20\n",
      "         52       1.00      0.85      0.92        20\n",
      "         53       0.95      0.95      0.95        20\n",
      "         54       1.00      0.95      0.97        20\n",
      "         55       0.95      0.90      0.92        20\n",
      "         56       1.00      0.85      0.92        20\n",
      "         57       1.00      1.00      1.00        20\n",
      "         58       0.91      1.00      0.95        20\n",
      "         59       1.00      1.00      1.00        20\n",
      "         60       1.00      1.00      1.00        20\n",
      "         61       1.00      1.00      1.00        20\n",
      "         62       1.00      0.90      0.95        20\n",
      "         63       1.00      1.00      1.00        20\n",
      "         64       0.95      1.00      0.98        20\n",
      "         65       0.90      0.95      0.93        20\n",
      "         66       0.95      0.95      0.95        20\n",
      "         67       0.95      0.95      0.95        20\n",
      "         68       1.00      1.00      1.00        20\n",
      "         69       1.00      1.00      1.00        20\n",
      "         70       0.86      0.90      0.88        20\n",
      "         71       1.00      1.00      1.00        20\n",
      "         72       0.95      0.95      0.95        20\n",
      "         73       0.95      0.95      0.95        20\n",
      "         74       1.00      1.00      1.00        20\n",
      "         75       0.78      0.70      0.74        20\n",
      "         76       0.95      0.95      0.95        20\n",
      "\n",
      "avg / total       0.96      0.95      0.95      1540\n",
      "\n",
      "each loop acc 0.952597402597\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.90      0.90        20\n",
      "          1       0.95      1.00      0.98        20\n",
      "          2       0.95      1.00      0.98        20\n",
      "          3       1.00      1.00      1.00        20\n",
      "          4       1.00      1.00      1.00        20\n",
      "          5       1.00      1.00      1.00        20\n",
      "          6       1.00      1.00      1.00        20\n",
      "          7       0.95      1.00      0.98        20\n",
      "          8       0.62      0.75      0.68        20\n",
      "          9       0.95      1.00      0.98        20\n",
      "         10       0.77      0.85      0.81        20\n",
      "         11       1.00      1.00      1.00        20\n",
      "         12       1.00      1.00      1.00        20\n",
      "         13       1.00      1.00      1.00        20\n",
      "         14       1.00      1.00      1.00        20\n",
      "         15       0.76      0.95      0.84        20\n",
      "         16       1.00      1.00      1.00        20\n",
      "         17       1.00      1.00      1.00        20\n",
      "         18       1.00      1.00      1.00        20\n",
      "         19       0.95      0.95      0.95        20\n",
      "         20       0.91      1.00      0.95        20\n",
      "         21       1.00      0.95      0.97        20\n",
      "         22       1.00      0.95      0.97        20\n",
      "         23       0.95      1.00      0.98        20\n",
      "         24       0.94      0.75      0.83        20\n",
      "         25       0.86      0.90      0.88        20\n",
      "         26       1.00      1.00      1.00        20\n",
      "         27       0.70      0.70      0.70        20\n",
      "         28       1.00      1.00      1.00        20\n",
      "         29       1.00      1.00      1.00        20\n",
      "         30       1.00      0.95      0.97        20\n",
      "         31       1.00      1.00      1.00        20\n",
      "         32       1.00      1.00      1.00        20\n",
      "         33       1.00      1.00      1.00        20\n",
      "         34       0.95      0.95      0.95        20\n",
      "         35       0.95      0.95      0.95        20\n",
      "         36       1.00      1.00      1.00        20\n",
      "         37       1.00      0.95      0.97        20\n",
      "         38       1.00      1.00      1.00        20\n",
      "         39       0.95      0.95      0.95        20\n",
      "         40       1.00      0.95      0.97        20\n",
      "         41       0.86      0.90      0.88        20\n",
      "         42       1.00      0.95      0.97        20\n",
      "         43       0.90      0.90      0.90        20\n",
      "         44       1.00      1.00      1.00        20\n",
      "         45       1.00      0.95      0.97        20\n",
      "         46       0.86      0.90      0.88        20\n",
      "         47       1.00      0.95      0.97        20\n",
      "         48       0.95      1.00      0.98        20\n",
      "         49       1.00      1.00      1.00        20\n",
      "         50       0.81      0.65      0.72        20\n",
      "         51       0.81      0.85      0.83        20\n",
      "         52       0.95      0.95      0.95        20\n",
      "         53       0.95      1.00      0.98        20\n",
      "         54       1.00      1.00      1.00        20\n",
      "         55       1.00      1.00      1.00        20\n",
      "         56       1.00      1.00      1.00        20\n",
      "         57       1.00      1.00      1.00        20\n",
      "         58       1.00      1.00      1.00        20\n",
      "         59       1.00      1.00      1.00        20\n",
      "         60       1.00      1.00      1.00        20\n",
      "         61       1.00      1.00      1.00        20\n",
      "         62       0.90      0.90      0.90        20\n",
      "         63       1.00      1.00      1.00        20\n",
      "         64       1.00      0.90      0.95        20\n",
      "         65       0.89      0.80      0.84        20\n",
      "         66       0.95      0.95      0.95        20\n",
      "         67       1.00      0.95      0.97        20\n",
      "         68       1.00      1.00      1.00        20\n",
      "         69       1.00      1.00      1.00        20\n",
      "         70       0.95      0.95      0.95        20\n",
      "         71       1.00      1.00      1.00        20\n",
      "         72       0.82      0.90      0.86        20\n",
      "         73       1.00      0.95      0.97        20\n",
      "         74       0.95      1.00      0.98        20\n",
      "         75       0.88      0.75      0.81        20\n",
      "         76       0.89      0.80      0.84        20\n",
      "\n",
      "avg / total       0.95      0.95      0.95      1540\n",
      "\n",
      "each loop acc 0.950649350649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98        20\n",
      "          1       1.00      1.00      1.00        20\n",
      "          2       0.95      1.00      0.98        20\n",
      "          3       1.00      1.00      1.00        20\n",
      "          4       1.00      1.00      1.00        20\n",
      "          5       1.00      1.00      1.00        20\n",
      "          6       1.00      1.00      1.00        20\n",
      "          7       1.00      1.00      1.00        20\n",
      "          8       0.76      0.95      0.84        20\n",
      "          9       1.00      0.95      0.97        20\n",
      "         10       0.95      0.95      0.95        20\n",
      "         11       1.00      1.00      1.00        20\n",
      "         12       0.95      1.00      0.98        20\n",
      "         13       1.00      1.00      1.00        20\n",
      "         14       1.00      1.00      1.00        20\n",
      "         15       0.90      0.90      0.90        20\n",
      "         16       1.00      0.90      0.95        20\n",
      "         17       1.00      1.00      1.00        20\n",
      "         18       1.00      1.00      1.00        20\n",
      "         19       0.95      1.00      0.98        20\n",
      "         20       1.00      0.95      0.97        20\n",
      "         21       1.00      0.95      0.97        20\n",
      "         22       1.00      0.95      0.97        20\n",
      "         23       1.00      1.00      1.00        20\n",
      "         24       0.87      1.00      0.93        20\n",
      "         25       0.95      0.95      0.95        20\n",
      "         26       1.00      1.00      1.00        20\n",
      "         27       0.85      0.85      0.85        20\n",
      "         28       1.00      0.95      0.97        20\n",
      "         29       1.00      1.00      1.00        20\n",
      "         30       1.00      1.00      1.00        20\n",
      "         31       1.00      1.00      1.00        20\n",
      "         32       1.00      1.00      1.00        20\n",
      "         33       0.95      0.95      0.95        20\n",
      "         34       0.95      1.00      0.98        20\n",
      "         35       0.90      0.95      0.93        20\n",
      "         36       1.00      1.00      1.00        20\n",
      "         37       0.95      0.95      0.95        20\n",
      "         38       1.00      1.00      1.00        20\n",
      "         39       1.00      1.00      1.00        20\n",
      "         40       1.00      1.00      1.00        20\n",
      "         41       1.00      1.00      1.00        20\n",
      "         42       1.00      1.00      1.00        20\n",
      "         43       1.00      0.95      0.97        20\n",
      "         44       1.00      0.95      0.97        20\n",
      "         45       1.00      1.00      1.00        20\n",
      "         46       1.00      0.90      0.95        20\n",
      "         47       1.00      1.00      1.00        20\n",
      "         48       1.00      1.00      1.00        20\n",
      "         49       0.91      1.00      0.95        20\n",
      "         50       0.95      0.90      0.92        20\n",
      "         51       1.00      0.95      0.97        20\n",
      "         52       1.00      1.00      1.00        20\n",
      "         53       1.00      1.00      1.00        20\n",
      "         54       1.00      1.00      1.00        20\n",
      "         55       0.95      1.00      0.98        20\n",
      "         56       0.95      1.00      0.98        20\n",
      "         57       1.00      1.00      1.00        20\n",
      "         58       0.95      0.95      0.95        20\n",
      "         59       1.00      1.00      1.00        20\n",
      "         60       1.00      1.00      1.00        20\n",
      "         61       1.00      1.00      1.00        20\n",
      "         62       1.00      1.00      1.00        20\n",
      "         63       1.00      1.00      1.00        20\n",
      "         64       0.95      1.00      0.98        20\n",
      "         65       0.89      0.85      0.87        20\n",
      "         66       1.00      1.00      1.00        20\n",
      "         67       1.00      0.95      0.97        20\n",
      "         68       1.00      1.00      1.00        20\n",
      "         69       1.00      1.00      1.00        20\n",
      "         70       0.95      0.90      0.92        20\n",
      "         71       1.00      1.00      1.00        20\n",
      "         72       1.00      0.95      0.97        20\n",
      "         73       1.00      0.95      0.97        20\n",
      "         74       0.95      1.00      0.98        20\n",
      "         75       0.94      0.80      0.86        20\n",
      "         76       0.95      1.00      0.98        20\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1540\n",
      "\n",
      "each loop acc 0.975324675325\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.90      0.92        20\n",
      "          1       1.00      1.00      1.00        20\n",
      "          2       1.00      1.00      1.00        20\n",
      "          3       1.00      1.00      1.00        20\n",
      "          4       1.00      1.00      1.00        20\n",
      "          5       0.95      1.00      0.98        20\n",
      "          6       1.00      1.00      1.00        20\n",
      "          7       0.95      0.95      0.95        20\n",
      "          8       0.82      0.90      0.86        20\n",
      "          9       0.90      0.95      0.93        20\n",
      "         10       0.76      0.80      0.78        20\n",
      "         11       1.00      1.00      1.00        20\n",
      "         12       1.00      1.00      1.00        20\n",
      "         13       1.00      1.00      1.00        20\n",
      "         14       0.91      1.00      0.95        20\n",
      "         15       0.84      0.80      0.82        20\n",
      "         16       0.95      1.00      0.98        20\n",
      "         17       1.00      1.00      1.00        20\n",
      "         18       0.90      0.95      0.93        20\n",
      "         19       0.95      1.00      0.98        20\n",
      "         20       0.95      0.95      0.95        20\n",
      "         21       0.87      1.00      0.93        20\n",
      "         22       1.00      1.00      1.00        20\n",
      "         23       0.95      1.00      0.98        20\n",
      "         24       0.82      0.90      0.86        20\n",
      "         25       1.00      1.00      1.00        20\n",
      "         26       0.91      1.00      0.95        20\n",
      "         27       0.68      0.85      0.76        20\n",
      "         28       1.00      0.95      0.97        20\n",
      "         29       1.00      1.00      1.00        20\n",
      "         30       1.00      1.00      1.00        20\n",
      "         31       1.00      1.00      1.00        20\n",
      "         32       1.00      1.00      1.00        20\n",
      "         33       1.00      0.90      0.95        20\n",
      "         34       1.00      0.95      0.97        20\n",
      "         35       0.90      0.95      0.93        20\n",
      "         36       1.00      0.80      0.89        20\n",
      "         37       0.95      1.00      0.98        20\n",
      "         38       1.00      1.00      1.00        20\n",
      "         39       0.86      0.90      0.88        20\n",
      "         40       0.95      1.00      0.98        20\n",
      "         41       1.00      0.95      0.97        20\n",
      "         42       0.79      0.95      0.86        20\n",
      "         43       0.90      0.95      0.93        20\n",
      "         44       1.00      0.90      0.95        20\n",
      "         45       1.00      1.00      1.00        20\n",
      "         46       1.00      0.95      0.97        20\n",
      "         47       1.00      1.00      1.00        20\n",
      "         48       1.00      1.00      1.00        20\n",
      "         49       1.00      0.95      0.97        20\n",
      "         50       0.86      0.90      0.88        20\n",
      "         51       0.74      0.70      0.72        20\n",
      "         52       1.00      0.90      0.95        20\n",
      "         53       0.94      0.75      0.83        20\n",
      "         54       0.91      1.00      0.95        20\n",
      "         55       0.95      1.00      0.98        20\n",
      "         56       1.00      0.95      0.97        20\n",
      "         57       1.00      1.00      1.00        20\n",
      "         58       0.91      1.00      0.95        20\n",
      "         59       1.00      1.00      1.00        20\n",
      "         60       1.00      1.00      1.00        20\n",
      "         61       1.00      1.00      1.00        20\n",
      "         62       0.93      0.70      0.80        20\n",
      "         63       0.95      0.95      0.95        20\n",
      "         64       0.95      0.95      0.95        20\n",
      "         65       0.80      0.80      0.80        20\n",
      "         66       0.86      0.90      0.88        20\n",
      "         67       0.95      0.90      0.92        20\n",
      "         68       1.00      1.00      1.00        20\n",
      "         69       1.00      1.00      1.00        20\n",
      "         70       0.95      0.95      0.95        20\n",
      "         71       1.00      0.95      0.97        20\n",
      "         72       0.95      0.90      0.92        20\n",
      "         73       1.00      0.85      0.92        20\n",
      "         74       0.95      1.00      0.98        20\n",
      "         75       0.79      0.55      0.65        20\n",
      "         76       0.95      0.95      0.95        20\n",
      "\n",
      "avg / total       0.94      0.94      0.94      1540\n",
      "\n",
      "each loop acc 0.942857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        20\n",
      "          1       1.00      0.90      0.95        20\n",
      "          2       1.00      1.00      1.00        20\n",
      "          3       1.00      1.00      1.00        20\n",
      "          4       1.00      1.00      1.00        20\n",
      "          5       1.00      1.00      1.00        20\n",
      "          6       0.95      1.00      0.98        20\n",
      "          7       1.00      1.00      1.00        20\n",
      "          8       0.86      0.95      0.90        20\n",
      "          9       0.95      1.00      0.98        20\n",
      "         10       0.87      1.00      0.93        20\n",
      "         11       0.95      1.00      0.98        20\n",
      "         12       1.00      1.00      1.00        20\n",
      "         13       1.00      1.00      1.00        20\n",
      "         14       1.00      0.95      0.97        20\n",
      "         15       0.87      1.00      0.93        20\n",
      "         16       1.00      1.00      1.00        20\n",
      "         17       1.00      1.00      1.00        20\n",
      "         18       1.00      1.00      1.00        20\n",
      "         19       0.95      1.00      0.98        20\n",
      "         20       0.95      0.95      0.95        20\n",
      "         21       0.90      0.95      0.93        20\n",
      "         22       1.00      1.00      1.00        20\n",
      "         23       0.95      1.00      0.98        20\n",
      "         24       0.89      0.80      0.84        20\n",
      "         25       0.83      0.95      0.88        20\n",
      "         26       1.00      0.95      0.97        20\n",
      "         27       0.85      0.85      0.85        20\n",
      "         28       1.00      1.00      1.00        20\n",
      "         29       1.00      0.90      0.95        20\n",
      "         30       1.00      1.00      1.00        20\n",
      "         31       1.00      1.00      1.00        20\n",
      "         32       0.91      1.00      0.95        20\n",
      "         33       0.90      0.90      0.90        20\n",
      "         34       0.79      0.95      0.86        20\n",
      "         35       0.91      1.00      0.95        20\n",
      "         36       1.00      0.90      0.95        20\n",
      "         37       0.90      0.90      0.90        20\n",
      "         38       1.00      1.00      1.00        20\n",
      "         39       0.95      0.95      0.95        20\n",
      "         40       1.00      1.00      1.00        20\n",
      "         41       0.94      0.85      0.89        20\n",
      "         42       1.00      0.85      0.92        20\n",
      "         43       1.00      1.00      1.00        20\n",
      "         44       1.00      1.00      1.00        20\n",
      "         45       1.00      1.00      1.00        20\n",
      "         46       0.89      0.85      0.87        20\n",
      "         47       1.00      0.95      0.97        20\n",
      "         48       0.95      0.90      0.92        20\n",
      "         49       1.00      1.00      1.00        20\n",
      "         50       1.00      0.85      0.92        20\n",
      "         51       1.00      0.95      0.97        20\n",
      "         52       1.00      1.00      1.00        20\n",
      "         53       0.87      1.00      0.93        20\n",
      "         54       1.00      1.00      1.00        20\n",
      "         55       1.00      1.00      1.00        20\n",
      "         56       1.00      1.00      1.00        20\n",
      "         57       1.00      1.00      1.00        20\n",
      "         58       0.95      0.90      0.92        20\n",
      "         59       1.00      1.00      1.00        20\n",
      "         60       1.00      1.00      1.00        20\n",
      "         61       1.00      1.00      1.00        20\n",
      "         62       1.00      1.00      1.00        20\n",
      "         63       1.00      1.00      1.00        20\n",
      "         64       0.95      0.95      0.95        20\n",
      "         65       0.78      0.90      0.84        20\n",
      "         66       1.00      1.00      1.00        20\n",
      "         67       0.94      0.80      0.86        20\n",
      "         68       0.87      1.00      0.93        20\n",
      "         69       1.00      1.00      1.00        20\n",
      "         70       1.00      0.90      0.95        20\n",
      "         71       1.00      1.00      1.00        20\n",
      "         72       0.94      0.80      0.86        20\n",
      "         73       1.00      1.00      1.00        20\n",
      "         74       1.00      1.00      1.00        20\n",
      "         75       0.94      0.80      0.86        20\n",
      "         76       0.95      0.95      0.95        20\n",
      "\n",
      "avg / total       0.96      0.96      0.96      1540\n",
      "\n",
      "each loop acc 0.96038961039\n",
      "random f1  0.9563069975151448\n",
      "random Accuracy: 0.9563636363636363\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import lsanomaly\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn import utils  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "\n",
    "# import the CSV from http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "# this will return a pandas dataframe.\n",
    "data = pd.read_csv('C:/Users/S/Documents/PY/increased100features.csv', low_memory=False)\n",
    "'''data.loc[data['UUID'] == \"RVTNB1502866560357\", \"attack\"] = 1  \n",
    "data.loc[data['UUID'] != \"RVTNB1502866560357\", \"attack\"] = -1\n",
    "df_majority = data[data['attack']==-1]\n",
    "df_minority = data[data['attack']==1]\n",
    "from sklearn.utils import resample\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=830,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#print(data['attack'].value_counts())'''\n",
    "\n",
    "#target=np.array(target)\n",
    "#target = pd.DataFrame(target,columns=['attack'])\n",
    "\n",
    "#data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "categorical_columns=[\"UUID\"]\n",
    "cate_data = data[categorical_columns]\n",
    "\n",
    "#for col in data.columns.values:\n",
    "#    print(col, data[col].unique())\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def label_encode(cate_data, columns):\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        col_values_unique = list(cate_data[col].unique())\n",
    "        le_fitted = le.fit(col_values_unique)\n",
    " \n",
    "        col_values = list(cate_data[col].values)\n",
    "        le.classes_\n",
    "        col_values_transformed = le.transform(col_values)\n",
    "        cate_data[col] = col_values_transformed\n",
    " \n",
    "to_be_encoded_cols = cate_data.columns.values\n",
    "label_encode(cate_data, to_be_encoded_cols)\n",
    "display(cate_data.head())\n",
    "target=cate_data['UUID']\n",
    "target=np.array(target)\n",
    "#target = pd.DataFrame(target)\n",
    "#target=target1.values\n",
    "\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "data=pd.concat([data,cate_data], axis=1)\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "\n",
    "# check the shape for sanity checking.\n",
    "data.shape\n",
    "display(data.head())\n",
    "print(\"initial data info\",data.info())\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"data is\",data.shape)\n",
    "from skfeature.function.information_theoretical_based import LCSI\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "\n",
    "from skfeature.utility.entropy_estimators import *\n",
    "import scipy.io\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#scaled_data=data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaleddata= pd.DataFrame(scaled_data)\n",
    "scaled_data=np.array(scaled_data)\n",
    "\n",
    "print(scaled_data.shape)\n",
    "print(target.shape)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "#display(target.head())\n",
    "#idx=MRMR.mrmr(scaled_data,target,n_selected_features=50)\n",
    "'''from sklearn import cross_validation\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "correct = 0\n",
    "print(\"scaled data details - \",scaled_data.info())\n",
    "print(\"target data details - \",target.info())\n",
    "for train, test in ss:\n",
    "    #print(scaled_data[train])\n",
    "    #print(target[train])\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=50)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:50]]\n",
    "print(features)    '''\n",
    "'''skb= SVC(kernel=\"linear\")\n",
    "rfe = RFE(estimator=skb, n_features_to_select=70)\n",
    "rfe=rfe.fit(scaleddata,target)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "skft = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "for train, test in skft:\n",
    "    X_train,X_test=scaled_data.iloc[train],scaled_data.iloc[test]\n",
    "    Y_train,y_test=target.iloc[train],target.iloc[test]\n",
    "    model1 = svm.OneClassSVM(nu=nu, kernel='rbf', gamma=0.10000000000000001)  \n",
    "    model1.fit(X_train, Y_train)\n",
    "    scores = cross_val_score(model1,X_test,y_test, cv=5, scoring='accuracy')\n",
    "    print(scores)\n",
    "print(scores.mean())'''\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5,random_state=36851234)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "''''from sklearn.model_selection import GridSearchCV\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "clf = svm.SVC(decision_function_shape='ovo',kernel='rbf')    # linear SVM\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=skf)\n",
    "grid.fit(scaled_data, target)\n",
    "print(\"The best parameters are %s with a score of %0.2f\"% (grid.best_params_, grid.best_score_))'''\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "clf=RandomForestClassifier(n_estimators=5,class_weight='balanced')\n",
    "\n",
    "\n",
    "correct = 0\n",
    "fscoreTotal =0\n",
    "\n",
    "results=[]\n",
    "for train, test in skf.split(scaled_data,target):\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=36)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:79]]\n",
    "    \n",
    "        # train a classification model with the selected features on the training dataset\n",
    "    clf.fit(features[train], target[train])\n",
    "    #clf1.fit(scaled_data[train],target[train])\n",
    "    \n",
    "        # predict the class labels of test data\n",
    "    y_predict = clf.predict(features[test])\n",
    "    #y_predict = clf1.predict(scaled_data[test])\n",
    "    \n",
    "    print(\"metrics\")\n",
    "        # obtain the classification accuracy on the test data\n",
    "    acc = accuracy_score(target[test], y_predict)\n",
    "    \n",
    "    correct = correct + acc\n",
    "    \n",
    "    fscore=f1_score(target[test], y_predict,average='weighted')\n",
    "    \n",
    "    fscoreTotal=fscoreTotal+fscore\n",
    "    \n",
    "        #print(\"fsc \",f1_score(target[test], y_predict,average='weighted'))\n",
    "        #print(\"conf mat \",confusion_matrix(target[test],y_predict))\n",
    "        #print(\"ACCURACY: \", (accuracy_score(target[test], y_predict)))\n",
    "    report = classification_report(target[test], y_predict)\n",
    "    print(report)\n",
    "    \n",
    "    print(\"each loop acc\",acc)\n",
    "   \n",
    "score=float(correct)/5\n",
    "\n",
    "results.append(score)\n",
    "\n",
    "print(\"random f1 \",float(fscoreTotal)/5)\n",
    "    # output the average classification accuracy over all 10 folds\n",
    "print(\"random Accuracy:\", float(correct)/5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
